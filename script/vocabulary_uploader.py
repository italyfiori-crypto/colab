#!/usr/bin/env python3
"""
è¯æ±‡ä¸Šä¼ æœåŠ¡
å¤„ç†è¯æ±‡è¡¨å’Œç« èŠ‚è¯æ±‡å…³è”çš„ä¸Šä¼ é€»è¾‘
"""

import os
import json
import time
import logging
from typing import Dict, List
from pathlib import Path
from wechat_api import WeChatCloudAPI
from data_parser import DataParser


class VocabularyUploader:
    """è¯æ±‡ä¸Šä¼ æœåŠ¡ç±»"""
    
    def __init__(self, api_client: WeChatCloudAPI):
        self.api = api_client
        self.parser = DataParser()
        self.logger = logging.getLogger(__name__)
        
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        self.program_root = os.path.dirname(os.path.dirname(__file__))
        
    def upload_vocabularies(self, book_dir: Path, book_id: str) -> bool:
        """ä¸Šä¼ å½“å‰ä¹¦ç±çš„è¯æ±‡æ•°æ®"""
        try:
            # è·å–è¯æ±‡æ€»è¡¨è·¯å¾„ï¼ˆä½¿ç”¨åŸå§‹è„šæœ¬çš„è·¯å¾„ï¼‰
            master_vocab_path = os.path.join(self.program_root, "output", "vocabulary", "master_vocabulary.json")
            
            if not os.path.exists(master_vocab_path):
                self.logger.error(f"è¯æ±‡æ€»è¡¨ä¸å­˜åœ¨: {master_vocab_path}")
                return False
            
            # è§£æè¯æ±‡æ€»è¡¨
            master_vocabulary_data = self.parser.parse_vocabulary_data(master_vocab_path)
            
            # æ”¶é›†å½“å‰ä¹¦ç±çš„æ‰€æœ‰å•è¯
            book_words = self._collect_book_words(book_dir)
            
            if not book_words:
                self.logger.info("æ²¡æœ‰è¯æ±‡éœ€è¦ä¸Šä¼ ")
                return True
            
            # è¿‡æ»¤å‡ºå½“å‰ä¹¦ç±çš„è¯æ±‡æ•°æ®
            book_vocabulary_data = {word: master_vocabulary_data[word] 
                                  for word in book_words 
                                  if word in master_vocabulary_data}
            
            if not book_vocabulary_data:
                self.logger.info("æ²¡æœ‰åŒ¹é…çš„è¯æ±‡æ•°æ®")
                return True
            
            # é€ä¸ªå¤„ç†å•è¯
            success_count = 0
            skip_count = 0
            filtered_words = list(book_vocabulary_data.values())
            
            self.logger.info(f"å¼€å§‹å¤„ç† {len(filtered_words)} ä¸ªè¯æ±‡...")
            
            for idx, word_data in enumerate(filtered_words):
                word = word_data['word']
                
                # æŸ¥è¯¢å•è¯æ˜¯å¦å·²å­˜åœ¨
                existing_word = self.api.query_database('vocabularies', {'word': word}, limit=1)
                
                if existing_word:
                    skip_count += 1
                else:
                    # æ’å…¥æ–°å•è¯ï¼ˆåŒ…å«é‡è¯•æœºåˆ¶ï¼‰
                    if self._insert_word_with_retry(word_data):
                        success_count += 1
                    else:
                        self.logger.error(f"å•è¯æ’å…¥å¤±è´¥: {word}")
                
                # æ¯10ä¸ªå•è¯æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if (idx + 1) % 10 == 0:
                    self.logger.info(f"ğŸ“ è¿›åº¦: {idx + 1}/{len(filtered_words)}, æ–°å¢: {success_count}, è·³è¿‡: {skip_count}")
            
            self.logger.info(f"è¯æ±‡ä¸Šä¼ å®Œæˆ: æ–°å¢ {success_count}, è·³è¿‡ {skip_count}")
            return True
            
        except Exception as e:
            self.logger.error(f"è¯æ±‡ä¸Šä¼ å¤±è´¥: {e}")
            return False

    def upload_chapter_vocabularies(self, book_dir: Path, book_id: str, vocabulary_data: Dict) -> bool:
        """ä¸Šä¼ ç« èŠ‚è¯æ±‡å…³è”æ•°æ®"""
        try:
            vocab_subchapters_dir = book_dir / "vocabulary" / "subchapters"
            if not vocab_subchapters_dir.exists():
                return True
            
            # è·å–å½“å‰ä¹¦ç±çš„å•è¯é¡ºåº
            book_words = self._collect_book_words(book_dir)
            
            # å¤„ç†æ¯ä¸ªå­ç« èŠ‚çš„è¯æ±‡æ–‡ä»¶
            for vocab_file in vocab_subchapters_dir.glob("*.json"):
                try:
                    with open(vocab_file, 'r', encoding='utf-8') as f:
                        vocab_data = json.load(f)
                    
                    subchapter_id = vocab_data.get("subchapter_id", vocab_file.stem)
                    chapter_words = vocab_data.get("words", [])
                    
                    if not chapter_words:
                        continue
                    
                    # æŒ‰ç…§book_wordsçš„é¡ºåºæ’åºç« èŠ‚å•è¯
                    ordered_words = [word for word in book_words if word in chapter_words]
                    
                    chapter_vocab_record = {
                        "_id": f"{book_id}_{subchapter_id}",
                        "book_id": book_id,
                        "chapter_id": subchapter_id,
                        "words": ordered_words,
                        "created_at": vocabulary_data.get("created_at", "")
                    }
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    existing_record = self.api.query_database('chapter_vocabularies', 
                                                            {'_id': chapter_vocab_record["_id"]}, limit=1)
                    
                    if not existing_record:
                        if not self.api.add_database_records('chapter_vocabularies', [chapter_vocab_record]):
                            self.logger.error(f"ç« èŠ‚è¯æ±‡æ’å…¥å¤±è´¥: {subchapter_id}")
                            return False
                    
                except Exception as e:
                    self.logger.error(f"å¤„ç†ç« èŠ‚è¯æ±‡æ–‡ä»¶å¤±è´¥ {vocab_file}: {e}")
                    continue
            
            return True
            
        except Exception as e:
            self.logger.error(f"ç« èŠ‚è¯æ±‡ä¸Šä¼ å¤±è´¥: {e}")
            return False

    def _collect_book_words(self, book_dir: Path) -> List[str]:
        """æ”¶é›†å½“å‰ä¹¦ç±çš„æ‰€æœ‰å•è¯ï¼ˆæŒ‰é¡ºåºï¼‰"""
        book_words = []
        vocab_subchapters_dir = book_dir / "vocabulary" / "subchapters"
        
        if not vocab_subchapters_dir.exists():
            return book_words
        
        # æŒ‰æ–‡ä»¶åæ’åºå¤„ç†ç« èŠ‚è¯æ±‡
        vocab_files = sorted(vocab_subchapters_dir.glob("*.json"))
        
        for vocab_file in vocab_files:
            try:
                with open(vocab_file, 'r', encoding='utf-8') as f:
                    vocab_data = json.load(f)
                
                chapter_words = vocab_data.get("words", [])
                for word in chapter_words:
                    if word not in book_words:
                        book_words.append(word)
                        
            except Exception as e:
                self.logger.error(f"è¯»å–ç« èŠ‚è¯æ±‡æ–‡ä»¶å¤±è´¥ {vocab_file}: {e}")
                continue
        
        return book_words

    def _insert_word_with_retry(self, word_data: Dict, max_retries: int = 3) -> bool:
        """æ’å…¥å•è¯ï¼ˆåŒ…å«é‡è¯•æœºåˆ¶ï¼‰"""
        word = word_data['word']
        
        for retry_count in range(max_retries):
            try:
                if self.api.add_database_records('vocabularies', [word_data]):
                    return True
                else:
                    if retry_count < max_retries - 1:
                        time.sleep(1)
                    
            except Exception as e:
                if retry_count < max_retries - 1:
                    time.sleep(2)
                else:
                    self.logger.error(f"å•è¯æ’å…¥å¤±è´¥: {word} - {e}")
                    
        return False
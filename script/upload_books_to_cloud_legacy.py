#!/usr/bin/env python3
"""
å¾®ä¿¡äº‘æœåŠ¡ä¹¦ç±æ•°æ®ä¸Šä¼ è„šæœ¬
ç”¨äºå°†outputç›®å½•ä¸‹çš„æœ‰å£°ä¹¦æ•°æ®ä¸Šä¼ åˆ°å¾®ä¿¡äº‘æ•°æ®åº“å’Œå­˜å‚¨
"""

import os
import json
import time
import requests
import logging
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import re

# è·å–é¡¹ç›®æ ¹ç›®å½•
program_root = os.path.dirname(os.path.dirname(__file__))


class WeChatCloudUploader:
    """å¾®ä¿¡äº‘æœåŠ¡ä¸Šä¼ å™¨"""
    
    def __init__(self, app_id: str, app_secret: str, env_id: str):
        """
        åˆå§‹åŒ–ä¸Šä¼ å™¨
        
        Args:
            app_id: å¾®ä¿¡å°ç¨‹åºAppID
            app_secret: å¾®ä¿¡å°ç¨‹åºAppSecret
            env_id: å¾®ä¿¡äº‘ç¯å¢ƒID
        """
        self.app_id = app_id
        self.app_secret = app_secret
        self.env_id = env_id
        
        # Access Tokenç›¸å…³
        self.access_token = None
        self.token_expires_at = None
        
        # APIç«¯ç‚¹
        self.token_url = "https://api.weixin.qq.com/cgi-bin/token"
        self.upload_file_url = "https://api.weixin.qq.com/tcb/uploadfile"
        self.database_add_url = "https://api.weixin.qq.com/tcb/databaseadd"
        self.database_query_url = "https://api.weixin.qq.com/tcb/databasequery"
        self.database_update_url = "https://api.weixin.qq.com/tcb/databaseupdate"
        self.database_delete_url = "https://api.weixin.qq.com/tcb/databasedelete"
        
        # é…ç½®æ—¥å¿—
        self._setup_logging()
        
        # æ•°æ®ç›®å½•
        self.output_dir = Path("output")
        
    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        # æ§åˆ¶å°å¤„ç†å™¨ - åªè®°å½•é‡è¦ä¿¡æ¯
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(logging.Formatter('%(message)s'))
        
        # é…ç½®æ ¹æ—¥å¿—å™¨
        logging.basicConfig(
            level=logging.INFO,
            handlers=[console_handler]
        )
        self.logger = logging.getLogger(__name__)
        
    def calculate_md5(self, file_path: str) -> str:
        """
        è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        if not os.path.exists(file_path):
            return ""
            
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            self.logger.error(f"âŒ è®¡ç®—MD5å¤±è´¥ {file_path}: {e}")
            return ""
        
    def get_access_token(self) -> str:
        """
        è·å–Access Token
        
        Returns:
            access_tokenå­—ç¬¦ä¸²
        """
        # æ£€æŸ¥tokenæ˜¯å¦è¿‡æœŸ
        if (self.access_token and self.token_expires_at and 
            datetime.now() < self.token_expires_at):
            return self.access_token
            
        self.logger.info("æ­£åœ¨è·å–æ–°çš„Access Token...")
        
        params = {
            'grant_type': 'client_credential',
            'appid': self.app_id,
            'secret': self.app_secret
        }
        
        try:
            response = requests.get(self.token_url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            if 'access_token' in data:
                self.access_token = data['access_token']
                # æå‰5åˆ†é’Ÿè¿‡æœŸï¼Œé¿å…è¾¹ç•Œæƒ…å†µ
                expires_in = data.get('expires_in', 7200) - 300
                self.token_expires_at = datetime.now() + timedelta(seconds=expires_in)
                
                self.logger.info(f"Access Tokenè·å–æˆåŠŸï¼Œæœ‰æ•ˆæœŸè‡³: {self.token_expires_at}")
                return self.access_token
            else:
                raise Exception(f"è·å–Access Tokenå¤±è´¥: {data}")
                
        except Exception as e:
            self.logger.error(f"è·å–Access Tokenå‡ºé”™: {e}")
            raise
            
    def upload_file(self, local_path: str, cloud_path: str) -> Optional[str]:
        """
        ä¸Šä¼ æ–‡ä»¶åˆ°äº‘å­˜å‚¨
        
        Args:
            local_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
            cloud_path: äº‘å­˜å‚¨è·¯å¾„
            
        Returns:
            ä¸Šä¼ æˆåŠŸè¿”å›file_idï¼Œå¤±è´¥è¿”å›None
        """
        filename = os.path.basename(local_path)
        
        if not os.path.exists(local_path):
            self.logger.error(f"âŒ æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {local_path}")
            return None
            
        try:
            # 1. è·å–ä¸Šä¼ é“¾æ¥
            access_token = self.get_access_token()
            
            upload_data = {
                "env": self.env_id,
                "path": cloud_path
            }
            
            response = requests.post(
                f"{self.upload_file_url}?access_token={access_token}",
                json=upload_data,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            
            if result.get('errcode') != 0:
                self.logger.error(f"âŒ è·å–ä¸Šä¼ é“¾æ¥å¤±è´¥: {result}")
                return None
            
            # æ£€æŸ¥å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨
            required_fields = ['url', 'file_id']
            for field in required_fields:
                if field not in result:
                    self.logger.error(f"âŒ APIå“åº”ç¼ºå°‘å¿…è¦å­—æ®µ '{field}': {result}")
                    return None
                    
            upload_url = result['url']
            file_id = result['file_id']
            
            # 2. ä¸Šä¼ æ–‡ä»¶
            # æŒ‰ç…§å®˜æ–¹æ–‡æ¡£æ ¼å¼ï¼šPOST multipart/form-data
            with open(local_path, 'rb') as f:
                # æ„å»ºPOSTè¡¨å•ï¼Œä¸¥æ ¼æŒ‰ç…§å®˜æ–¹æ–‡æ¡£æ ¼å¼
                files = {
                    'key': (None, cloud_path),                              # è¯·æ±‚åŒ…ä¸­çš„ path å­—æ®µ  
                    'Signature': (None, result['authorization']),           # è¿”å›æ•°æ®çš„ authorization å­—æ®µ
                    'x-cos-security-token': (None, result['token']),        # è¿”å›æ•°æ®çš„ token å­—æ®µ
                    'x-cos-meta-fileid': (None, result['cos_file_id']),     # è¿”å›æ•°æ®çš„ cos_file_id å­—æ®µ
                    'file': (os.path.basename(local_path), f)               # æ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹
                }
                
                upload_response = requests.post(upload_url, files=files, timeout=60)
                upload_response.raise_for_status()
            
            self.logger.info(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {filename}")
            return file_id
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥ {os.path.basename(local_path)}: {e}")
            return None
        except KeyError as e:
            self.logger.error(f"âŒ APIå“åº”å­—æ®µç¼ºå¤± {os.path.basename(local_path)}: ç¼ºå°‘å­—æ®µ {e}")
            self.logger.error(f"å®Œæ•´APIå“åº”: {locals().get('result', 'APIå“åº”æœªè·å–')}")
            return None
        except Exception as e:
            self.logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥ {os.path.basename(local_path)}: {type(e).__name__}: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return None
            
    def add_database_records(self, collection: str, records: List[Dict]) -> bool:
        """
        æ‰¹é‡æ·»åŠ æ•°æ®åº“è®°å½•
        
        Args:
            collection: é›†åˆåç§°
            records: è®°å½•åˆ—è¡¨
            
        Returns:
            æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
        """
        if not records:
            return True
            
        try:
            access_token = self.get_access_token()
            
            # æ„é€ æ•°æ®åº“æŸ¥è¯¢è¯­å¥ - ä½¿ç”¨å®‰å…¨çš„å­—ç¬¦ä¸²å¤„ç†
            records_str = json.dumps(records, ensure_ascii=False)
            # è½¬ä¹‰JavaScriptä¸­çš„ç‰¹æ®Šå­—ç¬¦
            records_str_escaped = records_str.replace('\\', '\\\\').replace("'", "\\'").replace('\n', '\\n').replace('\r', '\\r')
            query = f"db.collection('{collection}').add({{data: {records_str_escaped}}})"
            
            data = {
                "env": self.env_id,
                "query": query
            }
            
            response = requests.post(
                f"{self.database_add_url}?access_token={access_token}",
                json=data,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            if result.get('errcode') != 0:
                self.logger.error(f"âŒ æ•°æ®åº“æ’å…¥å¤±è´¥: {result}")
                return False
                
            self.logger.info(f"âœ… æˆåŠŸæ’å…¥{len(records)}æ¡è®°å½•åˆ°{collection}é›†åˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
            return False
            
    def update_database_records(self, collection: str, record_id: str, update_data: Dict) -> bool:
        """
        æ›´æ–°æ•°æ®åº“è®°å½•
        
        Args:
            collection: é›†åˆåç§°
            record_id: è®°å½•ID
            update_data: æ›´æ–°æ•°æ®
            
        Returns:
            æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
        """
        if not update_data:
            return True
            
        try:
            access_token = self.get_access_token()
            
            # æ„é€ æ•°æ®åº“æ›´æ–°è¯­å¥
            # è¿‡æ»¤æ‰_idå­—æ®µï¼Œå› ä¸ºæ›´æ–°æ—¶ä¸èƒ½åŒ…å«_id
            filtered_data = {k: v for k, v in update_data.items() if k != '_id'}
            update_str = json.dumps(filtered_data, ensure_ascii=False)
            query = f"db.collection('{collection}').doc('{record_id}').update({{data: {update_str}}})"
            
            data = {
                "env": self.env_id,
                "query": query
            }
            
            response = requests.post(
                f"{self.database_update_url}?access_token={access_token}",
                json=data,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            if result.get('errcode') != 0:
                self.logger.error(f"âŒ æ•°æ®åº“æ›´æ–°å¤±è´¥: {result}")
                return False
                
            modified_count = result.get('modified', 0)
            if modified_count > 0:
                self.logger.info(f"âœ… æˆåŠŸæ›´æ–°è®°å½•: {record_id}")
            else:
                self.logger.warning(f"âš ï¸ è®°å½•æœªå‘ç”Ÿå˜åŒ–: {record_id}")
                
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®åº“æ›´æ–°å¤±è´¥: {e}")
            return False
            
    def query_database(self, collection: str, query_filter: Dict = None, skip: int = 0, limit: int = 20) -> List[Dict]:
        """
        æŸ¥è¯¢æ•°æ®åº“è®°å½•
        
        Args:
            collection: é›†åˆåç§°
            query_filter: æŸ¥è¯¢æ¡ä»¶ï¼Œå¦‚æœä¸ºNoneåˆ™æŸ¥è¯¢æ‰€æœ‰è®°å½•
            skip: è·³è¿‡çš„è®°å½•æ•°
            limit: é™åˆ¶è¿”å›çš„è®°å½•æ•°ï¼ˆæœ€å¤§20ï¼‰
            
        Returns:
            æŸ¥è¯¢ç»“æœåˆ—è¡¨
        """
        try:
            access_token = self.get_access_token()
            
            # æ„é€ æŸ¥è¯¢è¯­å¥
            if query_filter:
                filter_str = json.dumps(query_filter, ensure_ascii=False)
                query = f"db.collection('{collection}').where({filter_str}).skip({skip}).limit({limit}).get()"
            else:
                query = f"db.collection('{collection}').skip({skip}).limit({limit}).get()"
            
            data = {
                "env": self.env_id,
                "query": query
            }
            
            response = requests.post(
                f"{self.database_query_url}?access_token={access_token}",
                json=data,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            if result.get('errcode') != 0:
                self.logger.error(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {result}")
                return []
                
            # è§£ææŸ¥è¯¢ç»“æœ
            data_list = result.get('data', [])
            if isinstance(data_list, str):
                # å¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æJSON
                data_list = json.loads(data_list)
            
            # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®ï¼Œå¤„ç†åµŒå¥—JSONå­—ç¬¦ä¸²
            if isinstance(data_list, list):
                parsed_list = []
                for item in data_list:
                    if isinstance(item, str):
                        # å¦‚æœåˆ—è¡¨ä¸­çš„é¡¹æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºJSON
                        try:
                            parsed_item = json.loads(item)
                            parsed_list.append(parsed_item)
                        except json.JSONDecodeError:
                            # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œä¿æŒåŸæ ·
                            parsed_list.append(item)
                    else:
                        # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥æ·»åŠ 
                        parsed_list.append(item)
                return parsed_list
            else:
                return []
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
            return []
            
    def query_all_records(self, collection: str, query_filter: Dict = None) -> List[Dict]:
        """
        åˆ†æ‰¹æŸ¥è¯¢æ‰€æœ‰è®°å½•ï¼Œè‡ªåŠ¨å¤„ç†åˆ†é¡µ
        
        Args:
            collection: é›†åˆåç§°
            query_filter: æŸ¥è¯¢æ¡ä»¶ï¼Œå¦‚æœä¸ºNoneåˆ™æŸ¥è¯¢æ‰€æœ‰è®°å½•
            
        Returns:
            æ‰€æœ‰æŸ¥è¯¢ç»“æœçš„åˆ—è¡¨
        """
        all_records = []
        batch_size = 20  # æ¯æ‰¹20æ¡è®°å½•
        skip = 0
        
        self.logger.info(f"ğŸ” å¼€å§‹åˆ†æ‰¹æŸ¥è¯¢{collection}é›†åˆ...")
        
        while True:
            # æŸ¥è¯¢å½“å‰æ‰¹æ¬¡
            batch_records = self.query_database(collection, query_filter, skip, batch_size)
            
            if not batch_records:
                # æ²¡æœ‰æ›´å¤šè®°å½•ï¼Œé€€å‡ºå¾ªç¯
                break
                
            all_records.extend(batch_records)
            
            # å¦‚æœè¿”å›çš„è®°å½•æ•°å°‘äºæ‰¹æ¬¡å¤§å°ï¼Œè¯´æ˜å·²ç»æ˜¯æœ€åä¸€æ‰¹
            if len(batch_records) < batch_size:
                break
                
            skip += batch_size
            self.logger.info(f"ğŸ“„ å·²æŸ¥è¯¢{len(all_records)}æ¡è®°å½•...")
        
        self.logger.info(f"âœ… å®ŒæˆæŸ¥è¯¢{collection}é›†åˆï¼Œå…±{len(all_records)}æ¡è®°å½•")
        return all_records
            
    def compare_book_data(self, new_data: Dict, existing_data: Dict) -> Tuple[bool, List[str]]:
        """
        æ¯”è¾ƒä¹¦ç±æ•°æ®æ˜¯å¦éœ€è¦æ›´æ–°
        
        Args:
            new_data: æ–°çš„ä¹¦ç±æ•°æ®
            existing_data: æ•°æ®åº“ä¸­çš„ç°æœ‰æ•°æ®
            
        Returns:
            (éœ€è¦æ›´æ–°, å˜åŒ–å­—æ®µåˆ—è¡¨) å…ƒç»„
        """
        if not existing_data:
            return True, ["new_book"]
            
        # ç±»å‹æ£€æŸ¥å’Œè½¬æ¢
        if isinstance(existing_data, str):
            try:
                existing_data = json.loads(existing_data)
            except json.JSONDecodeError:
                self.logger.error(f"âŒ æ— æ³•è§£æç°æœ‰æ•°æ®: {existing_data}")
                return True, ["data_parse_error"]
        
        if not isinstance(existing_data, dict):
            self.logger.error(f"âŒ ç°æœ‰æ•°æ®æ ¼å¼é”™è¯¯: {type(existing_data)}")
            return True, ["data_format_error"]
        
        # éœ€è¦æ¯”è¾ƒçš„å­—æ®µåˆ—è¡¨
        compare_fields = [
            'title', 'author', 'description', 'category', 
            'total_chapters', 'total_duration', 'is_active', 
            'cover_md5', 'tags'
        ]
        
        changed_fields = []
        
        for field in compare_fields:
            new_value = new_data.get(field, '')
            existing_value = existing_data.get(field, '')
            
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒï¼Œé¿å…ç±»å‹é—®é¢˜
            new_str = str(new_value).strip()
            existing_str = str(existing_value).strip()
            if new_str != existing_str:
                changed_fields.append(field)
                # self.logger.info(f"ğŸ”„ ä¹¦ç±å­—æ®µå˜åŒ– [{field}]: {existing_str} â†’ {new_str}")
        
        needs_update = len(changed_fields) > 0
        return needs_update, changed_fields
        
    def compare_chapter_data(self, new_data: Dict, existing_data: Dict) -> Tuple[bool, List[str]]:
        """
        æ¯”è¾ƒç« èŠ‚æ•°æ®æ˜¯å¦éœ€è¦æ›´æ–°
        
        Args:
            new_data: æ–°çš„ç« èŠ‚æ•°æ®
            existing_data: æ•°æ®åº“ä¸­çš„ç°æœ‰æ•°æ®
            
        Returns:
            (éœ€è¦æ›´æ–°, å˜åŒ–å­—æ®µåˆ—è¡¨) å…ƒç»„
        """
        if not existing_data:
            return True, ["new_chapter"]
        
        # éœ€è¦æ¯”è¾ƒçš„å­—æ®µåˆ—è¡¨
        compare_fields = [
            'title', 'duration', 'is_active', 
            'audio_md5', 'subtitle_md5', 'chapter_number'
        ]
        
        changed_fields = []
        
        for field in compare_fields:
            new_value = new_data.get(field, '')
            existing_value = existing_data.get(field, '')
            
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒï¼Œé¿å…ç±»å‹é—®é¢˜
            new_str = str(new_value).strip()
            existing_str = str(existing_value).strip()
            if new_str != existing_str:
                changed_fields.append(field)
                # self.logger.info(f"ğŸ”„ ç« èŠ‚å­—æ®µå˜åŒ– [{field}]: {existing_str} â†’ {new_str}")
        
        needs_update = len(changed_fields) > 0
        return needs_update, changed_fields
        
    def parse_book_data(self, book_dir: Path) -> Tuple[Dict, List[Dict]]:
        """
        è§£æå•æœ¬ä¹¦çš„æ•°æ®
        
        Args:
            book_dir: ä¹¦ç±ç›®å½•è·¯å¾„
            
        Returns:
            (book_data, chapters_data) å…ƒç»„
        """
        book_id = book_dir.name
        meta_file = book_dir / "meta.json"
        
        if not meta_file.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°meta.jsonæ–‡ä»¶: {meta_file}")
            
        with open(meta_file, 'r', encoding='utf-8') as f:
            meta_data = json.load(f)
            
        book_info = meta_data['book']
        chapters_info = meta_data['chapters']
        
        # è®¡ç®—å°é¢æ–‡ä»¶MD5
        cover_file_path = os.path.join(book_dir, book_info.get('local_cover_file', ''))
        cover_md5 = self.calculate_md5(cover_file_path)
        
        # æ„é€ booksè¡¨æ•°æ®
        book_data = {
            '_id': book_id,
            'title': book_info['title'],
            'author': book_info.get('author', ''),
            'cover_url': '',  # ç¨åä¸Šä¼ å°é¢åå¡«å……
            'cover_md5': cover_md5,  # æ–°å¢å°é¢MD5å­—æ®µ
            'category': book_info.get('category', ''),  # é»˜è®¤æ–‡å­¦ç±»
            'description': book_info.get('description', ''),
            'total_chapters': book_info.get('total_chapters', 0),
            'total_duration': int(book_info.get('total_duration', 0)), 
            'is_active': True,
            'tags': book_info.get('tags', []),
            'local_cover_file': cover_file_path,
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat(),
            'done': book_info.get('done', False)
        }
        
        # æ„é€ chaptersè¡¨æ•°æ®
        chapters_data = []
        
        for i, chapter_info in enumerate(chapters_info):
            # è®¡ç®—éŸ³é¢‘æ–‡ä»¶MD5
            audio_file_path = os.path.join(book_dir, chapter_info.get('local_audio_file', ''))
            audio_md5 = self.calculate_md5(audio_file_path)
            
            # è®¡ç®—å­—å¹•æ–‡ä»¶MD5
            subtitle_file_path = os.path.join(book_dir, chapter_info.get('local_subtitle_file', ''))
            subtitle_md5 = self.calculate_md5(subtitle_file_path)

            # ä»éŸ³é¢‘æ–‡ä»¶è·¯å¾„æå–å­ç« èŠ‚æ–‡ä»¶åä½œä¸ºID
            audio_filename = os.path.basename(chapter_info.get('local_audio_file', ''))
            subchapter_name = os.path.splitext(audio_filename)[0]  # å»æ‰æ‰©å±•å
            
            chapter_data = {
                '_id': f"{book_id}_{subchapter_name}",
                'book_id': book_id,
                'chapter_number': chapter_info['chapter_number'],
                'title': chapter_info['title_cn'] or chapter_info['title'],
                'duration': int(chapter_info['duration']) if chapter_info['duration'] else 0,
                'is_active': True,
                'audio_url': '',  # ç¨åä¸Šä¼ éŸ³é¢‘åå¡«å……
                'audio_md5': audio_md5,  # æ–°å¢éŸ³é¢‘MD5å­—æ®µ
                'subtitle_url': '',  # ç¨åä¸Šä¼ å­—å¹•åå¡«å……
                'subtitle_md5': subtitle_md5,  # æ–°å¢å­—å¹•MD5å­—æ®µ
                'local_audio_file': audio_file_path,
                'local_subtitle_file': subtitle_file_path,
                'created_at': datetime.now().isoformat(),
                'updated_at': datetime.now().isoformat()
            }
            
            chapters_data.append(chapter_data)
            
        return book_data, chapters_data
        
    def upload_book_cover(self, book_dir: Path, book_id: str, book_data: Dict) -> str:
        """
        ä¸Šä¼ ä¹¦ç±å°é¢æ–‡ä»¶
        
        Args:
            book_dir: ä¹¦ç±ç›®å½•
            book_id: ä¹¦ç±ID
            
        Returns:
            cover_file_id
        """
        # ä¸Šä¼ å°é¢
        cover_file = book_data.get('local_cover_file', '')
        if os.path.exists(cover_file):
            cloud_path = f"books/{book_id}/cover.jpg"
            cover_file_id = self.upload_file(str(cover_file), cloud_path)
        else:
            self.logger.warning(f"âš ï¸  å°é¢æ–‡ä»¶ä¸å­˜åœ¨: {cover_file}")
            cover_file_id = ""

        return cover_file_id
        
    def upload_book_if_needed(self, book_dir: Path, book_data: Dict, existing_book: Dict, changed_fields: List[str]) -> bool:
        """
        æ ¹æ®å˜åŒ–å­—æ®µå†³å®šæ˜¯å¦ä¸Šä¼ ä¹¦ç±ç›¸å…³æ–‡ä»¶
        
        Args:
            book_dir: ä¹¦ç±ç›®å½•
            book_data: ä¹¦ç±æ•°æ®
            existing_book: æ•°æ®åº“ä¸­çš„ç°æœ‰ä¹¦ç±æ•°æ®
            changed_fields: å‘ç”Ÿå˜åŒ–çš„å­—æ®µåˆ—è¡¨
            
        Returns:
            ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        book_id = book_data['_id']
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸Šä¼ å°é¢
        if 'cover_md5' in changed_fields or not existing_book:
            cover_file_id = self.upload_book_cover(book_dir, book_id, book_data)
            if cover_file_id:
                book_data['cover_url'] = cover_file_id
                return True
            elif not existing_book:
                # æ–°ä¹¦ä½†å°é¢ä¸Šä¼ å¤±è´¥
                return False
            else:
                # æ›´æ–°ä¹¦ç±ä½†å°é¢ä¸Šä¼ å¤±è´¥ï¼Œä¿ç•™åŸURL
                book_data['cover_url'] = existing_book.get('cover_url', '')
                return True
        else:
            # å°é¢æ²¡æœ‰å˜åŒ–ï¼Œä¿ç•™åŸURL
            book_data['cover_url'] = existing_book.get('cover_url', '')
            return True
            
    def upload_chapter_if_needed(self, book_dir: Path, book_id: str, chapter_data: Dict, existing_chapter: Dict, changed_fields: List[str]) -> bool:
        """
        æ ¹æ®å˜åŒ–å­—æ®µå†³å®šæ˜¯å¦ä¸Šä¼ ç« èŠ‚ç›¸å…³æ–‡ä»¶
        
        Args:
            book_dir: ä¹¦ç±ç›®å½•
            book_id: ä¹¦ç±ID
            chapter_data: ç« èŠ‚æ•°æ®
            existing_chapter: æ•°æ®åº“ä¸­çš„ç°æœ‰ç« èŠ‚æ•°æ®
            changed_fields: å‘ç”Ÿå˜åŒ–çš„å­—æ®µåˆ—è¡¨
            
        Returns:
            ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        audio_file_id = ""
        subtitle_file_id = ""
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
        if 'audio_md5' in changed_fields or not existing_chapter:
            audio_file = chapter_data.get('local_audio_file', '')
            if audio_file and os.path.exists(audio_file):
                audio_filename = os.path.basename(audio_file)
                audio_cloud_path = f"books/{book_id}/audio/{audio_filename}"
                audio_file_id = self.upload_file(audio_file, audio_cloud_path)
                if not audio_file_id and not existing_chapter:
                    return False  # æ–°ç« èŠ‚éŸ³é¢‘ä¸Šä¼ å¤±è´¥
        else:
            # éŸ³é¢‘æ²¡æœ‰å˜åŒ–ï¼Œä¿ç•™åŸURL
            audio_file_id = existing_chapter.get('audio_url', '')
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸Šä¼ å­—å¹•æ–‡ä»¶
        if 'subtitle_md5' in changed_fields or not existing_chapter:
            subtitle_file = chapter_data.get('local_subtitle_file', '')
            if subtitle_file and os.path.exists(subtitle_file):
                subtitle_filename = os.path.basename(subtitle_file)
                subtitle_cloud_path = f"books/{book_id}/subtitles/{subtitle_filename}"
                subtitle_file_id = self.upload_file(subtitle_file, subtitle_cloud_path)
        else:
            # å­—å¹•æ²¡æœ‰å˜åŒ–ï¼Œä¿ç•™åŸURL
            subtitle_file_id = existing_chapter.get('subtitle_url', '')
        
        # æ›´æ–°ç« èŠ‚æ–‡ä»¶URL
        if audio_file_id:
            chapter_data['audio_url'] = audio_file_id
        if subtitle_file_id:
            chapter_data['subtitle_url'] = subtitle_file_id
            
        return True
        
    def process_single_chapter(self, book_dir: Path, book_id: str, chapter_data: Dict, existing_chapters_dict: Dict, stats: Dict) -> bool:
        """
        å¤„ç†å•ä¸ªç« èŠ‚çš„ä¸Šä¼ å’Œæ•°æ®åº“æ›´æ–°
        
        Args:
            book_dir: ä¹¦ç±ç›®å½•
            book_id: ä¹¦ç±ID
            chapter_data: ç« èŠ‚æ•°æ®
            existing_chapters_dict: ç°æœ‰ç« èŠ‚æ•°æ®å­—å…¸
            stats: ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        chapter_id = chapter_data['_id']
        existing_chapter = existing_chapters_dict.get(chapter_id)
        
        # æ¯”è¾ƒç« èŠ‚æ•°æ®æ˜¯å¦éœ€è¦æ›´æ–°
        needs_update, changed_fields = self.compare_chapter_data(chapter_data, existing_chapter)
        
        if not needs_update:
            stats['skipped_chapters'] += 1
            self.logger.info(f"â­ï¸ ç« èŠ‚æ— å˜åŒ–ï¼Œè·³è¿‡: {chapter_data['title']}")
            return True
        
        # è®°å½•å˜åŒ–ä¿¡æ¯
        if existing_chapter:
            stats['updated_chapters'] += 1
            self.logger.info(f"ğŸ“ æ£€æµ‹åˆ°ç« èŠ‚å˜åŒ– {chapter_data['title']}: {', '.join(changed_fields)}")
        else:
            stats['new_chapters'] += 1
            self.logger.info(f"ğŸ†• æ–°ç« èŠ‚: {chapter_data['title']}")
        
        # ä¸Šä¼ ç« èŠ‚æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not self.upload_chapter_if_needed(book_dir, book_id, chapter_data, existing_chapter, changed_fields):
            self.logger.error(f"âŒ ç« èŠ‚æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {chapter_data['title']}")
            return False
        
        # æ›´æ–°æ—¶é—´æˆ³
        chapter_data['updated_at'] = datetime.now().isoformat()
        if existing_chapter:
            chapter_data['created_at'] = existing_chapter.get('created_at', chapter_data['created_at'])
        
        # æ¸…ç†æœ¬åœ°æ–‡ä»¶è·¯å¾„
        chapter_clean = chapter_data.copy()
        del chapter_clean['local_audio_file']
        del chapter_clean['local_subtitle_file']
        
        # æ’å…¥æˆ–æ›´æ–°ç« èŠ‚æ•°æ®
        self.logger.info(f"ğŸ” ç« èŠ‚ID: {chapter_id}, æ˜¯å¦å­˜åœ¨: {existing_chapter is not None}")
        if existing_chapter:
            # æ›´æ–°ç°æœ‰è®°å½•
            self.logger.info(f"ğŸ’¾ æ›´æ–°ç« èŠ‚æ•°æ®: {chapter_data['title']}")
            if not self.update_database_records('chapters', chapter_id, chapter_clean):
                self.logger.error(f"âŒ ç« èŠ‚æ•°æ®æ›´æ–°å¤±è´¥: {chapter_data['title']}")
                return False
        else:
            # æ’å…¥æ–°è®°å½•
            self.logger.info(f"ğŸ’¾ æ’å…¥ç« èŠ‚æ•°æ®: {chapter_data['title']}")
            if not self.add_database_records('chapters', [chapter_clean]):
                self.logger.error(f"âŒ ç« èŠ‚æ•°æ®æ’å…¥å¤±è´¥: {chapter_data['title']}")
                return False
        
        return True
        
    def process_single_book(self, book_dir: Path, stats: Dict) -> bool:
        """
        å¤„ç†å•æœ¬ä¹¦çš„ä¸Šä¼ å’Œæ•°æ®åº“æ›´æ–°
        
        Args:
            book_dir: ä¹¦ç±ç›®å½•
            stats: ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        book_id = book_dir.name
        self.logger.info(f"\nğŸ“– å¤„ç†ä¹¦ç±: {book_id}")
        
        try:
            # è§£æä¹¦ç±æ•°æ®
            book_data, chapters_data = self.parse_book_data(book_dir)

            # å¦‚æœä¹¦ç±å·²å¤„ç†å®Œæˆï¼Œåˆ™è·³è¿‡
            if book_data and book_data['done']:
                stats['skipped_books'] += 1
                stats['skipped_chapters'] += len(chapters_data)
                self.logger.info(f"â­ï¸ ä¹¦ç±å·²å¤„ç†å®Œæˆï¼Œè·³è¿‡: {book_id}")
                return True
            
            # æŸ¥è¯¢æ•°æ®åº“ä¸­çš„ä¹¦ç±è®°å½•
            existing_books = self.query_database('books', {'_id': book_id})
            existing_book = existing_books[0] if existing_books else None
            
            # æ¯”è¾ƒä¹¦ç±æ•°æ®æ˜¯å¦éœ€è¦æ›´æ–°
            book_needs_update, changed_fields = self.compare_book_data(book_data, existing_book)
            
            # å¤„ç†ä¹¦ç±æ•°æ®
            if book_needs_update:
                # è®°å½•å˜åŒ–ä¿¡æ¯
                if existing_book:
                    stats['updated_books'] += 1
                    self.logger.info(f"ğŸ“š æ£€æµ‹åˆ°ä¹¦ç±å˜åŒ– {book_data['title']}: {', '.join(changed_fields)}")
                else:
                    stats['new_books'] += 1
                    self.logger.info(f"ğŸ†• æ–°ä¹¦ç±: {book_data['title']}")
                
                # ä¸Šä¼ ä¹¦ç±æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if not self.upload_book_if_needed(book_dir, book_data, existing_book, changed_fields):
                    self.logger.error(f"âŒ ä¹¦ç±æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {book_id}")
                    return False
                
                # æ›´æ–°æ—¶é—´æˆ³
                book_data['updated_at'] = datetime.now().isoformat()
                if existing_book:
                    book_data['created_at'] = existing_book.get('created_at', book_data['created_at'])
                
                # æ¸…ç†æœ¬åœ°æ–‡ä»¶è·¯å¾„
                book_data_clean = book_data.copy()
                del book_data_clean['local_cover_file']
                
                # æ’å…¥æˆ–æ›´æ–°ä¹¦ç±æ•°æ®
                if existing_book:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    self.logger.info(f"ğŸ’¾ æ›´æ–°ä¹¦ç±æ•°æ®: {book_data['title']}")
                    if not self.update_database_records('books', book_id, book_data_clean):
                        self.logger.error(f"âŒ ä¹¦ç±æ•°æ®æ›´æ–°å¤±è´¥: {book_id}")
                        return False
                else:
                    # æ’å…¥æ–°è®°å½•
                    self.logger.info(f"ğŸ’¾ æ’å…¥ä¹¦ç±æ•°æ®: {book_data['title']}")
                    if not self.add_database_records('books', [book_data_clean]):
                        self.logger.error(f"âŒ ä¹¦ç±æ•°æ®æ’å…¥å¤±è´¥: {book_id}")
                        return False
            else:
                stats['skipped_books'] += 1
                self.logger.info(f"â­ï¸ ä¹¦ç±æ— å˜åŒ–ï¼Œè·³è¿‡: {book_data['title']}")
            
            # æŸ¥è¯¢æ•°æ®åº“ä¸­çš„ç« èŠ‚è®°å½•ï¼ˆåˆ†æ‰¹æŸ¥è¯¢ï¼‰
            existing_chapters = self.query_all_records('chapters', {'book_id': book_id})
            existing_chapters_dict = {ch['_id']: ch for ch in existing_chapters}
            
            # å¤„ç†ç« èŠ‚æ•°æ®
            for i, chapter in enumerate(chapters_data):
                self.logger.info(f"ğŸ“ å¤„ç†ç« èŠ‚ {i+1}/{len(chapters_data)}: {chapter['title']}")
                if not self.process_single_chapter(book_dir, book_id, chapter, existing_chapters_dict, stats):
                    return False

            # æ¸…ç†äº‘ç«¯å¤šä½™çš„ç« èŠ‚æ•°æ®
            self.logger.info(f"ğŸ§¹ æ£€æŸ¥å¹¶æ¸…ç†äº‘ç«¯å¤šä½™çš„ç« èŠ‚æ•°æ®...")
            local_chapter_ids = set(chapter['_id'] for chapter in chapters_data)
            if not self.cleanup_orphaned_chapters(book_id, local_chapter_ids, existing_chapters_dict):
                self.logger.warning(f"âš ï¸ ç« èŠ‚æ•°æ®æ¸…ç†éƒ¨åˆ†å¤±è´¥")
            
            # å¤„ç†è¯æ±‡æ•°æ®
            self.logger.info(f"ğŸ“š å¼€å§‹å¤„ç†è¯æ±‡æ•°æ®...")
            if not self.upload_vocabularies(book_dir, book_id):
                self.logger.error(f"âŒ è¯æ±‡æ•°æ®ä¸Šä¼ å¤±è´¥: {book_id}")
                return False
            
            self.logger.info(f"âœ… ä¹¦ç±{book_id}å¤„ç†å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ å¤„ç†ä¹¦ç±{book_dir.name}å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False
            
    def print_upload_statistics(self, stats: Dict) -> None:
        """
        æ‰“å°ä¸Šä¼ ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        self.logger.info(f"\nğŸ“Š ä¸Šä¼ ç»Ÿè®¡:")
        self.logger.info(f"   æ€»ä¹¦ç±æ•°: {stats['total_books']}")
        self.logger.info(f"   æ–°å¢ä¹¦ç±: {stats['new_books']}")
        self.logger.info(f"   æ›´æ–°ä¹¦ç±: {stats['updated_books']}")
        self.logger.info(f"   è·³è¿‡ä¹¦ç±: {stats['skipped_books']}")
        self.logger.info(f"   æ–°å¢ç« èŠ‚: {stats['new_chapters']}")
        self.logger.info(f"   æ›´æ–°ç« èŠ‚: {stats['updated_chapters']}")
        self.logger.info(f"   è·³è¿‡ç« èŠ‚: {stats['skipped_chapters']}")
        
    def upload_all_books(self) -> bool:
        """
        æ™ºèƒ½ä¸Šä¼ æ‰€æœ‰ä¹¦ç±æ•°æ®ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰
        
        Returns:
            æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
        """
        # éªŒè¯è¾“å‡ºç›®å½•
        if not self.output_dir.exists():
            self.logger.error(f"âŒ è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {self.output_dir}")
            return False
            
        # è·å–ä¹¦ç±ç›®å½•åˆ—è¡¨
        book_dirs = [d for d in self.output_dir.iterdir() 
                    if d.is_dir() and (d / "meta.json").exists()]
                    
        if not book_dirs:
            self.logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•åŒ…å«meta.jsonçš„ä¹¦ç±ç›®å½•")
            return False
            
        self.logger.info(f"ğŸ“š å‘ç°{len(book_dirs)}æœ¬ä¹¦ç±: {[d.name for d in book_dirs]}")
        
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_books': len(book_dirs),
            'new_books': 0,
            'updated_books': 0,
            'skipped_books': 0,
            'new_chapters': 0,
            'updated_chapters': 0,
            'skipped_chapters': 0
        }
        
        # å¤„ç†æ‰€æœ‰ä¹¦ç±
        success = True
        for book_dir in book_dirs:
            if not self.process_single_book(book_dir, stats):
                success = False
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self.print_upload_statistics(stats)
                    
        return success
    
    def parse_vocabulary_data(self, master_vocab_path: str) -> Dict[str, Dict]:
        """è§£ææ€»è¯æ±‡è¡¨æ•°æ®å¹¶è½¬æ¢ä¸ºæ•°æ®åº“æ ¼å¼"""
        vocabulary_data = {}
        
        if not os.path.exists(master_vocab_path):
            self.logger.warning(f"âš ï¸ æ€»è¯æ±‡è¡¨ä¸å­˜åœ¨: {master_vocab_path}")
            return vocabulary_data
            
        try:
            with open(master_vocab_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        word_data = json.loads(line.strip())
                        
                        # è§£ætagså­—ç¬¦ä¸²ä¸ºæ•°ç»„
                        tags_str = word_data.get('tags', '')
                        tags = tags_str.split() if tags_str else []
                        
                        # è§£ætranslationå­—ç¬¦ä¸²ä¸ºå¯¹è±¡æ•°ç»„
                        translation_str = word_data.get('translation', '')
                        translation = self._parse_translation(translation_str)
                        
                        # è§£æexchangeå­—ç¬¦ä¸²ä¸ºå¯¹è±¡æ•°ç»„
                        exchange_str = word_data.get('exchange', '')
                        exchange = self._parse_exchange(exchange_str)
                        
                        # æ„é€ æ•°æ®åº“æ ¼å¼
                        db_word_data = {
                            '_id': word_data['word'],
                            'word': word_data['word'],
                            'phonetic': word_data.get('phonetic', ''),
                            'translation': translation,
                            'tags': tags,
                            'exchange': exchange,
                            'bnc': word_data.get('bnc', 0),
                            'frq': word_data.get('frq', 0),
                            'audio_url': word_data.get('audio', ''),
                            'created_at': datetime.now().isoformat(),
                            'updated_at': datetime.now().isoformat()
                        }
                        
                        vocabulary_data[word_data['word']] = db_word_data
                        
        except Exception as e:
            self.logger.error(f"âŒ è§£æè¯æ±‡æ•°æ®å¤±è´¥: {e}")
            
        return vocabulary_data
    
    def _parse_translation(self, translation_str: str) -> List[Dict]:
        """è§£æç¿»è¯‘å­—ç¬¦ä¸²ä¸ºå¯¹è±¡æ•°ç»„"""
        if not translation_str:
            return []
            
        translations = []
        # æŒ‰æ¢è¡Œç¬¦åˆ†å‰²ä¸åŒå«ä¹‰
        parts = translation_str.split('\\n')
        
        for part in parts:
            part = part.strip()
            if not part:
                continue
                
            # åŒ¹é…è¯æ€§å’Œå«ä¹‰ (å¦‚: "n. æ„å¤–äº‹ä»¶, æœºé‡")
            match = re.match(r'^([a-z]+\.)\s*(.+)$', part)
            if match:
                pos_type = match.group(1)
                meaning = match.group(2)
                translations.append({
                    'type': pos_type,
                    'meaning': meaning,
                    'example': ''
                })
            else:
                # å¦‚æœæ²¡æœ‰è¯æ€§æ ‡è®°ï¼Œä½œä¸ºé€šç”¨ç¿»è¯‘
                translations.append({
                    'type': '',
                    'meaning': part,
                    'example': ''
                })
                
        return translations
    
    def _parse_exchange(self, exchange_str: str) -> List[Dict]:
        """è§£æè¯å½¢å˜åŒ–å­—ç¬¦ä¸²ä¸ºå¯¹è±¡æ•°ç»„"""
        if not exchange_str:
            return []
            
        exchanges = []
        # exchangeæ ¼å¼é€šå¸¸æ˜¯: "p:past/d:done/i:doing"
        if ':' in exchange_str:
            parts = exchange_str.split('/')
            for part in parts:
                if ':' in part:
                    type_code, form = part.split(':', 1)
                    exchanges.append({
                        'type': type_code.strip(),
                        'form': form.strip()
                    })
        
        return exchanges
    
    def upload_vocabularies(self, book_dir: Path, book_id: str) -> bool:
        """ä¸Šä¼ å½“å‰ä¹¦ç±çš„è¯æ±‡æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            # 1. å…ˆä»ç« èŠ‚è¯æ±‡æ–‡ä»¶ä¸­æ”¶é›†å½“å‰ä¹¦ç±çš„æ‰€æœ‰å•è¯
            vocab_subchapters_dir = book_dir / "vocabulary" / "subchapters"
            if not vocab_subchapters_dir.exists():
                self.logger.warning(f"âš ï¸ ç« èŠ‚è¯æ±‡ç›®å½•ä¸å­˜åœ¨: {vocab_subchapters_dir}ï¼Œè·³è¿‡è¯æ±‡ä¸Šä¼ ")
                return True
                
            chapter_vocab_files = list(vocab_subchapters_dir.glob("*.json"))
            if not chapter_vocab_files:
                self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ç« èŠ‚è¯æ±‡æ–‡ä»¶ï¼Œè·³è¿‡è¯æ±‡ä¸Šä¼ ")
                return True
            
            # æ”¶é›†å½“å‰ä¹¦ç±çš„æ‰€æœ‰å•è¯ï¼ˆä¿æŒé¡ºåºï¼‰
            book_words = []
            seen_words = set()
            for vocab_file in chapter_vocab_files:
                try:
                    with open(vocab_file, 'r', encoding='utf-8') as f:
                        chapter_data = json.load(f)
                    words = chapter_data.get('words', [])
                    # ä¿æŒé¡ºåºï¼Œä½†å»é‡
                    for word in words:
                        if word not in seen_words:
                            book_words.append(word)
                            seen_words.add(word)
                except Exception as e:
                    self.logger.warning(f"âš ï¸ è¯»å–ç« èŠ‚è¯æ±‡æ–‡ä»¶å¤±è´¥: {vocab_file}, {e}")
                    continue
            
            if not book_words:
                self.logger.warning(f"âš ï¸ å½“å‰ä¹¦ç±æ²¡æœ‰è¯æ±‡æ•°æ®")
                return True
                
            self.logger.info(f"ğŸ“ å½“å‰ä¹¦ç±åŒ…å« {len(book_words)} ä¸ªå”¯ä¸€å•è¯")
            
            # 2. ä»æ€»è¯æ±‡è¡¨ä¸­ç­›é€‰å½“å‰ä¹¦ç±çš„å•è¯ä¿¡æ¯
            master_vocab_path = Path(os.path.join(program_root, "output/vocabulary/master_vocabulary.json"))
            if not master_vocab_path.exists():
                self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ€»è¯æ±‡è¡¨æ–‡ä»¶: {master_vocab_path}ï¼Œè·³è¿‡è¯æ±‡ä¸Šä¼ ")
                return True
            
            vocabulary_data = self.parse_vocabulary_data(str(master_vocab_path))
            if not vocabulary_data:
                self.logger.warning(f"âš ï¸ æ€»è¯æ±‡è¡¨ä¸ºç©º")
                return True
            
            # ç­›é€‰å‡ºå½“å‰ä¹¦ç±éœ€è¦çš„å•è¯
            book_vocabulary_data = {}
            for word in book_words:
                if word in vocabulary_data:
                    book_vocabulary_data[word] = vocabulary_data[word]
                    
            self.logger.info(f"ğŸ“ ä»æ€»è¯æ±‡è¡¨ä¸­ç­›é€‰å‡º {len(book_vocabulary_data)} ä¸ªå•è¯ä¿¡æ¯")
            
            # 3. å…ˆæŸ¥è¯¢åæ’å…¥ï¼Œé¿å…é‡å¤é”®é”™è¯¯
            filtered_words = list(book_vocabulary_data.values())
            if filtered_words:
                self.logger.info(f"ğŸ“ å‡†å¤‡ä¸Šä¼  {len(filtered_words)} ä¸ªå•è¯åˆ°vocabulariesè¡¨...")
                
                # é€ä¸ªæŸ¥è¯¢å’Œæ’å…¥å•è¯
                success_count = 0
                skip_count = 0
                
                self.logger.info(f"ğŸ“ å¼€å§‹é€ä¸ªå¤„ç† {len(filtered_words)} ä¸ªå•è¯...")
                
                for idx, word_data in enumerate(filtered_words):
                    word = word_data['word']
                    
                    # æŸ¥è¯¢å•è¯æ˜¯å¦å·²å­˜åœ¨
                    existing_word = self.query_database('vocabularies', {'word': word}, limit=1)
                    
                    if existing_word:
                        skip_count += 1
                        if (idx + 1) % 10 == 0:  # æ¯å¤„ç†50ä¸ªå•è¯æ‰“å°ä¸€æ¬¡è¿›åº¦
                            self.logger.info(f"ğŸ“ è¿›åº¦: {idx + 1}/{len(filtered_words)}, æ–°å¢: {success_count}, è·³è¿‡: {skip_count}")
                        continue
                    
                    # å¦‚æœä¸å­˜åœ¨ï¼Œç«‹å³æ’å…¥
                    retry_count = 0
                    max_retries = 3
                    
                    while retry_count < max_retries:
                        try:
                            if self.add_database_records('vocabularies', [word_data]):
                                success_count += 1
                                break
                            else:
                                retry_count += 1
                                if retry_count < max_retries:
                                    self.logger.warning(f"âš ï¸ å•è¯ '{word}' æ’å…¥å¤±è´¥ï¼Œé‡è¯• {retry_count}/{max_retries}...")
                                    time.sleep(1)
                                else:
                                    self.logger.error(f"âŒ å•è¯ '{word}' æ’å…¥å¤±è´¥ï¼Œè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°")
                        except Exception as e:
                            retry_count += 1
                            if retry_count < max_retries:
                                self.logger.warning(f"âš ï¸ å•è¯ '{word}' ç½‘ç»œé”™è¯¯ï¼Œé‡è¯• {retry_count}/{max_retries}: {e}")
                                time.sleep(2)
                            else:
                                self.logger.error(f"âŒ å•è¯ '{word}' æ’å…¥å¤±è´¥ï¼Œç½‘ç»œé”™è¯¯: {e}")
                                break
                    
                    # æ‰“å°è¿›åº¦ï¼ˆæ¯å¤„ç†50ä¸ªå•è¯ï¼‰
                    if (idx + 1) % 50 == 0:
                        self.logger.info(f"ğŸ“ è¿›åº¦: {idx + 1}/{len(filtered_words)}, æ–°å¢: {success_count}, è·³è¿‡: {skip_count}")
                
                self.logger.info(f"âœ… è¯æ±‡ä¸Šä¼ å®Œæˆ: æ–°å¢ {success_count} ä¸ª, è·³è¿‡ {skip_count} ä¸ªå·²å­˜åœ¨")
            else:
                self.logger.info(f"ğŸ“ æ²¡æœ‰è¯æ±‡éœ€è¦ä¸Šä¼ ")
            
            # ä¸Šä¼ ç« èŠ‚è¯æ±‡å…³è”
            return self.upload_chapter_vocabularies(book_dir, book_id, book_vocabulary_data)
            
        except Exception as e:
            self.logger.error(f"âŒ è¯æ±‡ä¸Šä¼ å¤±è´¥: {e}")
            return False
    
    def upload_chapter_vocabularies(self, book_dir: Path, book_id: str, vocabulary_data: Dict) -> bool:
        """ä¸Šä¼ ç« èŠ‚è¯æ±‡å…³è”æ•°æ®"""
        try:
            vocab_subchapters_dir = book_dir / "vocabulary" / "subchapters"
            if not vocab_subchapters_dir.exists():
                self.logger.warning(f"âš ï¸ ç« èŠ‚è¯æ±‡ç›®å½•ä¸å­˜åœ¨: {vocab_subchapters_dir}")
                return True
                
            # è¯»å–æ‰€æœ‰ç« èŠ‚è¯æ±‡æ–‡ä»¶
            chapter_vocab_files = list(vocab_subchapters_dir.glob("*.json"))
            if not chapter_vocab_files:
                self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ç« èŠ‚è¯æ±‡æ–‡ä»¶")
                return True
                
            self.logger.info(f"ğŸ“ å¤„ç† {len(chapter_vocab_files)} ä¸ªç« èŠ‚çš„è¯æ±‡å…³è”...")
            
            chapter_vocab_records = []
            for vocab_file in chapter_vocab_files:
                try:
                    with open(vocab_file, 'r', encoding='utf-8') as f:
                        chapter_data = json.load(f)
                        
                    subchapter_id = chapter_data.get('subchapter_id', '')
                    words = chapter_data.get('words', [])
                    
                    if not subchapter_id or not words:
                        continue
                    
                    # ç”Ÿæˆç»Ÿä¸€çš„chapter_idæ ¼å¼: book_id + å­ç« èŠ‚æ–‡ä»¶å
                    chapter_id = f"{book_id}_{subchapter_id}"
                    
                    # è¿‡æ»¤å‡ºå­˜åœ¨äºè¯æ±‡è¡¨ä¸­çš„å•è¯ï¼ˆä¿æŒåŸå§‹é¡ºåºï¼‰
                    filtered_words = [word for word in words if word in vocabulary_data]
                    
                    if filtered_words:
                        # åˆ›å»ºç« èŠ‚è¯æ±‡è®°å½•ï¼ˆæ–°è¡¨ç»“æ„ï¼šæ¯ä¸ªç« èŠ‚ä¸€æ¡è®°å½•ï¼‰
                        record = {
                            '_id': chapter_id,
                            'book_id': book_id,
                            'chapter_id': chapter_id,
                            'words': filtered_words,  # å•è¯æ•°ç»„ï¼Œä¿æŒé¡ºåº
                            'created_at': datetime.now().isoformat()
                        }
                        chapter_vocab_records.append(record)
                            
                except Exception as e:
                    self.logger.error(f"âŒ å¤„ç†ç« èŠ‚è¯æ±‡æ–‡ä»¶å¤±è´¥ {vocab_file}: {e}")
                    continue
            
            # æ‰¹é‡ä¸Šä¼ ç« èŠ‚è¯æ±‡å…³è”
            if chapter_vocab_records:
                self.logger.info(f"ğŸ“ ä¸Šä¼  {len(chapter_vocab_records)} æ¡ç« èŠ‚è¯æ±‡å…³è”...")
                
                # åˆ†æ‰¹ä¸Šä¼ 
                batch_size = 100
                for i in range(0, len(chapter_vocab_records), batch_size):
                    batch = chapter_vocab_records[i:i + batch_size]
                    if not self.add_database_records('chapter_vocabularies', batch):
                        self.logger.error(f"âŒ ç« èŠ‚è¯æ±‡å…³è”æ‰¹æ¬¡ä¸Šä¼ å¤±è´¥")
                        return False
                        
                self.logger.info(f"âœ… ç« èŠ‚è¯æ±‡å…³è”ä¸Šä¼ å®Œæˆ")
            else:
                self.logger.warning(f"âš ï¸ æ²¡æœ‰ç« èŠ‚è¯æ±‡å…³è”æ•°æ®éœ€è¦ä¸Šä¼ ")
                
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ç« èŠ‚è¯æ±‡å…³è”ä¸Šä¼ å¤±è´¥: {e}")
            return False
    
    def delete_database_record(self, collection: str, record_id: str) -> bool:
        """åˆ é™¤æ•°æ®åº“è®°å½•"""
        try:
            access_token = self.get_access_token()
            
            # æ„é€ åˆ é™¤æŸ¥è¯¢è¯­å¥ï¼ˆå‚è€ƒå¾®ä¿¡å®˜æ–¹æ–‡æ¡£æ ¼å¼ï¼‰
            query = f"db.collection('{collection}').where({{_id: '{record_id}'}}).remove()"
            
            data = {
                "env": self.env_id,
                "query": query
            }
            
            response = requests.post(
                f"{self.database_delete_url}?access_token={access_token}",
                json=data,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            if result.get('errcode') == 0:
                deleted_count = result.get('deleted', 0)
                if deleted_count > 0:
                    self.logger.info(f"âœ… åˆ é™¤è®°å½•æˆåŠŸ: {collection}.{record_id}")
                else:
                    self.logger.warning(f"âš ï¸ è®°å½•ä¸å­˜åœ¨æˆ–å·²åˆ é™¤: {collection}.{record_id}")
                return True
            else:
                self.logger.error(f"âŒ åˆ é™¤è®°å½•å¤±è´¥: {collection}.{record_id}, {result}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ åˆ é™¤è®°å½•å¼‚å¸¸: {collection}.{record_id}, {e}")
            return False

    def cleanup_orphaned_chapters(self, book_id: str, local_chapter_ids: set, existing_chapters_dict: dict) -> bool:
        """æ¸…ç†äº‘ç«¯å¤šä½™çš„ç« èŠ‚æ•°æ®"""
        try:
            # æ‰¾å‡ºäº‘ç«¯å­˜åœ¨ä½†æœ¬åœ°ä¸å­˜åœ¨çš„ç« èŠ‚
            cloud_chapter_ids = set(existing_chapters_dict.keys())
            orphaned_chapter_ids = cloud_chapter_ids - local_chapter_ids
            
            if not orphaned_chapter_ids:
                self.logger.info(f"âœ… æ²¡æœ‰éœ€è¦æ¸…ç†çš„ç« èŠ‚æ•°æ®")
                return True
                
            self.logger.info(f"ğŸ§¹ å‘ç° {len(orphaned_chapter_ids)} ä¸ªéœ€è¦æ¸…ç†çš„ç« èŠ‚:")
            for chapter_id in orphaned_chapter_ids:
                chapter_title = existing_chapters_dict[chapter_id].get('title', chapter_id)
                self.logger.info(f"  - {chapter_title} ({chapter_id})")
            
            # 1. åˆ é™¤chaptersè¡¨è®°å½•
            for chapter_id in orphaned_chapter_ids:
                if not self.delete_database_record('chapters', chapter_id):
                    self.logger.error(f"âŒ åˆ é™¤ç« èŠ‚è®°å½•å¤±è´¥: {chapter_id}")
                    return False
            
            # 2. åˆ é™¤å¯¹åº”çš„chapter_vocabulariesè®°å½•ï¼ˆæ–°è¡¨ç»“æ„ï¼šæ¯ä¸ªç« èŠ‚ä¸€æ¡è®°å½•ï¼‰
            for chapter_id in orphaned_chapter_ids:
                # ç›´æ¥åˆ é™¤ç« èŠ‚è¯æ±‡è®°å½•ï¼ˆ_idå°±æ˜¯chapter_idï¼‰
                if not self.delete_database_record('chapter_vocabularies', chapter_id):
                    self.logger.warning(f"âš ï¸ åˆ é™¤ç« èŠ‚è¯æ±‡å…³è”å¤±è´¥: {chapter_id}")
            
            # 3. åˆ é™¤äº‘å­˜å‚¨ä¸­çš„éŸ³é¢‘å’Œå­—å¹•æ–‡ä»¶
            for chapter_id in orphaned_chapter_ids:
                chapter_data = existing_chapters_dict[chapter_id]
                # åˆ é™¤éŸ³é¢‘æ–‡ä»¶
                if 'audio_url' in chapter_data and chapter_data['audio_url']:
                    file_id = self.extract_file_id_from_url(chapter_data['audio_url'])
                    if file_id:
                        self.delete_cloud_file(file_id)
                
                # åˆ é™¤å­—å¹•æ–‡ä»¶  
                if 'subtitle_url' in chapter_data and chapter_data['subtitle_url']:
                    file_id = self.extract_file_id_from_url(chapter_data['subtitle_url'])
                    if file_id:
                        self.delete_cloud_file(file_id)
            
            self.logger.info(f"âœ… æ¸…ç†å®Œæˆ: åˆ é™¤äº† {len(orphaned_chapter_ids)} ä¸ªç« èŠ‚åŠå…¶å…³è”æ•°æ®")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ç« èŠ‚æ•°æ®æ¸…ç†å¤±è´¥: {e}")
            return False

    def extract_file_id_from_url(self, url: str) -> Optional[str]:
        """ä»äº‘å­˜å‚¨URLä¸­æå–æ–‡ä»¶ID"""
        try:
            # å‡è®¾URLæ ¼å¼ç±»ä¼¼: https://xxx.com/file_id
            # éœ€è¦æ ¹æ®å®é™…å¾®ä¿¡äº‘å­˜å‚¨URLæ ¼å¼è°ƒæ•´
            if url and '/' in url:
                return url.split('/')[-1]
            return None
        except Exception:
            return None

    def delete_cloud_file(self, file_id: str) -> bool:
        """åˆ é™¤äº‘å­˜å‚¨æ–‡ä»¶"""
        try:
            # ä½¿ç”¨å¾®ä¿¡äº‘å­˜å‚¨APIåˆ é™¤æ–‡ä»¶
            url = f"https://api.weixin.qq.com/tcb/deletefile"
            data = {
                "env": self.env_id,
                "fileid_list": [file_id]
            }
            
            response = requests.post(url, 
                                   params={'access_token': self.access_token},
                                   json=data,
                                   timeout=30)
            
            result = response.json()
            if result.get('errcode') == 0:
                self.logger.info(f"âœ… åˆ é™¤äº‘å­˜å‚¨æ–‡ä»¶æˆåŠŸ: {file_id}")
                return True
            else:
                self.logger.warning(f"âš ï¸ åˆ é™¤äº‘å­˜å‚¨æ–‡ä»¶å¤±è´¥: {file_id}, {result}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ åˆ é™¤äº‘å­˜å‚¨æ–‡ä»¶å¼‚å¸¸: {file_id}, {e}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºä¸´æ—¶uploaderå®ä¾‹ä»¥ä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—ç³»ç»Ÿ
    temp_uploader = WeChatCloudUploader("temp", "temp", "temp")
    logger = temp_uploader.logger
    
    logger.info("å¾®ä¿¡äº‘æœåŠ¡ä¹¦ç±æ•°æ®ä¸Šä¼ è„šæœ¬")
    logger.info("=" * 50)
    logger.info("æ­¤è„šæœ¬å°†æŠŠoutputç›®å½•ä¸‹çš„æœ‰å£°ä¹¦æ•°æ®ä¸Šä¼ åˆ°å¾®ä¿¡äº‘æœåŠ¡")
    logger.info("è¯·ç¡®ä¿å·²ç»åˆ›å»ºäº†bookså’Œchaptersæ•°æ®åº“é›†åˆ")
    logger.info("=" * 50)
    
    # è·å–ç”¨æˆ·è¾“å…¥
    app_id = input("è¯·è¾“å…¥AppID (é»˜è®¤: wx7040936883aa6dad): ").strip() or "wx7040936883aa6dad"
    app_secret = input("è¯·è¾“å…¥AppSecret: ").strip() or "94051e8239a7a5181f32695c3c4895d9"
    env_id = input("è¯·è¾“å…¥äº‘ç¯å¢ƒID: ").strip() or "cloud1-1gpp78j208f0f610"
    
    if not app_secret or not env_id:
        logger.error("é”™è¯¯: AppSecretå’Œäº‘ç¯å¢ƒIDä¸èƒ½ä¸ºç©º")
        return
        
    # ç¡®è®¤æ“ä½œ
    logger.info(f"\né…ç½®ä¿¡æ¯:")
    logger.info(f"AppID: {app_id}")
    logger.info(f"AppSecret: {'*' * len(app_secret)}")
    logger.info(f"äº‘ç¯å¢ƒID: {env_id}")
    
    confirm = input("\nç¡®è®¤å¼€å§‹ä¸Šä¼ ï¼Ÿ(y/N): ").strip().lower()
    if confirm != 'y':
        logger.info("æ“ä½œå·²å–æ¶ˆ")
        return
        
    # åˆ›å»ºæ­£å¼çš„ä¸Šä¼ å™¨
    uploader = WeChatCloudUploader(app_id, app_secret, env_id)
    
    # æµ‹è¯•è¿æ¥
    try:
        token = uploader.get_access_token()
        uploader.logger.info(f"âœ… è¿æ¥æˆåŠŸï¼ŒAccess Token: {token[:20]}...")
    except Exception as e:
        uploader.logger.error(f"âŒ è¿æ¥å¤±è´¥: {e}")
        return
        
    # å¼€å§‹ä¸Šä¼ 
    uploader.logger.info("\nğŸš€ å¼€å§‹ä¸Šä¼ ä¹¦ç±æ•°æ®...")
    start_time = time.time()
    
    try:
        success = uploader.upload_all_books()
        
        elapsed_time = time.time() - start_time
        
        if success:
            uploader.logger.info(f"\nğŸ‰ ä¸Šä¼ å®Œæˆï¼è€—æ—¶: {elapsed_time:.2f}ç§’")
            uploader.logger.info("ğŸ“‹ è¯·æ£€æŸ¥å¾®ä¿¡äº‘æ§åˆ¶å°ç¡®è®¤æ•°æ®æ˜¯å¦æ­£ç¡®ä¸Šä¼ ")
        else:
            uploader.logger.error(f"\nâŒ ä¸Šä¼ è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ§åˆ¶å°è¾“å‡º")
            
    except Exception as e:
        uploader.logger.error(f"\nâŒ ä¸Šä¼ å¤±è´¥: {e}")
        
    uploader.logger.info("\nğŸ“ è¯¦ç»†æ—¥å¿—å·²æ˜¾ç¤ºåœ¨æ§åˆ¶å°")


if __name__ == "__main__":
    main()
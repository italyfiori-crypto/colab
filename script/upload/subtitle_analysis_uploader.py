#!/usr/bin/env python3
"""
å­—å¹•è§£æä¿¡æ¯ä¸Šä¼ æœåŠ¡
å¤„ç†å­—å¹•AIè§£ææ•°æ®çš„ä¸Šä¼ é€»è¾‘
"""

import os
import json
import time
import logging
from typing import Dict, List
from wechat_api import WeChatCloudAPI


class SubtitleAnalysisUploader:
    """å­—å¹•è§£æä¿¡æ¯ä¸Šä¼ æœåŠ¡ç±»"""
    
    def __init__(self, api_client: WeChatCloudAPI):
        self.api = api_client
        self.logger = logging.getLogger(__name__)
        
    def parse_analysis_file(self, analysis_file_path: str, book_id: str, chapter_id: str) -> List[Dict]:
        """è§£æå­—å¹•è§£æJSONæ–‡ä»¶"""
        try:
            with open(analysis_file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                
            # æŒ‰è¡Œåˆ†å‰²ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªJSONå¯¹è±¡
            analysis_records = []
            for line_num, line in enumerate(content.split('\n'), 1):
                line = line.strip()
                if not line:
                    continue
                    
                try:
                    analysis_data_local = json.loads(line)

                    analysis_data = {
                        '_id': f"{book_id}-{chapter_id}-{analysis_data_local.get('subtitle_index')}",
                        'book_id': book_id,
                        'chapter_id': chapter_id,
                        'subtitle_index': analysis_data_local.get('subtitle_index'),
                        
                        'timestamp': analysis_data_local.get('timestamp'),
                        'english_text': analysis_data_local.get('english_text', ''),
                        'chinese_text': analysis_data_local.get('chinese_text', ''),

                        'sentence_structure': analysis_data_local.get('sentence_structure', ''),
                        'structure_explanation': analysis_data_local.get('structure_explanation', ''),
                        'key_words': analysis_data_local.get('key_words', []),
                        'fixed_phrases': analysis_data_local.get('fixed_phrases', []),
                        'colloquial_expression': analysis_data_local.get('colloquial_expression', []),

                        'created_at': int(time.time() * 1000),
                        'updated_at': int(time.time() * 1000),
                    }
                                        
                    analysis_records.append(analysis_data)
                    
                except json.JSONDecodeError as e:
                    self.logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆJSONè¡Œ {line_num}: {e}")
                    continue
                    
            return analysis_records
            
        except Exception as e:
            self.logger.error(f"âŒ è§£æå­—å¹•åˆ†ææ–‡ä»¶å¤±è´¥ {analysis_file_path}: {e}")
            return []
    
    def get_existing_analysis_records(self, book_id: str, chapter_id: str) -> Dict[int, Dict]:
        """è·å–ç°æœ‰çš„å­—å¹•è§£æè®°å½•"""
        try:
            existing_records = self.api.query_all_records('analysis', {
                'book_id': book_id,
                'chapter_id': chapter_id
            })
            return {record['subtitle_index']: record for record in existing_records}
        except Exception as e:
            self.logger.error(f"âŒ æŸ¥è¯¢ç°æœ‰å­—å¹•è§£æè®°å½•å¤±è´¥: {e}")
            return {}
    
    def upload_analysis_records(self, book_id: str, chapter_id: str, analysis_records: List[Dict]) -> Dict:
        """ä¸Šä¼ å­—å¹•è§£æè®°å½•"""
        stats = {
            'added': 0,
            'updated': 0,
            'skipped': 0,
            'failed': 0
        }
        
        if not analysis_records:
            return stats
            
        # è·å–ç°æœ‰è®°å½•
        existing_records = self.get_existing_analysis_records(book_id, chapter_id)
        
        # åˆ†æ‰¹å¤„ç†è®°å½•
        batch_size = 1
        for i in range(0, len(analysis_records), batch_size):
            batch = analysis_records[i:i + batch_size]
            
            records_to_add = []
            records_to_update = []
            
            for record in batch:
                subtitle_index = record['subtitle_index']
                existing_record = existing_records.get(subtitle_index)
                
                if existing_record:
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆæ¯”è¾ƒå…³é”®å­—æ®µï¼‰
                    if self._needs_update(record, existing_record):
                        record['updated_at'] = int(time.time() * 1000)
                        records_to_update.append(record)
                    else:
                        stats['skipped'] += 1
                else:
                    records_to_add.append(record)
            
            # æ‰¹é‡æ·»åŠ æ–°è®°å½•
            if records_to_add:
                try:
                    if self.api.add_database_records('analysis', records_to_add):
                        stats['added'] += len(records_to_add)
                        self.logger.info(f"âœ… æ–°å¢ {len(records_to_add)} æ¡å­—å¹•è§£æè®°å½•")
                    else:
                        stats['failed'] += len(records_to_add)
                        self.logger.error(f"âŒ æ–°å¢å­—å¹•è§£æè®°å½•å¤±è´¥,records_to_add: {records_to_add}")
                except Exception as e:
                    stats['failed'] += len(records_to_add)
                    self.logger.error(f"âŒ æ–°å¢å­—å¹•è§£æè®°å½•å¼‚å¸¸: {e}")
            
            # æ‰¹é‡æ›´æ–°ç°æœ‰è®°å½•
            for record in records_to_update:
                try:
                    if self.api.update_database_record('analysis', record['_id'], record):
                        stats['updated'] += 1
                    else:
                        stats['failed'] += 1
                        self.logger.error(f"âŒ æ›´æ–°å­—å¹•è§£æè®°å½•å¤±è´¥: {record['_id']}")
                except Exception as e:
                    stats['failed'] += 1
                    self.logger.error(f"âŒ æ›´æ–°å­—å¹•è§£æè®°å½•å¼‚å¸¸ {record['_id']}: {e}")
        
        return stats
    
    def _needs_update(self, new_record: Dict, existing_record: Dict) -> bool:
        """æ£€æŸ¥è®°å½•æ˜¯å¦éœ€è¦æ›´æ–°"""
        # æ¯”è¾ƒå…³é”®å­—æ®µ
        key_fields = ['timestamp', 'english_text', 'chinese_text', 'sentence_structure', 
                     'structure_explanation', 'key_words', 'fixed_phrases', 'colloquial_expression']
        
        for field in key_fields:
            new_value = new_record.get(field)
            existing_value = existing_record.get(field)

            # å¯¹äºå­˜å‚¨ä¸ºJSONå­—ç¬¦ä¸²çš„å­—æ®µï¼Œéœ€è¦å…ˆè§£æå†æ¯”è¾ƒ
            if field in ['key_words', 'fixed_phrases', 'colloquial_expression']:
                try:
                    new_value = json.loads(new_value) if isinstance(new_value, str) else new_value
                    existing_value = json.loads(existing_value) if isinstance(existing_value, str) else existing_value
                except json.JSONDecodeError:
                    self.logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥ï¼Œå­—æ®µ: {field}, new_value: {new_value}, existing_value: {existing_value}")
                    # å¦‚æœè§£æå¤±è´¥ï¼Œåˆ™æŒ‰åŸå§‹å€¼æ¯”è¾ƒ
                    pass

            if new_value != existing_value:
                return True
        return False
    
    def process_book_analysis(self, book_dir: str, book_id: str) -> Dict:
        """å¤„ç†å•æœ¬ä¹¦çš„å­—å¹•è§£æä¸Šä¼ """
        analysis_dir = os.path.join(book_dir, 'analysis')
        
        if not os.path.exists(analysis_dir):
            self.logger.info(f"ğŸ“ ä¹¦ç± {book_id} æ²¡æœ‰å­—å¹•è§£æç›®å½•")
            return {'total_processed': 0}
        
        total_stats = {
            'files_processed': 0,
            'total_records': 0,
            'added': 0,
            'updated': 0,
            'skipped': 0,
            'failed': 0
        }
        
        # éå†è§£ææ–‡ä»¶
        for filename in os.listdir(analysis_dir):
            if not filename.endswith('.json'):
                continue
                
            # ä»æ–‡ä»¶åæå–chapter_id (å»æ‰.jsonåç¼€)
            chapter_id = os.path.splitext(filename)[0]
            analysis_file_path = os.path.join(analysis_dir, filename)
            
            self.logger.info(f"ğŸ“ å¤„ç†å­—å¹•è§£ææ–‡ä»¶: {filename}")
            
            # è§£ææ–‡ä»¶
            analysis_records = self.parse_analysis_file(analysis_file_path, book_id, chapter_id)
            
            if analysis_records:
                # ä¸Šä¼ è®°å½•
                file_stats = self.upload_analysis_records(book_id, chapter_id, analysis_records)
                
                # æ›´æ–°æ€»ç»Ÿè®¡
                total_stats['total_records'] += len(analysis_records)
                total_stats['added'] += file_stats['added']
                total_stats['updated'] += file_stats['updated']
                total_stats['skipped'] += file_stats['skipped']
                total_stats['failed'] += file_stats['failed']
                
                self.logger.info(f"ğŸ“Š æ–‡ä»¶ {filename} ç»Ÿè®¡: æ–°å¢{file_stats['added']}, æ›´æ–°{file_stats['updated']}, è·³è¿‡{file_stats['skipped']}, å¤±è´¥{file_stats['failed']}")
            
            total_stats['files_processed'] += 1
        
        if total_stats['files_processed'] > 0:
            self.logger.info(f"âœ… ä¹¦ç± {book_id} å­—å¹•è§£æå¤„ç†å®Œæˆ: å¤„ç†{total_stats['files_processed']}ä¸ªæ–‡ä»¶, å…±{total_stats['total_records']}æ¡è®°å½•")
        
        return total_stats
    
    def cleanup_orphaned_analysis(self, book_id: str, existing_articles: set) -> bool:
        """æ¸…ç†å­¤ç«‹çš„å­—å¹•è§£ææ•°æ®"""
        try:
            # æŸ¥è¯¢æ•°æ®åº“ä¸­è¯¥ä¹¦ç±çš„æ‰€æœ‰è§£æè®°å½•
            existing_analysis = self.api.query_all_records('analysis', {'book_id': book_id})
            
            orphaned_records = []
            for record in existing_analysis:
                if record['chapter_id'] not in existing_articles:
                    orphaned_records.append(record['_id'])
            
            if not orphaned_records:
                self.logger.info(f"âœ… ä¹¦ç± {book_id} æ— éœ€æ¸…ç†å­—å¹•è§£ææ•°æ®")
                return True
            
            self.logger.info(f"ğŸ§¹ æ¸…ç†ä¹¦ç± {book_id} çš„ {len(orphaned_records)} æ¡å­¤ç«‹å­—å¹•è§£æè®°å½•...")
            
            # æ‰¹é‡åˆ é™¤
            success_count = 0
            for record_id in orphaned_records:
                if self.api.delete_database_record('analysis', record_id):
                    success_count += 1
                    
            self.logger.info(f"âœ… æˆåŠŸæ¸…ç† {success_count} æ¡å­¤ç«‹å­—å¹•è§£æè®°å½•")
            return success_count == len(orphaned_records)
            
        except Exception as e:
            self.logger.error(f"âŒ æ¸…ç†å­¤ç«‹å­—å¹•è§£ææ•°æ®å¤±è´¥: {e}")
            return False
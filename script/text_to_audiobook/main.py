#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç« èŠ‚æ‹†åˆ†ä¸»ç¨‹åº - ç²¾ç®€ç‰ˆ
æ”¯æŒé…ç½®æ–‡ä»¶å’Œæœ€å°‘çš„å‘½ä»¤è¡Œå‚æ•°
"""

import argparse
import sys
import os
import time

from infra.config_loader import ConfigLoader
from service.workflow_executor import WorkflowExecutor
from util import OUTPUT_DIRECTORIES
from util.file_utils import find_txt_files
from util.file_utils import get_existing_files


def extract_paths_from_sub_chapter(sub_chapter_file: str) -> tuple[str, str, str]:
    """
    ä»å­ç« èŠ‚æ–‡ä»¶è·¯å¾„æå–å¿…è¦ä¿¡æ¯
    
    Args:
        sub_chapter_file: å­ç« èŠ‚æ–‡ä»¶è·¯å¾„
        ä¾‹å¦‚: /path/to/output/book_name/sub_chapters/chapter_01_001.txt
        
    Returns:
        (output_dir, book_name, base_name)
        - output_dir: /path/to/output/book_name/
        - book_name: book_name  
        - base_name: chapter_01_001
    """
    # æ ‡å‡†åŒ–è·¯å¾„
    sub_chapter_file = os.path.abspath(sub_chapter_file)
    
    # éªŒè¯æ–‡ä»¶è·¯å¾„åŒ…å«sub_chapters
    if 'sub_chapters' not in sub_chapter_file:
        raise ValueError(f"æ–‡ä»¶ä¸åœ¨sub_chaptersç›®å½•ä¸­: {sub_chapter_file}")
    
    # æå–åŸºç¡€æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    base_name = os.path.splitext(os.path.basename(sub_chapter_file))[0]
    
    # è·å–è¾“å‡ºç›®å½•ï¼ˆsub_chaptersçš„çˆ¶ç›®å½•ï¼‰
    sub_chapters_dir = os.path.dirname(sub_chapter_file)
    output_dir = os.path.dirname(sub_chapters_dir)
    
    # æå–ä¹¦ç±åç§°ï¼ˆè¾“å‡ºç›®å½•çš„æœ€åä¸€çº§ï¼‰
    book_name = os.path.basename(output_dir)
    
    return output_dir, book_name, base_name


def cleanup_sub_chapter_files(sub_chapter_file: str, output_dir: str, verbose: bool = False):
    """
    æ¸…ç†å­ç« èŠ‚å¯¹åº”çš„ç›¸å…³æ–‡ä»¶
    
    Args:
        sub_chapter_file: å­ç« èŠ‚æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    """
    _, _, base_name = extract_paths_from_sub_chapter(sub_chapter_file)
    
    # å®šä¹‰éœ€è¦æ¸…ç†çš„æ–‡ä»¶è·¯å¾„
    files_to_clean = [
        os.path.join(output_dir, OUTPUT_DIRECTORIES['sentences'], f"{base_name}.jsonl"),
        os.path.join(output_dir, OUTPUT_DIRECTORIES['audio'], f"{base_name}.wav"),
        os.path.join(output_dir, OUTPUT_DIRECTORIES['subtitles'], f"{base_name}.jsonl"),
        os.path.join(output_dir, OUTPUT_DIRECTORIES['analysis'], f"{base_name}.jsonl"),
        os.path.join(output_dir, OUTPUT_DIRECTORIES['compressed_audio'], f"{base_name}.mp3"),
    ]
    
    cleaned_count = 0
    for file_path in files_to_clean:
        if os.path.exists(file_path):
            try:
                os.remove(file_path)
                cleaned_count += 1
                if verbose:
                    print(f"  ğŸ—‘ï¸  åˆ é™¤: {os.path.relpath(file_path, output_dir)}")
            except Exception as e:
                print(f"  âŒ åˆ é™¤å¤±è´¥ {os.path.relpath(file_path, output_dir)}: {e}")
    
    if verbose and cleaned_count > 0:
        print(f"ğŸ§¹ å·²æ¸…ç† {cleaned_count} ä¸ªç›¸å…³æ–‡ä»¶")
    elif verbose:
        print("ğŸ§¹ æœªæ‰¾åˆ°éœ€è¦æ¸…ç†çš„æ–‡ä»¶")

def process_single_book(input_file: str, args, config: dict, workflow: 'WorkflowExecutor') -> dict:
    """
    å¤„ç†å•æœ¬ä¹¦ç±æ–‡ä»¶ï¼ˆä»åŸå§‹æ–‡æœ¬å¼€å§‹çš„å®Œæ•´æµç¨‹ï¼‰
    
    Args:
        input_file: ä¹¦ç±æ–‡æœ¬æ–‡ä»¶è·¯å¾„
        args: å‘½ä»¤è¡Œå‚æ•°
        config: é…ç½®ä¿¡æ¯
        workflow: å·¥ä½œæµæ‰§è¡Œå™¨
        
    Returns:
        åŒ…å«å¤„ç†ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    # é»˜è®¤ç›®å½•
    program_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
    file_name = os.path.basename(input_file)
    output_dir = os.path.join(program_root, "output", os.path.splitext(file_name)[0])
    master_vocab_file = os.path.join(program_root, "output", "vocabulary", "master_vocabulary.json")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    try:
        os.makedirs(output_dir, exist_ok=True)
    except Exception as e:
        raise Exception(f"æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½•: {e}")
    
    if args.verbose:
        print(f"\nğŸ“ å¤„ç†æ–‡ä»¶: {input_file}")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
    
    # è®°å½•æ–‡ä»¶å¤„ç†å¼€å§‹æ—¶é—´
    file_start_time = time.time()
    
    # æ‰§è¡Œå„ä¸ªå¤„ç†æµç¨‹
    sub_chapter_files, sentence_files = [], []
    chapter_time, sentence_time = 0, 0
    
    # ç« èŠ‚å’Œå­ç« èŠ‚æ‹†åˆ†
    if args.chapter:
        _, sub_chapter_files, chapter_time = workflow.execute_chapter_processing(input_file, output_dir, args.verbose)
    else:
        # è·å–å·²å­˜åœ¨çš„å­ç« èŠ‚æ–‡ä»¶
        sub_chapter_files = get_existing_files(output_dir, OUTPUT_DIRECTORIES['sub_chapters'], ".txt")
    
    # å¥å­æ‹†åˆ†
    if args.sentence:
        if not sub_chapter_files:
            raise Exception("æœªæ‰¾åˆ°å­ç« èŠ‚æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ --chapter è¿›è¡Œç« èŠ‚æ‹†åˆ†")
        sentence_files, sentence_time = workflow.execute_sentence_processing(sub_chapter_files, output_dir, args.verbose)
    else:
        # è·å–å·²å­˜åœ¨çš„å¥å­æ–‡ä»¶
        sentence_files = get_existing_files(output_dir, OUTPUT_DIRECTORIES['sentences'], ".jsonl")
    
    # éŸ³é¢‘&å­—å¹•ç”Ÿæˆ
    audio_files, subtitle_files, audio_time = [], [], 0
    if args.audio:
        audio_files, subtitle_files, audio_time = workflow.execute_audio_processing(sentence_files, output_dir, args.voice, args.speed, args.verbose)
    else:
        audio_files = get_existing_files(output_dir, OUTPUT_DIRECTORIES['audio'], ".wav")
        subtitle_files = get_existing_files(output_dir, OUTPUT_DIRECTORIES['subtitles'], ".jsonl")

    # åˆ†æå¤„ç†
    analyzed_files, analysis_time = [], 0
    if args.analysis:
        analyzed_files, analysis_time = workflow.execute_analysis(subtitle_files, output_dir, args.verbose)

    # éŸ³é¢‘å‹ç¼©
    compressed_files, compression_time = [], 0
    if args.compress:
        if not audio_files:
            print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ --audio è¿›è¡ŒéŸ³é¢‘ç”Ÿæˆ")
        else:
            compressed_files, compression_time = workflow.execute_audio_compression(audio_files, output_dir, args.verbose)

    # è¯æ±‡å¤„ç†
    chapter_vocab_files, vocabulary_time = [], 0
    if args.vocabulary:
        book_name = os.path.splitext(os.path.basename(input_file))[0]
        chapter_vocab_files, vocabulary_time = workflow.execute_vocabulary_processing(sub_chapter_files, output_dir, book_name, master_vocab_file, args.verbose)
    
    # ç»Ÿè®¡ä¿¡æ¯æ”¶é›†
    statistics_time = 0
    if args.stats:
        # ç‹¬ç«‹æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        _, statistics_time = workflow.execute_statistics_collection(sub_chapter_files, audio_files, output_dir, args.verbose)
    
    # è®¡ç®—è€—æ—¶
    total_time = chapter_time + sentence_time + audio_time + analysis_time + compression_time + vocabulary_time + statistics_time
    file_total_time = time.time() - file_start_time
    
    return {
        'input_file': input_file,
        'output_dir': output_dir,
        'success': True,
        'times': {
            'chapter': chapter_time,
            'sentence': sentence_time,
            'audio': audio_time,
            'analysis': analysis_time,
            'compression': compression_time,
            'vocabulary': vocabulary_time,
            'statistics': statistics_time,
            'total': total_time,
            'file_total': file_total_time
        },
        'files': {
            'sentence_files': len(sentence_files),
            'audio_files': len(audio_files),
            'subtitle_files': len(subtitle_files),
            'compressed_files': len(compressed_files),
            'analyzed_files': len(analyzed_files),
            'chapter_vocab_files': len(chapter_vocab_files)
        }
    }


def process_single_sub_chapter(sub_chapter_file: str, args, config: dict, workflow: 'WorkflowExecutor') -> dict:
    """
    å¤„ç†å•ä¸ªå­ç« èŠ‚æ–‡ä»¶ï¼ˆä»å¥å­æ‹†åˆ†å¼€å§‹çš„æµç¨‹ï¼‰
    
    Args:
        sub_chapter_file: å­ç« èŠ‚æ–‡ä»¶è·¯å¾„
        args: å‘½ä»¤è¡Œå‚æ•°
        config: é…ç½®ä¿¡æ¯
        workflow: å·¥ä½œæµæ‰§è¡Œå™¨
        
    Returns:
        åŒ…å«å¤„ç†ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    try:
        # è·¯å¾„è§£æ
        output_dir, book_name, base_name = extract_paths_from_sub_chapter(sub_chapter_file)
        master_vocab_file = os.path.join(os.path.dirname(output_dir), "vocabulary", "master_vocabulary.json")
        
        if args.verbose:
            print(f"\nğŸ“ å¤„ç†å­ç« èŠ‚: {sub_chapter_file}")
            print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
            print(f"ğŸ“– ä¹¦ç±åç§°: {book_name}")
            print(f"ğŸ“„ ç« èŠ‚åç§°: {base_name}")
        
        # æ–‡ä»¶æ¸…ç†ï¼ˆå¦‚æœå¯ç”¨è¦†ç›–æ¨¡å¼ï¼‰
        if args.overwrite:
            print(f"ğŸ§¹ æ¸…ç†å­ç« èŠ‚ç›¸å…³æ–‡ä»¶...")
            cleanup_sub_chapter_files(sub_chapter_file, output_dir, args.verbose)
        
        # è®°å½•å¤„ç†å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # åˆå§‹åŒ–è®¡æ—¶å˜é‡
        sentence_time, audio_time, analysis_time = 0, 0, 0
        compression_time, vocabulary_time, statistics_time = 0, 0, 0
        
        # å¥å­æ‹†åˆ†ï¼ˆå¿…é¡»æ‰§è¡Œï¼Œå› ä¸ºæ˜¯ä»å­ç« èŠ‚å¼€å§‹ï¼‰
        sentence_files = []
        if args.sentence:
            sentence_files, sentence_time = workflow.execute_sentence_processing([sub_chapter_file], output_dir, args.verbose)
        else:
            # è·å–å·²å­˜åœ¨çš„å¥å­æ–‡ä»¶
            sentence_files = get_existing_files(output_dir, OUTPUT_DIRECTORIES['sentences'], ".jsonl")
            if not sentence_files and args.verbose:
                print("âš ï¸ æœªæ‰¾åˆ°å¯¹åº”çš„å¥å­æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ --sentence è¿›è¡Œå¥å­æ‹†åˆ†")
        
        # éŸ³é¢‘ç”Ÿæˆ
        audio_files, subtitle_files = [], []
        if args.audio and sentence_files:
            audio_files, subtitle_files, audio_time = workflow.execute_audio_processing(sentence_files, output_dir, args.voice, args.speed, args.verbose)
        else:
            # è·å–å·²å­˜åœ¨çš„éŸ³é¢‘å’Œå­—å¹•æ–‡ä»¶
            audio_files = [os.path.join(output_dir, OUTPUT_DIRECTORIES['audio'], f"{base_name}.wav")] if os.path.exists(os.path.join(output_dir, OUTPUT_DIRECTORIES['audio'], f"{base_name}.wav")) else []
            subtitle_files = [os.path.join(output_dir, OUTPUT_DIRECTORIES['subtitles'], f"{base_name}.jsonl")] if os.path.exists(os.path.join(output_dir, OUTPUT_DIRECTORIES['subtitles'], f"{base_name}.jsonl")) else []
        
        # åˆ†æå¤„ç†
        analyzed_files = []
        if args.analysis and subtitle_files:
            analyzed_files, analysis_time = workflow.execute_analysis(subtitle_files, output_dir, args.verbose)

        # éŸ³é¢‘å‹ç¼©
        compressed_files = []
        if args.compress and audio_files:
            compressed_files, compression_time = workflow.execute_audio_compression(audio_files, output_dir, args.verbose)

        # è¯æ±‡å¤„ç†
        chapter_vocab_files = []
        if args.vocabulary and sentence_files:
            chapter_vocab_files, vocabulary_time = workflow.execute_vocabulary_processing([sub_chapter_file], output_dir, book_name, master_vocab_file, args.verbose)
        
        # ç»Ÿè®¡ä¿¡æ¯æ”¶é›†ï¼ˆä»…é’ˆå¯¹å•ä¸ªå­ç« èŠ‚çš„ç»Ÿè®¡ï¼‰
        if args.stats:
            # å¯¹äºå•ä¸ªå­ç« èŠ‚ï¼Œåªæ”¶é›†å½“å‰å¤„ç†çš„ç»Ÿè®¡
            _, statistics_time = workflow.execute_statistics_collection([sub_chapter_file], audio_files, output_dir, args.verbose)
        
        # è®¡ç®—è€—æ—¶
        total_time = sentence_time + audio_time + analysis_time + compression_time + vocabulary_time + statistics_time
        file_total_time = time.time() - start_time
        
        return {
            'input_file': sub_chapter_file,
            'output_dir': output_dir,
            'book_name': book_name,
            'base_name': base_name,
            'success': True,
            'times': {
                'sentence': sentence_time,
                'audio': audio_time,
                'analysis': analysis_time,
                'compression': compression_time,
                'vocabulary': vocabulary_time,
                'statistics': statistics_time,
                'total': total_time,
                'file_total': file_total_time
            },
            'files': {
                'sentence_files': len(sentence_files),
                'audio_files': len(audio_files),
                'subtitle_files': len(subtitle_files),
                'compressed_files': len(compressed_files),
                'analyzed_files': len(analyzed_files),
                'chapter_vocab_files': len(chapter_vocab_files)
            }
        }
        
    except Exception as e:
        return {
            'input_file': sub_chapter_file,
            'success': False,
            'error': str(e)
        }


def print_sub_chapter_results(results: list[dict]):
    """ä¸“é—¨ä¸ºå­ç« èŠ‚å¤„ç†ç»“æœè¾“å‡º"""
    if not results:
        return
    
    result = results[0]  # å•ä¸ªå­ç« èŠ‚ç»“æœ
    if not result['success']:
        return
    
    print(f"\nğŸ“Š å­ç« èŠ‚å¤„ç†å®Œæˆ:")
    print(f"  ğŸ“– ä¹¦ç±: {result['book_name']}")
    print(f"  ğŸ“„ ç« èŠ‚: {result['base_name']}")
    print(f"  ğŸ“‚ è¾“å‡ºç›®å½•: {result['output_dir']}")
    
    times = result['times']
    files = result['files']
    
    print(f"\nâ±ï¸ å¤„ç†è€—æ—¶:")
    if times['sentence'] > 0:
        print(f"  å¥å­æ‹†åˆ†: {times['sentence']:.2f}ç§’")
    if times['audio'] > 0:
        print(f"  éŸ³é¢‘ç”Ÿæˆ: {times['audio']:.2f}ç§’")
    if times['analysis'] > 0:
        print(f"  è¯­è¨€å­¦åˆ†æ: {times['analysis']:.2f}ç§’")
    if times['compression'] > 0:
        print(f"  éŸ³é¢‘å‹ç¼©: {times['compression']:.2f}ç§’")
    if times['vocabulary'] > 0:
        print(f"  è¯æ±‡å¤„ç†: {times['vocabulary']:.2f}ç§’")
    print(f"  æ€»è€—æ—¶: {times['file_total']:.2f}ç§’")
    
    print(f"\nğŸ“ ç”Ÿæˆæ–‡ä»¶:")
    if files['sentence_files'] > 0:
        print(f"  å¥å­æ–‡ä»¶: {files['sentence_files']} ä¸ª")
    if files['audio_files'] > 0:
        print(f"  éŸ³é¢‘æ–‡ä»¶: {files['audio_files']} ä¸ª")
    if files['subtitle_files'] > 0:
        print(f"  å­—å¹•æ–‡ä»¶: {files['subtitle_files']} ä¸ª")
    if files['compressed_files'] > 0:
        print(f"  å‹ç¼©æ–‡ä»¶: {files['compressed_files']} ä¸ª")
    if files['analyzed_files'] > 0:
        print(f"  åˆ†ææ–‡ä»¶: {files['analyzed_files']} ä¸ª")
    if files['chapter_vocab_files'] > 0:
        print(f"  è¯æ±‡æ–‡ä»¶: {files['chapter_vocab_files']} ä¸ª")


def print_book_results(results: list[dict], args, program_start_time: float):
    """ä¸“é—¨ä¸ºä¹¦ç±å¤„ç†ç»“æœè¾“å‡º"""
    if not results:
        return
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    program_total_time = time.time() - program_start_time
    successful_results = [r for r in results if r['success']]
    failed_results = [r for r in results if not r['success']]
    
    # æ±‡æ€»æ‰€æœ‰æˆåŠŸå¤„ç†çš„æ—¶é—´
    if successful_results:
        total_times = {
            'chapter': sum(r['times']['chapter'] for r in successful_results),
            'sentence': sum(r['times']['sentence'] for r in successful_results),
            'audio': sum(r['times']['audio'] for r in successful_results),
            'analysis': sum(r['times']['analysis'] for r in successful_results),
            'compression': sum(r['times']['compression'] for r in successful_results),
            'vocabulary': sum(r['times']['vocabulary'] for r in successful_results),
            'statistics': sum(r['times']['statistics'] for r in successful_results),
            'total': sum(r['times']['total'] for r in successful_results)
        }
        
        # æ‰“å°æ‰¹é‡å¤„ç†æ±‡æ€»
        print(f"\nğŸ“Š æ‰¹é‡å¤„ç†æ±‡æ€» ({len(successful_results)}/{len(results)} æˆåŠŸ):")
        if args.chapter and total_times['total'] > 0:
            print(f"  ç« èŠ‚æ‹†åˆ†: {total_times['chapter']:.2f}ç§’ ({total_times['chapter']/total_times['total']*100:.1f}%)")
        if args.sentence and total_times['total'] > 0:
            print(f"  å¥å­æ‹†åˆ†: {total_times['sentence']:.2f}ç§’ ({total_times['sentence']/total_times['total']*100:.1f}%)")
        if args.audio and total_times['total'] > 0:
            print(f"  éŸ³é¢‘ç”Ÿæˆ: {total_times['audio']:.2f}ç§’ ({total_times['audio']/total_times['total']*100:.1f}%)")
        if args.analysis and total_times['total'] > 0:
            print(f"  è¯­è¨€å­¦åˆ†æ: {total_times['analysis']:.2f}ç§’ ({total_times['analysis']/total_times['total']*100:.1f}%)")
        if args.compress and total_times['total'] > 0:
            print(f"  éŸ³é¢‘å‹ç¼©: {total_times['compression']:.2f}ç§’ ({total_times['compression']/total_times['total']*100:.1f}%)")
        if args.vocabulary and total_times['total'] > 0:
            print(f"  è¯æ±‡å¤„ç†: {total_times['vocabulary']:.2f}ç§’ ({total_times['vocabulary']/total_times['total']*100:.1f}%)")
        if args.stats and total_times['statistics'] > 0:
            print(f"  ç»Ÿè®¡æ”¶é›†: {total_times['statistics']:.2f}ç§’ ({total_times['statistics']/total_times['total']*100:.1f}%)")
        print(f"  æ ¸å¿ƒå¤„ç†æ€»è€—æ—¶: {total_times['total']:.2f}ç§’")
        print(f"  ç¨‹åºæ€»è€—æ—¶: {program_total_time:.2f}ç§’")
        
        if args.verbose:
            # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
            total_files = {
                'sentence_files': sum(r['files']['sentence_files'] for r in successful_results),
                'audio_files': sum(r['files']['audio_files'] for r in successful_results),
                'subtitle_files': sum(r['files']['subtitle_files'] for r in successful_results),
                'compressed_files': sum(r['files']['compressed_files'] for r in successful_results),
                'analyzed_files': sum(r['files']['analyzed_files'] for r in successful_results),
                'chapter_vocab_files': sum(r['files']['chapter_vocab_files'] for r in successful_results)
            }
            
            print(f"\nğŸ“ ç”Ÿæˆæ–‡ä»¶ç»Ÿè®¡:")
            print(f"  ç”Ÿæˆçš„å¥å­æ–‡ä»¶: {total_files['sentence_files']} ä¸ª")
            if args.audio and total_files['audio_files'] > 0:
                print(f"  ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶: {total_files['audio_files']} ä¸ª")
                print(f"  ç”Ÿæˆçš„å­—å¹•æ–‡ä»¶: {total_files['subtitle_files']} ä¸ª")
            if args.compress and total_files['compressed_files'] > 0:
                print(f"  å‹ç¼©çš„éŸ³é¢‘æ–‡ä»¶: {total_files['compressed_files']} ä¸ª")
            if args.analysis and total_files['analyzed_files'] > 0:
                print(f"  åˆ†æçš„å­—å¹•æ–‡ä»¶: {total_files['analyzed_files']} ä¸ª")
            if args.vocabulary and total_files['chapter_vocab_files'] > 0:
                print(f"  ç”Ÿæˆçš„ç« èŠ‚è¯æ±‡æ–‡ä»¶: {total_files['chapter_vocab_files']} ä¸ª")
    
    # æ˜¾ç¤ºå¤±è´¥çš„æ–‡ä»¶
    if failed_results:
        print(f"\nâŒ å¤±è´¥çš„æ–‡ä»¶ ({len(failed_results)} ä¸ª):")
        for result in failed_results:
            print(f"  â€¢ {os.path.basename(result['input_file'])}: {result['error']}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='ç« èŠ‚æ‹†åˆ†å·¥å…· - å°†ä¹¦ç±æ–‡æœ¬æ‹†åˆ†ä¸ºç‹¬ç«‹çš„ç« èŠ‚æ–‡ä»¶',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # ä¹¦ç±å¤„ç†æ¨¡å¼ï¼ˆå®Œæ•´æµç¨‹ï¼‰
  %(prog)s data/greens.txt --chapter --sentence --audio
  %(prog)s data/books/  # æ‰¹é‡å¤„ç†ç›®å½•ä¸‹æ‰€æœ‰.txtæ–‡ä»¶
  %(prog)s data/book.txt --chapter --sentence --audio --analysis --vocabulary
  %(prog)s data/book.txt --sentence --audio  # åªè¿è¡Œå¥å­æ‹†åˆ†å’ŒéŸ³é¢‘ç”Ÿæˆ
  %(prog)s data/book.txt --chapter  # åªè¿è¡Œç« èŠ‚æ‹†åˆ†
  
  # å­ç« èŠ‚å¤„ç†æ¨¡å¼ï¼ˆä»å¥å­æ‹†åˆ†å¼€å§‹ï¼‰
  %(prog)s output/book_name/sub_chapters/chapter_01_001.txt --sub-chapter --sentence --audio
  %(prog)s output/book_name/sub_chapters/chapter_01_001.txt --sub-chapter --sentence --audio --analysis
  %(prog)s output/book_name/sub_chapters/chapter_01_001.txt --sub-chapter --overwrite --sentence --audio  # è¦†ç›–æ¨¡å¼
  
é»˜è®¤é…ç½®æ–‡ä»¶: text_to_audiobook/config.json
é»˜è®¤è¾“å‡ºç›®å½•: ./output
é»˜è®¤æ€»è¯æ±‡è¡¨: script/text_to_audiobook/vocabulary/master_vocabulary.json
è¾“å‡ºæ ¼å¼: chapters/, sub_chapters/, sentences/, audio/, subtitles/, vocabulary/ ç›®å½•
        """
    )
    
    # æ ¸å¿ƒå‚æ•°
    parser.add_argument('input_path',help='è¾“å…¥æ–‡æœ¬æ–‡ä»¶ã€ç›®å½•è·¯å¾„æˆ–å­ç« èŠ‚æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--verbose','-v',action='store_true',help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
    
    # å¤„ç†æ¨¡å¼å‚æ•°
    parser.add_argument('--sub-chapter', action='store_true', help='å¤„ç†å•ä¸ªå­ç« èŠ‚æ–‡ä»¶ï¼ˆä»å¥å­æ‹†åˆ†å¼€å§‹ï¼‰')
    parser.add_argument('--overwrite', action='store_true', help='è¦†ç›–æ¨¡å¼ï¼šåˆ é™¤å­ç« èŠ‚å¯¹åº”çš„ç›¸å…³æ–‡ä»¶åé‡æ–°å¤„ç†')
    
    # æ–‡æœ¬æ‹†åˆ†å‚æ•°
    parser.add_argument('--chapter', action='store_true', help='å¯ç”¨ç« èŠ‚å’Œå­ç« èŠ‚æ‹†åˆ†')
    parser.add_argument('--sentence', action='store_true', help='å¯ç”¨å¥å­æ‹†åˆ†')

    # éŸ³é¢‘ç”Ÿæˆå‚æ•°
    parser.add_argument('--audio', action='store_true', help='å¯ç”¨éŸ³é¢‘ç”Ÿæˆ')
    parser.add_argument('--voice', default='af_bella', help='è¯­éŸ³æ¨¡å‹ (é»˜è®¤: af_bella)')
    parser.add_argument('--speed', type=float, default=0.8, help='è¯­éŸ³é€Ÿåº¦ (é»˜è®¤: 1.0)')
    
    # ç¿»è¯‘å’Œåˆ†æå‚æ•°
    parser.add_argument('--analysis', action='store_true', help='å¯ç”¨è¯­è¨€å­¦åˆ†æ')
    
    # éŸ³é¢‘å‹ç¼©å‚æ•°
    parser.add_argument('--compress', action='store_true', help='å¯ç”¨éŸ³é¢‘å‹ç¼©')
    
    # è¯æ±‡å¤„ç†å‚æ•°
    parser.add_argument('--vocabulary', action='store_true', help='å¯ç”¨è¯æ±‡æå–å’Œåˆ†çº§')
    
    # ç»Ÿè®¡å‚æ•°
    parser.add_argument('--stats', action='store_true', help='å¯ç”¨ç»Ÿè®¡ä¿¡æ¯æ”¶é›†')
    args = parser.parse_args()

    # é…ç½®è·¯å¾„
    config_path = os.path.join(os.path.dirname(__file__), 'config.json')

    # è®°å½•ç¨‹åºå¼€å§‹æ—¶é—´
    program_start_time = time.time()
    
    try:
        # åŠ è½½é…ç½®
        if not os.path.exists(config_path):
            print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return 1
        
        config_loader = ConfigLoader()
        config = config_loader.load_config(config_path)
        if args.verbose:
            print(f"å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        
        # åˆå§‹åŒ–å·¥ä½œæµæ‰§è¡Œå™¨
        workflow = WorkflowExecutor(config)
        
        # æ ¹æ®å¤„ç†æ¨¡å¼åˆ†å‘
        if args.sub_chapter:
            # å•ä¸ªå­ç« èŠ‚å¤„ç†æ¨¡å¼
            if not os.path.isfile(args.input_path):
                print("âŒ é”™è¯¯: å­ç« èŠ‚æ¨¡å¼éœ€è¦æŒ‡å®šå•ä¸ªæ–‡ä»¶")
                return 1
                
            # éªŒè¯æ–‡ä»¶æ˜¯å¦åœ¨sub_chaptersç›®å½•ä¸­
            if 'sub_chapters' not in args.input_path:
                print("âŒ é”™è¯¯: æ–‡ä»¶ä¸åœ¨sub_chaptersç›®å½•ä¸­")
                return 1
                
            print(f"ğŸ”„ å­ç« èŠ‚å¤„ç†æ¨¡å¼")
            print(f"ğŸ“ å¤„ç†æ–‡ä»¶: {os.path.basename(args.input_path)}")
            
            # å¤„ç†å•ä¸ªå­ç« èŠ‚
            try:
                result = process_single_sub_chapter(args.input_path, args, config, workflow)
                if result['success']:
                    print(f"âœ… å­ç« èŠ‚å¤„ç†å®Œæˆ: {result['base_name']} ({result['times']['file_total']:.2f}ç§’)")
                    print_sub_chapter_results([result])
                else:
                    print(f"âŒ å­ç« èŠ‚å¤„ç†å¤±è´¥: {result['error']}")
                    return 1
                    
            except Exception as e:
                print(f"âŒ å­ç« èŠ‚å¤„ç†å¼‚å¸¸: {e}")
                if args.verbose:
                    import traceback
                    traceback.print_exc()
                return 1
                
        else:
            # åŸæœ‰çš„ä¹¦ç±å¤„ç†æ¨¡å¼
            # å‘ç°å¾…å¤„ç†çš„æ–‡ä»¶
            try:
                input_files = find_txt_files(args.input_path)
            except ValueError as e:
                print(f"âŒ é”™è¯¯: {e}")
                return 1
            
            # æ˜¾ç¤ºå‘ç°çš„æ–‡ä»¶
            print(f"ğŸ” å‘ç° {len(input_files)} ä¸ª .txt æ–‡ä»¶:")
            for i, file in enumerate(input_files, 1):
                print(f"  {i}. {os.path.basename(file)}")
            
            # æ‰¹é‡å¤„ç†æ–‡ä»¶
            results = []
            for i, input_file in enumerate(input_files, 1):
                print(f"\nğŸš€ [{i}/{len(input_files)}] å¼€å§‹å¤„ç†: {os.path.basename(input_file)}")
                
                try:
                    result = process_single_book(input_file, args, config, workflow)
                    results.append(result)
                    print(f"âœ… [{i}/{len(input_files)}] å¤„ç†å®Œæˆ: {os.path.basename(input_file)} ({result['times']['file_total']:.2f}ç§’)")
                    
                except Exception as e:
                    error_result = {
                        'input_file': input_file,
                        'success': False,
                        'error': str(e)
                    }
                    results.append(error_result)
                    print(f"âŒ [{i}/{len(input_files)}] å¤„ç†å¤±è´¥: {os.path.basename(input_file)} - {e}")
                    if args.verbose:
                        import traceback
                        traceback.print_exc()
            
            # æ˜¾ç¤ºä¹¦ç±å¤„ç†ç»“æœ
            print_book_results(results, args, program_start_time)
        
        # è¿”å›çŠ¶æ€ç  
        return 0
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())
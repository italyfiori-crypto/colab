#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å­—å¹•è§£ææ¨¡å—
ä½¿ç”¨ SiliconFlow API å¯¹è‹±æ–‡å­—å¹•è¿›è¡Œè¯­è¨€å­¦è§£æï¼Œç”Ÿæˆä¸­è‹±æ–‡å­—å¹•å’Œè¯¦ç»†çš„è§£æJSONæ–‡ä»¶
"""

import os
import time
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests


@dataclass
class SubtitleParserConfig:
    """å­—å¹•è§£æé…ç½®"""
    
    # API é…ç½®
    api_key: str = ""
    model: str = ""
    timeout: int = 0
    max_retries: int = 0
    max_concurrent_workers: int = 0


    # è¯·æ±‚é…ç½®
    base_url: str = "https://api.siliconflow.cn/v1"
    initial_retry_delay: float = 1.0
    max_retry_delay: float = 30.0


class SubtitleParser:
    """å­—å¹•è§£æå™¨ - ç”Ÿæˆä¸­è‹±æ–‡å­—å¹•å¹¶åˆ›å»ºè¯¦ç»†çš„è¯­è¨€å­¦è§£æJSONæ–‡ä»¶"""
    
    def __init__(self, config: SubtitleParserConfig):
        """
        åˆå§‹åŒ–å­—å¹•è§£æå™¨
        
        Args:
            config: è§£æé…ç½®
        """
        self.config = config
        
        if not self.config.api_key:
            raise RuntimeError("å­—å¹•è§£æå™¨åˆå§‹åŒ–å¤±è´¥: ç¼ºå°‘ SiliconFlow API å¯†é’¥")
    
    def parse_subtitle_files(self, subtitle_files: List[str], output_dir: str) -> List[str]:
        """
        æ‰¹é‡è§£æå­—å¹•æ–‡ä»¶ï¼Œç”Ÿæˆä¸­è‹±æ–‡å­—å¹•å’Œè¯­è¨€å­¦è§£æJSONæ–‡ä»¶
        
        Args:
            subtitle_files: å­—å¹•æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºæ ¹ç›®å½•
            
        Returns:
            æˆåŠŸè§£æçš„æ–‡ä»¶åˆ—è¡¨
        """        
        parsed_files = []
        total_files = len(subtitle_files)
        
        # åˆ›å»ºè§£æç»“æœç›®å½•
        analysis_dir = os.path.join(output_dir, "parsed_analysis")
        os.makedirs(analysis_dir, exist_ok=True)
        
        print(f"ğŸ”„ å¼€å§‹è§£æ {total_files} ä¸ªå­—å¹•æ–‡ä»¶...")
        
        total_stats = {
            'files_processed': 0,
            'files_failed': 0
        }
        
        for i, subtitle_file in enumerate(subtitle_files, 1):
            try:
                filename = os.path.basename(subtitle_file)
                print(f"\nğŸ” è§£æå­—å¹•æ–‡ä»¶ ({i}/{total_files}): {filename}")
                
                if self._parse_single_file(subtitle_file, analysis_dir):
                    parsed_files.append(subtitle_file)
                    total_stats['files_processed'] += 1
                else:
                    total_stats['files_failed'] += 1
                    total_stats['files_processed'] += 1
                
                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™æµ
                if i < total_files:
                    time.sleep(0.5)
                    
            except Exception as e:
                print(f"âŒ è§£ææ–‡ä»¶æ—¶å‡ºé”™ {os.path.basename(subtitle_file)}: {e}")
                total_stats['files_failed'] += 1
                total_stats['files_processed'] += 1
                continue
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        print(f"\nğŸ“Š è§£æå®Œæˆç»Ÿè®¡:")
        print(f"   ğŸ“ å¤„ç†æ–‡ä»¶: {total_stats['files_processed']}")
        print(f"   âŒ å¤±è´¥æ–‡ä»¶: {total_stats['files_failed']}")
        
        return parsed_files
    
    def _parse_single_file(self, subtitle_file: str, analysis_dir: str) -> bool:
        """
        è§£æå•ä¸ªå­—å¹•æ–‡ä»¶ï¼Œç”Ÿæˆä¸­è‹±æ–‡å­—å¹•å’ŒJSONè§£ææ–‡ä»¶
        æ”¯æŒå¢é‡è§£æï¼šåªé‡æ–°å¤„ç†å¤±è´¥çš„å­—å¹•è¡Œ
        
        Args:
            subtitle_file: å­—å¹•æ–‡ä»¶è·¯å¾„
            analysis_dir: è§£æç»“æœè¾“å‡ºç›®å½•
            
        Returns:
            æ˜¯å¦è§£ææˆåŠŸ
        """
        try:
            # è§£æSRTæ–‡ä»¶
            subtitle_entries = self._parse_srt_file(subtitle_file)
            if not subtitle_entries:
                print(f"âš ï¸ æ–‡ä»¶æ— æœ‰æ•ˆå­—å¹•: {os.path.basename(subtitle_file)}")
                return False
            
            # æ£€æŸ¥ç°æœ‰è§£æç»“æœ
            subtitle_name = os.path.splitext(os.path.basename(subtitle_file))[0]
            json_file = os.path.join(analysis_dir, f"{subtitle_name}.json")
            
            # è·å–éœ€è¦é‡æ–°å¤„ç†çš„å­—å¹•ç´¢å¼•ï¼ˆåŒ…æ‹¬å¤±è´¥å’Œæœªå¤„ç†çš„ï¼‰
            total_subtitles = len(subtitle_entries)
            unprocessed_indices = self._get_unprocessed_subtitle_indices(json_file, total_subtitles)
            
            if not unprocessed_indices:
                print(f"âœ… æ‰€æœ‰å­—å¹•å·²æˆåŠŸè§£æï¼Œè·³è¿‡æ–‡ä»¶: {os.path.basename(subtitle_file)}")
                return True
            
            # è¿‡æ»¤å‡ºéœ€è¦é‡æ–°è§£æçš„å­—å¹•æ¡ç›®
            entries_to_parse = [entry for entry in subtitle_entries 
                              if int(entry['index']) in unprocessed_indices]
            print(f"ğŸ”„ å‘ç° {len(unprocessed_indices)} æ¡éœ€è¦å¤„ç†çš„å­—å¹•ï¼Œå…± {total_subtitles} æ¡")
            print(f"ğŸ“ éœ€è¦é‡æ–°è§£æçš„å­—å¹•ç´¢å¼•: {sorted(list(unprocessed_indices))}")
            
            if not entries_to_parse:
                print(f"âœ… æ— éœ€é‡æ–°è§£æä»»ä½•å­—å¹•")
                return True
            
            # æ‰¹é‡è§£æéœ€è¦å¤„ç†çš„å­—å¹•æ¡ç›®
            newly_parsed_entries = self._parse_subtitle_entries(entries_to_parse)
            if not newly_parsed_entries:
                return False
            
            # å¦‚æœæ˜¯å¢é‡è§£æï¼Œéœ€è¦åˆå¹¶ç»“æœ
            if unprocessed_indices and os.path.exists(json_file):
                # è¯»å–ç°æœ‰çš„æ‰€æœ‰ç»“æœ
                existing_results = self._load_existing_analysis(json_file)
                
                # å°†æ–°è§£æçš„ç»“æœè½¬æ¢ä¸ºJSONæ ¼å¼
                new_analysis_results = []
                for entry in newly_parsed_entries:
                    analysis_json = {
                        "subtitle_index": int(entry['index']),
                        "english_text": entry['english_text']
                    }
                    
                    if 'analysis' in entry and entry['analysis']:
                        analysis_json.update(entry['analysis'])
                    else:
                        # å¦‚æœæ²¡æœ‰åˆ†æç»“æœï¼Œè®¾ç½®é»˜è®¤å€¼
                        analysis_json.update({
                            "translation": entry.get('chinese_text', f"[è§£æå¤±è´¥] {entry['english_text']}"),
                            "sentence_structure": "",
                            "key_words": [],
                            "fixed_phrases": [],
                            "core_grammar": [],
                            "colloquial_expression": []
                        })
                    
                    new_analysis_results.append(analysis_json)
                
                # åˆå¹¶ç°æœ‰ç»“æœä¸æ–°ç»“æœ
                merged_results = self._merge_analysis_results(existing_results, new_analysis_results)
                
                # æ›´æ–°å­—å¹•æ¡ç›®çš„ä¸­æ–‡ç¿»è¯‘
                for entry in subtitle_entries:
                    entry_index = int(entry['index'])
                    # æŸ¥æ‰¾å¯¹åº”çš„è§£æç»“æœ
                    for result in merged_results:
                        if result.get('subtitle_index') == entry_index:
                            entry['chinese_text'] = result.get('translation', f"[è§£æå¤±è´¥] {entry['english_text']}")
                            break
                
                # ä½¿ç”¨åˆå¹¶åçš„ç»“æœ
                final_parsed_entries = subtitle_entries
                
                # ç›´æ¥å†™å…¥åˆå¹¶åçš„JSONç»“æœ
                self._write_merged_analysis_json(merged_results, json_file)
            else:
                # é¦–æ¬¡è§£æï¼Œç›´æ¥ä½¿ç”¨æ–°ç»“æœ
                final_parsed_entries = newly_parsed_entries
                # å†™å…¥JSONè§£æç»“æœ
                self._write_analysis_json(final_parsed_entries, subtitle_file, analysis_dir)
            
            # å†™å›åŸæ–‡ä»¶ï¼ŒåŒ…å«ä¸­è‹±æ–‡
            self._write_bilingual_srt(final_parsed_entries, subtitle_file)
            
            return True
            
        except Exception as e:
            print(f"âŒ è§£æå•ä¸ªæ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def _parse_srt_file(self, srt_path: str) -> List[Dict]:
        """è§£æSRTæ–‡ä»¶"""
        subtitle_entries = []
        
        try:
            with open(srt_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            # æŒ‰ç©ºè¡Œåˆ†å‰²å­—å¹•æ¡ç›®
            blocks = content.split('\n\n')
            
            for block in blocks:
                lines = block.strip().split('\n')
                if len(lines) >= 3:
                    # è§£æå­—å¹•æ¡ç›®
                    index = lines[0].strip()
                    timestamp = lines[1].strip()
                    text = '\n'.join(lines[2:]).strip()
                    
                    subtitle_entries.append({
                        'index': index,
                        'timestamp': timestamp,
                        'english_text': text,
                        'chinese_text': ''
                    })
            
            return subtitle_entries
            
        except Exception as e:
            print(f"è§£æSRTæ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def _parse_subtitle_entries(self, subtitle_entries: List[Dict]) -> List[Dict]:
        """æ‰¹é‡è§£æå­—å¹•æ¡ç›®"""
        parsed_entries = []
        total_entries = len(subtitle_entries)
        
        # åˆ†æ‰¹å¤„ç†ï¼ˆæŒ‰å¹¶å‘æ•°åˆ†æ‰¹ï¼‰
        batch_size = self.config.max_concurrent_workers
        for i in range(0, total_entries, batch_size):
            batch = subtitle_entries[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (total_entries + batch_size - 1) // batch_size
            
            print(f"  è§£ææ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} æ¡å­—å¹•)")
            
            # è§£æå½“å‰æ‰¹æ¬¡
            parsed_batch = self._parse_batch(batch)
            if parsed_batch:
                parsed_entries.extend(parsed_batch)
            else:
                # è§£æå¤±è´¥ï¼Œä¿ç•™åŸè‹±æ–‡
                for entry in batch:
                    entry['chinese_text'] = f"[è§£æå¤±è´¥] {entry['english_text']}"
                    entry['analysis'] = {}
                parsed_entries.extend(batch)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…APIé™æµ
            if i + batch_size < total_entries:
                time.sleep(0.5)
        
        return parsed_entries
    
    def _get_unprocessed_subtitle_indices(self, json_file_path: str, total_subtitle_count: int) -> set:
        """
        è¯»å–ç°æœ‰JSONæ–‡ä»¶ï¼Œè¿”å›éœ€è¦é‡æ–°å¤„ç†çš„å­—å¹•ç´¢å¼•é›†åˆï¼ˆåŒ…æ‹¬å¤±è´¥å’Œæœªå¤„ç†çš„ï¼‰
        
        Args:
            json_file_path: JSONè§£æç»“æœæ–‡ä»¶è·¯å¾„
            total_subtitle_count: SRTæ–‡ä»¶ä¸­çš„æ€»å­—å¹•æ•°
            
        Returns:
            åŒ…å«éœ€è¦é‡æ–°å¤„ç†çš„å­—å¹•ç´¢å¼•çš„é›†åˆ
        """
        unprocessed_indices = set()
        processed_indices = set()
        
        if not os.path.exists(json_file_path):
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ‰€æœ‰å­—å¹•éƒ½éœ€è¦å¤„ç†
            return set(range(1, total_subtitle_count + 1))
            
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                
            # æŒ‰è¡Œåˆ†å‰²ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªJSONå¯¹è±¡
            for line_num, line in enumerate(content.split('\n'), 1):
                line = line.strip()
                if not line:
                    continue
                    
                try:
                    analysis_data = json.loads(line)
                    translation = analysis_data.get('translation', '')
                    subtitle_index = analysis_data.get('subtitle_index', line_num)
                    
                    # è®°å½•å·²å¤„ç†çš„ç´¢å¼•
                    processed_indices.add(subtitle_index)
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤±è´¥æ ‡è¯†
                    if translation.startswith('[è§£æå¤±è´¥]'):
                        unprocessed_indices.add(subtitle_index)
                        
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ è·³è¿‡æ— æ•ˆJSONè¡Œ {line_num}: {e}")
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ è¯»å–ç°æœ‰è§£ææ–‡ä»¶å¤±è´¥ {json_file_path}: {e}")
            # å‡ºé”™æ—¶è¿”å›æ‰€æœ‰ç´¢å¼•
            return set(range(1, total_subtitle_count + 1))
        
        # æ‰¾å‡ºæœªå¤„ç†çš„å­—å¹•ç´¢å¼•
        all_indices = set(range(1, total_subtitle_count + 1))
        missing_indices = all_indices - processed_indices
        
        # åˆå¹¶å¤±è´¥å’Œæœªå¤„ç†çš„ç´¢å¼•
        unprocessed_indices.update(missing_indices)
        
        return unprocessed_indices
    
    def _merge_analysis_results(self, existing_results: List[Dict], new_results: List[Dict]) -> List[Dict]:
        """
        åˆå¹¶ç°æœ‰è§£æç»“æœä¸æ–°è§£æç»“æœ
        
        Args:
            existing_results: ç°æœ‰çš„è§£æç»“æœåˆ—è¡¨
            new_results: æ–°çš„è§£æç»“æœåˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„è§£æç»“æœåˆ—è¡¨
        """
        # åˆ›å»ºç°æœ‰ç»“æœçš„ç´¢å¼•æ˜ å°„
        existing_map = {result.get('subtitle_index', 0): result for result in existing_results}
        
        # åˆ›å»ºæ–°ç»“æœçš„ç´¢å¼•æ˜ å°„
        new_map = {result.get('subtitle_index', 0): result for result in new_results}
        
        # åˆå¹¶ç»“æœï¼šæ–°ç»“æœä¼˜å…ˆï¼Œæœªæ›´æ–°çš„ä¿æŒåŸç»“æœ
        merged_results = []
        all_indices = set(existing_map.keys()) | set(new_map.keys())
        
        for index in sorted(all_indices):
            if index in new_map:
                merged_results.append(new_map[index])
            elif index in existing_map:
                merged_results.append(existing_map[index])
                
        return merged_results
    
    def _load_existing_analysis(self, json_file_path: str) -> List[Dict]:
        """
        åŠ è½½ç°æœ‰çš„è§£æç»“æœJSONæ–‡ä»¶
        
        Args:
            json_file_path: JSONæ–‡ä»¶è·¯å¾„
            
        Returns:
            è§£æç»“æœåˆ—è¡¨
        """
        existing_results = []
        
        if not os.path.exists(json_file_path):
            return existing_results
            
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                
            # æŒ‰è¡Œåˆ†å‰²ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªJSONå¯¹è±¡
            for line_num, line in enumerate(content.split('\n'), 1):
                line = line.strip()
                if not line:
                    continue
                    
                try:
                    analysis_data = json.loads(line)
                    existing_results.append(analysis_data)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ è·³è¿‡æ— æ•ˆJSONè¡Œ {line_num}: {e}")
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç°æœ‰è§£æç»“æœå¤±è´¥ {json_file_path}: {e}")
            
        return existing_results
    
    def _write_merged_analysis_json(self, merged_results: List[Dict], json_file_path: str):
        """
        å†™å…¥åˆå¹¶åçš„è§£æç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            merged_results: åˆå¹¶åçš„è§£æç»“æœåˆ—è¡¨
            json_file_path: JSONæ–‡ä»¶è·¯å¾„
        """
        try:
            with open(json_file_path, 'w', encoding='utf-8') as f:
                for result in merged_results:
                    json_line = json.dumps(result, ensure_ascii=False)
                    f.write(json_line + '\n')
                    
            print(f"âœ… åˆå¹¶è§£æç»“æœå·²å†™å…¥: {os.path.basename(json_file_path)}")
            
        except Exception as e:
            print(f"âŒ å†™å…¥åˆå¹¶è§£æç»“æœå¤±è´¥: {e}")
    
    def _parse_batch(self, batch: List[Dict]) -> Optional[List[Dict]]:
        """è§£æä¸€æ‰¹å­—å¹•æ¡ç›®ï¼ˆå¹¶å‘ç‰ˆæœ¬ï¼‰"""
        print(f"    ğŸš€ å¼€å§‹å¹¶å‘è§£æ {len(batch)} æ¡å­—å¹•...")
        
        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘å¤„ç†
        max_workers = min(len(batch), self.config.max_concurrent_workers)
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_entry = {
                executor.submit(self._analyze_single_sentence_safe, entry): entry 
                for entry in batch
            }
            
            parsed_batch = []
            completed_count = 0
            for future in as_completed(future_to_entry):
                entry = future_to_entry[future]
                try:
                    analysis_result = future.result()
                    if analysis_result:
                        entry['chinese_text'] = analysis_result.get('translation', f"[ç¿»è¯‘å¤±è´¥] {entry['english_text']}")
                        entry['analysis'] = analysis_result
                    else:
                        entry['chinese_text'] = f"[è§£æå¤±è´¥] {entry['english_text']}"
                        entry['analysis'] = {}
                    
                    parsed_batch.append(entry)
                    completed_count += 1
                    print(f"    âœ… è§£æå®Œæˆå­—å¹• {entry['index']} ({completed_count}/{len(batch)})")
                    
                except Exception as e:
                    print(f"    âŒ è§£æå­—å¹• {entry['index']} å¤±è´¥: {e}")
                    entry['chinese_text'] = f"[è§£æå¤±è´¥] {entry['english_text']}"
                    entry['analysis'] = {}
                    parsed_batch.append(entry)
        
        # æŒ‰åŸå§‹é¡ºåºæ’åº
        parsed_batch.sort(key=lambda x: int(x['index']))
        return parsed_batch
    
    def _analyze_single_sentence_safe(self, entry: Dict) -> Optional[Dict]:
        """
        çº¿ç¨‹å®‰å…¨çš„å•ä¸ªå¥å­åˆ†ææ–¹æ³•
        
        Args:
            entry: å­—å¹•æ¡ç›®å­—å…¸
            
        Returns:
            è§£æç»“æœæˆ–None
        """
        try:
            english_text = entry['english_text']
            print(f"    ğŸ“ è§£æå­—å¹• {entry['index']}: {english_text}")
            
            # è°ƒç”¨ç°æœ‰çš„åˆ†ææ–¹æ³•
            return self._analyze_single_sentence(english_text)
            
        except Exception as e:
            print(f"    âŒ çº¿ç¨‹è§£æå¤±è´¥: {e}")
            return None
    
    def _analyze_single_sentence(self, english_text: str) -> Optional[Dict]:
        """
        åˆ†æå•ä¸ªè‹±æ–‡å¥å­ï¼Œè¿”å›è¯¦ç»†çš„è¯­è¨€å­¦è§£æç»“æœ
        
        Args:
            english_text: è‹±æ–‡å¥å­
            
        Returns:
            åŒ…å«ç¿»è¯‘å’Œè¯­è¨€å­¦åˆ†æçš„å­—å…¸ï¼Œæˆ–Noneè¡¨ç¤ºå¤±è´¥
        """
        # æ„å»ºè¯­è¨€å­¦åˆ†ææç¤ºè¯
        system_prompt = """IMPORTANT: åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦ä»»ä½•é¢å¤–æ–‡å­—æˆ–è§£é‡Šã€‚

ä½œä¸ºè‹±è¯­è¯­è¨€å­¦å®¶ï¼Œè¯·åˆ†æç”¨æˆ·è¾“å…¥çš„è‹±è¯­å¥å­ï¼Œå¹¶ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºã€‚

å­—æ®µè¦æ±‚:
- translation: ä¸­æ–‡ç¿»è¯‘ï¼Œè¦æ±‚ä¿¡è¾¾é›…ï¼ˆå‡†ç¡®ä¼ è¾¾åŸæ„ï¼Œè¯­è¨€è‡ªç„¶æµç•…ï¼Œè¡¨è¾¾ä¼˜ç¾ï¼‰
- sentence_structure: å¥æ³•æˆåˆ†åˆ†æï¼ˆä¸»è¯­+è°“è¯­+å®¾è¯­+çŠ¶è¯­ç­‰ï¼‰
- key_words: å¥å­ä¸­æœ‰æ„ä¹‰çš„è¯æ±‡ï¼Œæ’é™¤theã€aã€isã€Mrs.ç­‰å¸¸è§è¯
- fixed_phrases: æœ‰å›ºå®šå«ä¹‰çš„çŸ­è¯­æ­é…ï¼Œæ’é™¤è¿‡äºç®€å•çš„ç»„åˆ
- core_grammar: é‡è¦è¯­æ³•ç°è±¡ï¼ˆæ—¶æ€ã€è¯­æ€ã€å¥å¼ç­‰ï¼‰
- colloquial_expression: æ­£å¼ä¸å£è¯­è¡¨è¾¾å¯¹æ¯”

è¾“å‡ºæ ¼å¼:
{
  "translation": "ä¸­æ–‡ç¿»è¯‘",
  "sentence_structure": "å¥å­ç»“æ„åˆ†æ",
  "key_words": [{"word": "å•è¯", "pos": "è¯æ€§", "meaning": "å«ä¹‰", "pronunciation": "éŸ³æ ‡"}],
  "fixed_phrases": [{"phrase": "çŸ­è¯­", "meaning": "å«ä¹‰"}],
  "core_grammar": [{"point": "è¯­æ³•ç‚¹", "explanation": "è§£é‡Š"}],
  "colloquial_expression": [{"formal": "æ­£å¼è¡¨è¾¾", "informal": "å£è¯­è¡¨è¾¾", "explanation": "ç”¨æ³•è¯´æ˜"}]
}

ç¤ºä¾‹1 (å¤åˆå¥):
è¾“å…¥: "The project that we've been working on for months, which involves multiple stakeholders, will be completed once we receive the final approval."
è¾“å‡º:
{
  "translation": "æˆ‘ä»¬å·²ç»å·¥ä½œäº†å‡ ä¸ªæœˆçš„è¿™ä¸ªé¡¹ç›®ï¼Œæ¶‰åŠå¤šä¸ªåˆ©ç›Šç›¸å…³è€…ï¼Œä¸€æ—¦æˆ‘ä»¬æ”¶åˆ°æœ€ç»ˆæ‰¹å‡†å°±ä¼šå®Œæˆã€‚",
  "sentence_structure": "ä¸»è¯­(The project) + å®šè¯­ä»å¥1(that we've been working on for months) + å®šè¯­ä»å¥2(which involves multiple stakeholders) + è°“è¯­(will be completed) + æ—¶é—´çŠ¶è¯­ä»å¥(once we receive the final approval)",
  "key_words": [{"word": "stakeholders", "pos": "n.", "meaning": "åˆ©ç›Šç›¸å…³è€…", "pronunciation": "/ËˆsteÉªkhoÊŠldÉ™rz/"}, {"word": "approval", "pos": "n.", "meaning": "æ‰¹å‡†ï¼ŒåŒæ„", "pronunciation": "/É™ËˆpruËvÉ™l/"}],
  "fixed_phrases": [{"phrase": "work on", "meaning": "ä»äº‹ï¼Œè‡´åŠ›äº"}],
  "core_grammar": [{"point": "å®šè¯­ä»å¥åµŒå¥—", "explanation": "ä¸¤ä¸ªå®šè¯­ä»å¥ä¿®é¥°åŒä¸€ä¸»è¯­ï¼Œ'that'å¼•å¯¼é™åˆ¶æ€§å®šè¯­ä»å¥ï¼Œ'which'å¼•å¯¼éé™åˆ¶æ€§å®šè¯­ä»å¥"}],
  "colloquial_expression": [{"formal": "receive the final approval", "informal": "get the green light", "explanation": "'get the green light'è¡¨ç¤ºè·å¾—è®¸å¯ï¼Œæ¯”'receive approval'æ›´ç”ŸåŠ¨"}]
}

ç¤ºä¾‹2 (ç®€å•å¥):
è¾“å…¥: "She is very happy."
è¾“å‡º:
{
  "translation": "å¥¹å¾ˆå¼€å¿ƒã€‚",
  "sentence_structure": "ä¸»è¯­(She) + ç³»åŠ¨è¯(is) + è¡¨è¯­(very happy)",
  "key_words": [],
  "fixed_phrases": [],
  "core_grammar": [],
  "colloquial_expression": []
}

æ³¨æ„: æ— ç›¸å…³å†…å®¹æ—¶å­—æ®µç•™ç©º(ç©ºæ•°ç»„[]æˆ–ç©ºå­—ç¬¦ä¸²"")ï¼Œä½†ä¸å¯çœç•¥å­—æ®µã€‚è®°ä½ï¼šä»…è¾“å‡ºJSONæ ¼å¼ã€‚"""

        user_prompt = f"è¯·åˆ†æä»¥ä¸‹è‹±è¯­å¥å­: \"{english_text}\""
        
        try:
            # è°ƒç”¨APIè¿›è¡Œåˆ†æ
            response = self._call_analysis_api(system_prompt, user_prompt)
            if not response:
                return None
            
            # è§£æJSONå“åº”
            return self._parse_analysis_response(response)
            
        except Exception as e:
            print(f"    âŒ å¥å­åˆ†æAPIè°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _call_analysis_api(self, system_prompt: str, user_prompt: str) -> Optional[str]:
        """è°ƒç”¨APIè¿›è¡Œè¯­è¨€å­¦åˆ†æ"""
        url = f"{self.config.base_url}/chat/completions"
        
        headers = {
            "accept": "application/json",
            "content-type": "application/json",
            "authorization": f"Bearer {self.config.api_key}"
        }
        
        payload = {
            "model": self.config.model,
            "messages": [
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user", 
                    "content": user_prompt
                }
            ],
            "stream": False,
            "max_tokens": 1500,
            "temperature": 0.2  # è¾ƒä½æ¸©åº¦ç¡®ä¿ç»“æœç¨³å®š
        }
        
        # å¸¦é‡è¯•çš„APIè°ƒç”¨
        retry_delay = self.config.initial_retry_delay
        
        for attempt in range(self.config.max_retries + 1):
            try:
                response = requests.post(url, json=payload, headers=headers, timeout=self.config.timeout)
                response.raise_for_status()
                
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    content = result['choices'][0]['message']['content']
                    return content.strip()
                else:
                    raise RuntimeError(f"APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")
                    
            except Exception as e:
                if attempt == self.config.max_retries:
                    print(f"    âŒ åˆ†æAPIè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°({self.config.max_retries + 1}æ¬¡): {e}")
                    return None
                
                print(f"    âš ï¸ åˆ†æAPIè°ƒç”¨å¤±è´¥(ç¬¬{attempt + 1}æ¬¡å°è¯•): {e}")
                time.sleep(retry_delay)
                retry_delay = min(retry_delay * 2, self.config.max_retry_delay)
        
        return None
    
    def _parse_analysis_response(self, response: str) -> Optional[Dict]:
        """è§£æAPIè¿”å›çš„è¯­è¨€å­¦åˆ†æç»“æœ"""
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            analysis = json.loads(response)
            
            # éªŒè¯å¿…è¦å­—æ®µ
            required_fields = ['translation', 'sentence_structure', 'key_words', 'fixed_phrases', 'core_grammar', 'colloquial_expression']
            for field in required_fields:
                if field not in analysis:
                    analysis[field] = [] if field != 'translation' and field != 'sentence_structure' else ""
            
            return analysis
            
        except json.JSONDecodeError as e:
            print(f"    âŒ JSONè§£æå¤±è´¥: {e}")
            # å°è¯•æå–å¯èƒ½çš„JSONç‰‡æ®µ
            try:
                # æŸ¥æ‰¾JSONå¼€å§‹å’Œç»“æŸæ ‡è®°
                start_idx = response.find('{')
                end_idx = response.rfind('}')
                if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                    json_text = response[start_idx:end_idx+1]
                    analysis = json.loads(json_text)
                    
                    # éªŒè¯å­—æ®µ
                    required_fields = ['translation', 'sentence_structure', 'key_words', 'fixed_phrases', 'core_grammar', 'colloquial_expression']
                    for field in required_fields:
                        if field not in analysis:
                            analysis[field] = [] if field != 'translation' and field != 'sentence_structure' else ""
                    
                    return analysis
            except:
                pass
            
            return None
        except Exception as e:
            print(f"    âŒ è§£æå“åº”å¤±è´¥: {e}")
            return None
    
    def _write_analysis_json(self, parsed_entries: List[Dict], subtitle_file: str, analysis_dir: str):
        """å°†è§£æç»“æœå†™å…¥JSONæ–‡ä»¶"""
        try:
            # è·å–å­—å¹•æ–‡ä»¶åï¼ˆä¸åŒ…å«æ‰©å±•åï¼‰
            subtitle_name = os.path.splitext(os.path.basename(subtitle_file))[0]
            json_file = os.path.join(analysis_dir, f"{subtitle_name}.json")
            
            with open(json_file, 'w', encoding='utf-8') as f:
                for entry in parsed_entries:
                    # æ„å»ºè¾“å‡ºJSONå¯¹è±¡
                    output_json = {
                        "subtitle_index": int(entry['index']),
                        "english_text": entry['english_text']
                    }
                    
                    # æ·»åŠ åˆ†æç»“æœ
                    if 'analysis' in entry and entry['analysis']:
                        output_json.update(entry['analysis'])
                    else:
                        # å¦‚æœæ²¡æœ‰åˆ†æç»“æœï¼Œè®¾ç½®é»˜è®¤å€¼
                        output_json.update({
                            "translation": entry.get('chinese_text', ''),
                            "sentence_structure": "",
                            "key_words": [],
                            "fixed_phrases": [],
                            "core_grammar": [],
                            "colloquial_expression": []
                        })
                    
                    # å†™å…¥ä¸€è¡ŒJSON
                    f.write(json.dumps(output_json, ensure_ascii=False, separators=(',', ':')) + '\n')
            
            print(f"    âœ… è§£æç»“æœå·²ä¿å­˜åˆ°: {json_file}")
            
        except Exception as e:
            print(f"    âŒ ä¿å­˜è§£æç»“æœå¤±è´¥: {e}")
    
    def _call_api(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨ SiliconFlow API"""
        url = f"{self.config.base_url}/chat/completions"
        
        headers = {
            "accept": "application/json",
            "content-type": "application/json",
            "authorization": f"Bearer {self.config.api_key}"
        }
        
        payload = {
            "model": self.config.model,
            "messages": [
                {
                    "role": "system", 
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‹±ä¸­ç¿»è¯‘ä¸“å®¶ï¼Œä¸“é—¨ç¿»è¯‘è‹±æ–‡å­—å¹•ã€‚ä½ çš„ç¿»è¯‘å‡†ç¡®ã€è‡ªç„¶ã€ç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯ã€‚"
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            "stream": False,
            "max_tokens": 2000,
            "temperature": 0.3
        }
        
        # å¸¦æŒ‡æ•°é€€é¿çš„é‡è¯•æœºåˆ¶
        retry_delay = self.config.initial_retry_delay
        
        for attempt in range(self.config.max_retries + 1):
            try:
                response = requests.post(url, json=payload, headers=headers, timeout=self.config.timeout)
                response.raise_for_status()
                
                result = response.json()
                if 'choices' in result and len(result['choices']) > 0:
                    content = result['choices'][0]['message']['content']
                    if attempt > 0:
                        print(f"âœ… APIè°ƒç”¨åœ¨ç¬¬{attempt + 1}æ¬¡å°è¯•åæˆåŠŸ")
                    return content.strip()
                else:
                    raise RuntimeError(f"APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")
                    
            except Exception as e:
                if attempt == self.config.max_retries:
                    print(f"âŒ APIè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°({self.config.max_retries + 1}æ¬¡)")
                    print(f"æœ€åé”™è¯¯: {e}")
                    return None
                
                print(f"âš ï¸ APIè°ƒç”¨å¤±è´¥(ç¬¬{attempt + 1}æ¬¡å°è¯•): {e}")
                print(f"å°†åœ¨{retry_delay}ç§’åé‡è¯•...")
                time.sleep(retry_delay)
                
                # æŒ‡æ•°é€€é¿ï¼Œä½†ä¸è¶…è¿‡æœ€å¤§å»¶è¿Ÿ
                retry_delay = min(retry_delay * 2, self.config.max_retry_delay)
        
        return None
    
    
    def _write_bilingual_srt(self, translated_entries: List[Dict], output_path: str):
        """å†™å…¥ä¸­è‹±æ–‡åŒè¯­SRTæ–‡ä»¶ï¼Œç›´æ¥è¦†ç›–åŸæ–‡ä»¶"""
        with open(output_path, 'w', encoding='utf-8') as f:
            for entry in translated_entries:
                f.write(f"{entry['index']}\n")
                f.write(f"{entry['timestamp']}\n")
                f.write(f"{entry['english_text']}\n")
                f.write(f"{entry['chinese_text']}\n\n")
    
    def translate_chapter_titles(self, chapter_titles: List[str]) -> List[str]:
        """
        ç¿»è¯‘ç« èŠ‚æ ‡é¢˜åˆ—è¡¨ï¼ˆå¹¶å‘å¤„ç†ï¼‰
        
        Args:
            chapter_titles: è‹±æ–‡ç« èŠ‚æ ‡é¢˜åˆ—è¡¨
            
        Returns:
            ä¸­æ–‡ç« èŠ‚æ ‡é¢˜åˆ—è¡¨
        """
        if not chapter_titles:
            return []
        
        print(f"ğŸŒ æ­£åœ¨å¹¶å‘ç¿»è¯‘ {len(chapter_titles)} ä¸ªç« èŠ‚æ ‡é¢˜...")
        
        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘å¤„ç†
        max_workers = min(len(chapter_titles), self.config.max_concurrent_workers)
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ç¿»è¯‘ä»»åŠ¡
            future_to_index = {
                executor.submit(self._translate_single_title, title, i): i
                for i, title in enumerate(chapter_titles)
            }
            
            # å­˜å‚¨ç»“æœï¼ˆæŒ‰åŸå§‹é¡ºåºï¼‰
            translated_titles = [''] * len(chapter_titles)
            completed_count = 0
            
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    translated_title = future.result()
                    translated_titles[index] = translated_title if translated_title else chapter_titles[index]
                    completed_count += 1
                    print(f"  âœ… ç¿»è¯‘å®Œæˆæ ‡é¢˜ {index + 1}/{len(chapter_titles)}: {chapter_titles[index]} -> {translated_titles[index]}")
                    
                except Exception as e:
                    print(f"  âŒ ç¿»è¯‘æ ‡é¢˜ {index + 1} å¤±è´¥: {e}")
                    translated_titles[index] = chapter_titles[index]  # ä¿ç•™åŸæ ‡é¢˜
        
        print(f"âœ… ç« èŠ‚æ ‡é¢˜ç¿»è¯‘å®Œæˆ")
        return translated_titles
    
    def _translate_single_title(self, title: str, index: int) -> Optional[str]:
        """
        ç¿»è¯‘å•ä¸ªç« èŠ‚æ ‡é¢˜ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        
        Args:
            title: è‹±æ–‡ç« èŠ‚æ ‡é¢˜
            index: æ ‡é¢˜ç´¢å¼•ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            ä¸­æ–‡ç¿»è¯‘æˆ–Noneï¼ˆè¡¨ç¤ºå¤±è´¥ï¼‰
        """
        try:
            print(f"  ğŸ“ ç¿»è¯‘æ ‡é¢˜ {index + 1}: {title}")
            
            # ä½¿ç”¨ç®€åŒ–çš„ç¿»è¯‘prompt
            system_prompt = "ä½ æ˜¯ä¸“ä¸šçš„è‹±ä¸­ç¿»è¯‘ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·è¾“å…¥çš„è‹±æ–‡ç« èŠ‚æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿æŒç®€æ´ä¼˜é›…ã€‚åªè¿”å›ç¿»è¯‘ç»“æœï¼Œæ— éœ€å…¶ä»–å†…å®¹ã€‚"
            user_prompt = f"è¯·ç¿»è¯‘ä»¥ä¸‹ç« èŠ‚æ ‡é¢˜: \"{title}\""
            
            response = self._call_analysis_api(system_prompt, user_prompt)
            if response and response.strip():
                return response.strip()
            else:
                print(f"  âš ï¸ æ ‡é¢˜ {index + 1} APIè¿”å›ç©ºç»“æœ")
                return None
                
        except Exception as e:
            print(f"  âŒ ç¿»è¯‘æ ‡é¢˜ {index + 1} å¼‚å¸¸: {e}")
            return None

    def test_connection(self) -> bool:
        """æµ‹è¯•APIè¿æ¥"""
        print("æ­£åœ¨æµ‹è¯• SiliconFlow API è¿æ¥...")
        try:
            response = self._call_analysis_api("ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹", "è¯·å›ç­”ï¼šä½ å¥½")
            if response:
                print(f"âœ… APIè¿æ¥æˆåŠŸï¼Œå“åº”: {response[:50]}...")
                return True
            else:
                print("âŒ APIè¿æ¥å¤±è´¥")
                return False
        except Exception as e:
            print(f"âŒ APIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False
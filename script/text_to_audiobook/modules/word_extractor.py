#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å•è¯æå–æ¨¡å—
ä»ç« èŠ‚æ–‡æœ¬ä¸­æå–å•è¯ï¼Œè¿›è¡Œè¯å½¢è¿˜åŸï¼Œè¿‡æ»¤å¤„ç†
"""

import os
import re
import json
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple
from dataclasses import dataclass
from collections import defaultdict

# éœ€è¦å®‰è£…: pip install spacy nltk
# ä¸‹è½½æ¨¡å‹: python -m spacy download en_core_web_sm
try:
    import spacy
    import nltk
    from nltk.corpus import stopwords
    from nltk.corpus import names
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {e}")
    print("è¯·å®‰è£…: pip install spacy nltk")
    print("å¹¶ä¸‹è½½æ¨¡å‹: python -m spacy download en_core_web_sm")
    raise


@dataclass
class WordExtractionConfig:
    """å•è¯æå–é…ç½®"""
    
    # è¿‡æ»¤é…ç½®
    min_word_length: int = 2
    max_word_length: int = 20
    filter_numbers: bool = True
    filter_proper_nouns: bool = True
    filter_stop_words: bool = True
    filter_names: bool = True
    
    # è¾“å‡ºé…ç½®
    vocabulary_subdir: str = "vocabulary"
    chapters_subdir: str = "chapters"
    
    # SpaCyæ¨¡å‹
    spacy_model: str = "en_core_web_sm"


class WordExtractor:
    """å•è¯æå–å™¨ - ä»æ–‡æœ¬ä¸­æå–å’Œæ ‡å‡†åŒ–å•è¯"""
    
    def __init__(self, config: WordExtractionConfig):
        """
        åˆå§‹åŒ–å•è¯æå–å™¨
        
        Args:
            config: æå–é…ç½®
        """
        self.config = config
        
        # åŠ è½½SpaCyæ¨¡å‹
        try:
            self.nlp = spacy.load(self.config.spacy_model)
            print(f"âœ… SpaCyæ¨¡å‹åŠ è½½æˆåŠŸ: {self.config.spacy_model}")
        except OSError:
            raise RuntimeError(f"SpaCyæ¨¡å‹åŠ è½½å¤±è´¥: {self.config.spacy_model}ï¼Œè¯·è¿è¡Œ: python -m spacy download {self.config.spacy_model}")
        
        # ä¸‹è½½NLTKæ•°æ®
        self._ensure_nltk_data()
        
        # è·å–åœç”¨è¯å’Œäººååˆ—è¡¨
        self.stop_words = set(stopwords.words('english'))
        self.names_list = set(names.words())
        
        print(f"ğŸ“ å•è¯æå–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _ensure_nltk_data(self):
        """ç¡®ä¿NLTKæ•°æ®å·²ä¸‹è½½"""
        try:
            nltk.data.find('corpora/stopwords')
            nltk.data.find('corpora/names')
        except LookupError:
            print("ğŸ“¥ ä¸‹è½½NLTKæ•°æ®...")
            nltk.download('stopwords', quiet=True)
            nltk.download('names', quiet=True)
    
    def extract_subchapter_words(self, sentence_files: List[str], output_dir: str, master_vocab_path: str) -> Tuple[List[str], List[str]]:
        """
        ä»å¥å­æ–‡ä»¶ä¸­æå–å­ç« èŠ‚è¯æ±‡ï¼ˆä»¥å­ç« èŠ‚ä¸ºå•ä½ï¼‰
        
        Args:
            sentence_files: å¥å­æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            master_vocab_path: æ€»è¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„
            
        Returns:
            (å¤„ç†çš„å­ç« èŠ‚è¯æ±‡æ–‡ä»¶åˆ—è¡¨, æ‰€æœ‰æ–°è¯åˆ—è¡¨)
        """
        # åŠ è½½å·²æœ‰çš„æ€»è¯æ±‡è¡¨
        existing_vocab = self._load_master_vocabulary(master_vocab_path)
        print(f"ğŸ“ åŠ è½½ç°æœ‰è¯æ±‡è¡¨: {len(existing_vocab)} ä¸ªå•è¯")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        vocab_dir = os.path.join(output_dir, self.config.vocabulary_subdir)
        subchapters_dir = os.path.join(vocab_dir, "subchapters")
        os.makedirs(subchapters_dir, exist_ok=True)
        
        subchapter_vocab_files = []
        all_new_words = set()
        
        # æŒ‰å­ç« èŠ‚å¤„ç†å¥å­æ–‡ä»¶ï¼ˆæ¯ä¸ªå¥å­æ–‡ä»¶å¯¹åº”ä¸€ä¸ªå­ç« èŠ‚ï¼‰
        for sentence_file in sentence_files:
            # è·å–å­ç« èŠ‚åç§°
            filename = os.path.basename(sentence_file)
            subchapter_name = os.path.splitext(filename)[0]
            
            print(f"ğŸ“ å¤„ç†å­ç« èŠ‚: {subchapter_name}")
            
            # æå–å­ç« èŠ‚æ‰€æœ‰å•è¯
            all_words, filtered_words = self._extract_words_from_files([sentence_file])
            
            # åˆ†ç¦»æ–°è¯å’Œå·²çŸ¥è¯
            known_words = [word for word in all_words if word in existing_vocab]
            new_words = [word for word in all_words if word not in existing_vocab]
            
            # æ”¶é›†æ‰€æœ‰æ–°è¯
            all_new_words.update(new_words)
            
            # ä¿å­˜å­ç« èŠ‚è¯æ±‡æ–‡ä»¶ï¼ˆåªåŒ…å«å•è¯æ–‡æœ¬ï¼‰
            subchapter_vocab_data = {
                "subchapter_id": subchapter_name,
                "words": sorted(list(set(all_words))),  # æå–æ‰€æœ‰å•è¯ï¼ˆåŒ…æ‹¬æ–°è¯å’Œå·²çŸ¥è¯ï¼‰
                "word_count": len(set(all_words)),
                "filtered_words": sorted(list(set(filtered_words))),
                "new_words_count": len(new_words),
                "known_words_count": len(known_words)
            }
            
            subchapter_vocab_file = os.path.join(subchapters_dir, f"{subchapter_name}.json")
            self._save_json(subchapter_vocab_data, subchapter_vocab_file)
            subchapter_vocab_files.append(subchapter_vocab_file)
            
            print(f"  ğŸ“„ å·²ä¿å­˜å­ç« èŠ‚è¯æ±‡: {subchapter_vocab_file}")
            print(f"  ğŸ“ˆ è¯æ±‡ç»Ÿè®¡: æ€»è®¡{len(set(all_words))}ä¸ª, æ–°è¯{len(new_words)}ä¸ª, å·²çŸ¥{len(known_words)}ä¸ª")
        
        print(f"\nğŸ“ å­ç« èŠ‚è¯æ±‡æå–å®Œæˆï¼Œå‘ç° {len(all_new_words)} ä¸ªæ–°è¯éœ€è¦å¤„ç†")
        return subchapter_vocab_files, list(all_new_words)
    
    def extract_chapter_words(self, sentence_files: List[str], output_dir: str, master_vocab_path: str) -> Tuple[List[str], List[str]]:
        """
        ä»å¥å­æ–‡ä»¶ä¸­æå–ç« èŠ‚è¯æ±‡ï¼ˆä¿ç•™ä»¥å…¼å®¹æ—§æ¥å£ï¼‰
        
        Args:
            sentence_files: å¥å­æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            master_vocab_path: æ€»è¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„
            
        Returns:
            (å¤„ç†çš„ç« èŠ‚è¯æ±‡æ–‡ä»¶åˆ—è¡¨, æ‰€æœ‰æ–°è¯åˆ—è¡¨)
        """
        print("âš ï¸ ä½¿ç”¨æ—§çš„ç« èŠ‚çº§åˆ«æ¥å£ï¼Œå»ºè®®ä½¿ç”¨ extract_subchapter_words æ–¹æ³•")
        return self.extract_subchapter_words(sentence_files, output_dir, master_vocab_path)
    
    def _group_sentence_files_by_chapter(self, sentence_files: List[str]) -> Dict[str, List[str]]:
        """æŒ‰ç« èŠ‚åç§°åˆ†ç»„å¥å­æ–‡ä»¶"""
        chapter_groups = defaultdict(list)
        
        for sentence_file in sentence_files:
            # ä»æ–‡ä»¶åæå–ç« èŠ‚åç§° (ä¾‹: 01_Down_the_Rabbit-Hole(1).txt -> 01_Down_the_Rabbit-Hole)
            filename = os.path.basename(sentence_file)
            base_name = os.path.splitext(filename)[0]
            
            # ç§»é™¤æ‹¬å·éƒ¨åˆ†ï¼Œè·å–ç« èŠ‚åŸºç¡€åç§°
            chapter_name = re.sub(r'\([^)]*\)$', '', base_name)
            chapter_groups[chapter_name].append(sentence_file)
        
        return dict(chapter_groups)
    
    def _extract_words_from_files(self, sentence_files: List[str]) -> Tuple[List[str], List[str]]:
        """
        ä»æ–‡ä»¶åˆ—è¡¨ä¸­æå–å•è¯
        
        Returns:
            (æœ‰æ•ˆå•è¯åˆ—è¡¨, è¢«è¿‡æ»¤å•è¯åˆ—è¡¨)
        """
        all_text = ""
        
        # è¯»å–æ‰€æœ‰æ–‡ä»¶å†…å®¹
        for sentence_file in sentence_files:
            try:
                with open(sentence_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # è·³è¿‡ç¬¬ä¸€è¡Œæ ‡é¢˜
                    lines = content.strip().split('\n')
                    if len(lines) > 1:
                        all_text += " " + " ".join(lines[1:])
            except Exception as e:
                print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥: {sentence_file}, {e}")
                continue
        
        return self._extract_words_from_text(all_text)
    
    def _extract_words_from_text(self, text: str) -> Tuple[List[str], List[str]]:
        """
        ä»æ–‡æœ¬ä¸­æå–å•è¯å¹¶è¿›è¡Œè¯å½¢è¿˜åŸå’Œè¿‡æ»¤
        
        Returns:
            (æœ‰æ•ˆå•è¯åˆ—è¡¨, è¢«è¿‡æ»¤å•è¯åˆ—è¡¨)
        """
        if not text.strip():
            return [], []
        
        # ä½¿ç”¨SpaCyå¤„ç†æ–‡æœ¬
        doc = self.nlp(text)
        
        valid_words = []
        filtered_words = []
        
        for token in doc:
            original = token.text.lower().strip()
            
            # åŸºç¡€è¿‡æ»¤
            if not original or not original.isalpha():
                filtered_words.append(original)
                continue
            
            # é•¿åº¦è¿‡æ»¤
            if len(original) < self.config.min_word_length or len(original) > self.config.max_word_length:
                filtered_words.append(original)
                continue
            
            # åº”ç”¨å„ç§è¿‡æ»¤è§„åˆ™
            filter_reason = self._get_filter_reason(token, original)
            if filter_reason:
                filtered_words.append(original)
                continue
            
            # åªå¯¹å¤æ•°å’Œç¬¬ä¸‰äººç§°å•æ•°è¿›è¡Œå½’ä¸€åŒ–
            normalized_word = self._normalize_word_selective(token, original)
            valid_words.append(normalized_word)
        
        return valid_words, filtered_words
    
    def _get_filter_reason(self, token, word: str) -> Optional[str]:
        """
        æ£€æŸ¥å•è¯æ˜¯å¦åº”è¯¥è¢«è¿‡æ»¤
        
        Returns:
            è¿‡æ»¤åŸå› ï¼Œå¦‚æœä¸éœ€è¦è¿‡æ»¤è¿”å›None
        """
        # åœç”¨è¯è¿‡æ»¤
        if self.config.filter_stop_words and word in self.stop_words:
            return "stop_word"
        
        # ä¸“æœ‰åè¯è¿‡æ»¤
        if self.config.filter_proper_nouns and token.pos_ == "PROPN":
            return "proper_noun"
        
        # äººåè¿‡æ»¤
        if self.config.filter_names and token.text in self.names_list:
            return "person_name"
        
        # æ•°å­—è¿‡æ»¤
        if self.config.filter_numbers and token.like_num:
            return "number"
        
        return None
    
    def _normalize_word_selective(self, token, word: str) -> str:
        """
        é€‰æ‹©æ€§è¯å½¢å½’ä¸€åŒ–ï¼šåªå½’ä¸€åŒ–å¤æ•°å’Œç¬¬ä¸‰äººç§°å•æ•°å½¢å¼
        
        Args:
            token: SpaCy tokenå¯¹è±¡
            word: åŸå§‹å•è¯
            
        Returns:
            å½’ä¸€åŒ–åçš„å•è¯
        """
        lemma = token.lemma_.lower().strip()
        
        # å¦‚æœåŸè¯å’Œè¯æ ¹ç›¸åŒï¼Œæ— éœ€å½’ä¸€åŒ–
        if word == lemma:
            return word
        
        # æ£€æŸ¥è¯æ€§å’Œå˜åŒ–ç±»å‹
        pos = token.pos_
        tag = token.tag_
        
        # åè¯å¤æ•°å½’ä¸€åŒ– (NNS -> NN)
        if pos == "NOUN" and tag == "NNS":
            return lemma
        
        # åŠ¨è¯ç¬¬ä¸‰äººç§°å•æ•°å½’ä¸€åŒ– (VBZ -> VB)
        if pos == "VERB" and tag == "VBZ":
            return lemma
        
        # å…¶ä»–æƒ…å†µä¿æŒåŸå½¢ï¼ˆåŒ…æ‹¬åŠ¨è¯æ—¶æ€VBD/VBG/VBNå’Œå½¢å®¹è¯æ¯”è¾ƒçº§JJR/JJSï¼‰
        return word
    
    def _load_master_vocabulary(self, master_vocab_path: str) -> Dict[str, Dict]:
        """åŠ è½½æ€»è¯æ±‡è¡¨"""
        if not os.path.exists(master_vocab_path):
            print(f"ğŸ“ æ€»è¯æ±‡è¡¨ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶: {master_vocab_path}")
            return {}
        
        try:
            with open(master_vocab_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('vocabulary', {})
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æ€»è¯æ±‡è¡¨å¤±è´¥: {e}ï¼Œä½¿ç”¨ç©ºè¯æ±‡è¡¨")
            return {}
    
    def _save_json(self, data: dict, file_path: str):
        """ä¿å­˜JSONæ•°æ®åˆ°æ–‡ä»¶"""
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å•è¯æå–æ¨¡å—
ä»ç« èŠ‚æ–‡æœ¬ä¸­æå–å•è¯ï¼Œè¿›è¡Œè¯å½¢è¿˜åŸï¼Œè¿‡æ»¤å¤„ç†
"""

import os
import re
import json
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple
from dataclasses import dataclass
from collections import defaultdict
from .vocabulary_enricher import load_master_vocabulary

# éœ€è¦å®‰è£…: pip install spacy
# ä¸‹è½½æ¨¡å‹: python -m spacy download en_core_web_sm
try:
    import spacy
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {e}")
    print("è¯·å®‰è£…: pip install spacy")
    print("å¹¶ä¸‹è½½æ¨¡å‹: python -m spacy download en_core_web_sm")
    raise


@dataclass
class WordExtractionConfig:
    """å•è¯æå–é…ç½®"""
    
    # è¿‡æ»¤é…ç½®
    min_word_length: int = 2
    max_word_length: int = 20
    filter_numbers: bool = True
    filter_proper_nouns: bool = True
    filter_stop_words: bool = True
    filter_names: bool = True
    
    # è¾“å‡ºé…ç½®
    vocabulary_subdir: str = "vocabulary"
    chapters_subdir: str = "chapters"
    
    # SpaCyæ¨¡å‹
    spacy_model: str = "en_core_web_sm"


class WordExtractor:
    """å•è¯æå–å™¨ - ä»æ–‡æœ¬ä¸­æå–å’Œæ ‡å‡†åŒ–å•è¯"""
    
    def __init__(self, config: WordExtractionConfig):
        """
        åˆå§‹åŒ–å•è¯æå–å™¨
        
        Args:
            config: æå–é…ç½®
        """
        self.config = config
        
        # åŠ è½½SpaCyæ¨¡å‹
        try:
            self.nlp = spacy.load(self.config.spacy_model)
            print(f"âœ… SpaCyæ¨¡å‹åŠ è½½æˆåŠŸ: {self.config.spacy_model}")
        except OSError:
            raise RuntimeError(f"SpaCyæ¨¡å‹åŠ è½½å¤±è´¥: {self.config.spacy_model}ï¼Œè¯·è¿è¡Œ: python -m spacy download {self.config.spacy_model}")
        
        # è·å–spaCyåœç”¨è¯
        self.stop_words = self.nlp.Defaults.stop_words
        
        print(f"ğŸ“ å•è¯æå–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    
    def extract_subchapter_words(self, sentence_files: List[str], output_dir: str, master_vocab_path: str) -> Tuple[List[str], List[str]]:
        """
        ä»å¥å­æ–‡ä»¶ä¸­æå–å­ç« èŠ‚è¯æ±‡ï¼ˆä»¥å­ç« èŠ‚ä¸ºå•ä½ï¼‰
        
        Args:
            sentence_files: å¥å­æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            master_vocab_path: æ€»è¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„
            
        Returns:
            (å¤„ç†çš„å­ç« èŠ‚è¯æ±‡æ–‡ä»¶åˆ—è¡¨, æ‰€æœ‰æ–°è¯åˆ—è¡¨)
        """
        # åŠ è½½å·²æœ‰çš„æ€»è¯æ±‡è¡¨
        existing_vocab = load_master_vocabulary(master_vocab_path)
        print(f"ğŸ“ åŠ è½½ç°æœ‰è¯æ±‡è¡¨: {len(existing_vocab)} ä¸ªå•è¯")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        vocab_dir = os.path.join(output_dir, self.config.vocabulary_subdir)
        os.makedirs(vocab_dir, exist_ok=True)
        
        subchapter_vocab_files = []
        all_new_words = set()
        
        # æŒ‰å­ç« èŠ‚å¤„ç†å¥å­æ–‡ä»¶ï¼ˆæ¯ä¸ªå¥å­æ–‡ä»¶å¯¹åº”ä¸€ä¸ªå­ç« èŠ‚ï¼‰
        for sentence_file in sentence_files:
            # è·å–å­ç« èŠ‚åç§°
            filename = os.path.basename(sentence_file)
            subchapter_name = os.path.splitext(filename)[0]
            
            print(f"ğŸ“ å¤„ç†å­ç« èŠ‚: {subchapter_name}")
            
            # æå–å­ç« èŠ‚æ‰€æœ‰å•è¯
            all_words, filtered_words = self._extract_words_from_files([sentence_file])
            
            # æ”¶é›†æ‰€æœ‰æå–çš„å•è¯ï¼ˆä¸åŒºåˆ†æ–°æ—§ï¼‰
            all_new_words.update(all_words)
            
            # ç¬¬ä¸€é˜¶æ®µï¼šä¿å­˜åŸå§‹å•è¯åˆ—è¡¨ï¼ˆä¸æ ¼å¼åŒ–ï¼‰ï¼Œä¿æŒåŸæ–‡é¡ºåº
            unique_words = self._preserve_order_dedup(all_words)  # ä¿åºå»é‡
            
            # ä¿å­˜å­ç« èŠ‚è¯æ±‡æ–‡ä»¶ï¼ˆç¬¬ä¸€é˜¶æ®µï¼šåªå«å•è¯åˆ—è¡¨ï¼‰
            subchapter_vocab_data = {
                "subchapter_id": subchapter_name,
                "words": unique_words,  # ç¬¬ä¸€é˜¶æ®µï¼šçº¯å•è¯åˆ—è¡¨ï¼Œä¿æŒåŸæ–‡é¡ºåº
                "word_count": len(unique_words),
                "filtered_words": sorted(list(set(filtered_words)))
            }
            
            subchapter_vocab_file = os.path.join(vocab_dir, f"{subchapter_name}.json")
            self._save_json(subchapter_vocab_data, subchapter_vocab_file)
            subchapter_vocab_files.append(subchapter_vocab_file)
            
            print(f"  ğŸ“„ å·²ä¿å­˜å­ç« èŠ‚è¯æ±‡: {subchapter_vocab_file}")
            print(f"  ğŸ“ˆ è¯æ±‡ç»Ÿè®¡: æ€»è®¡{len(set(all_words))}ä¸ª")
        
        print(f"\nğŸ“ å­ç« èŠ‚è¯æ±‡æå–å®Œæˆï¼Œå…±æå– {len(all_new_words)} ä¸ªå•è¯")
        return subchapter_vocab_files, list(all_new_words)
    
    def _group_sentence_files_by_chapter(self, sentence_files: List[str]) -> Dict[str, List[str]]:
        """æŒ‰ç« èŠ‚åç§°åˆ†ç»„å¥å­æ–‡ä»¶"""
        chapter_groups = defaultdict(list)
        
        for sentence_file in sentence_files:
            # ä»æ–‡ä»¶åæå–ç« èŠ‚åç§° (ä¾‹: 01_Down_the_Rabbit-Hole(1).txt -> 01_Down_the_Rabbit-Hole)
            filename = os.path.basename(sentence_file)
            base_name = os.path.splitext(filename)[0]
            
            # ç§»é™¤æ‹¬å·éƒ¨åˆ†ï¼Œè·å–ç« èŠ‚åŸºç¡€åç§°
            chapter_name = re.sub(r'\([^)]*\)$', '', base_name)
            chapter_groups[chapter_name].append(sentence_file)
        
        return dict(chapter_groups)
    
    def _extract_words_from_files(self, sentence_files: List[str]) -> Tuple[List[str], List[str]]:
        """
        ä»æ–‡ä»¶åˆ—è¡¨ä¸­æå–å•è¯
        
        Returns:
            (æœ‰æ•ˆå•è¯åˆ—è¡¨, è¢«è¿‡æ»¤å•è¯åˆ—è¡¨)
        """
        all_text = ""
        
        # è¯»å–æ‰€æœ‰æ–‡ä»¶å†…å®¹
        for sentence_file in sentence_files:
            try:
                with open(sentence_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # è·³è¿‡ç¬¬ä¸€è¡Œæ ‡é¢˜
                    lines = content.strip().split('\n')
                    if len(lines) > 1:
                        all_text += " " + " ".join(lines[1:])
            except Exception as e:
                print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥: {sentence_file}, {e}")
                continue
        
        return self._extract_words_from_text(all_text)
    
    def _preserve_order_dedup(self, items: List[str]) -> List[str]:
        """
        ä¿åºå»é‡å‡½æ•° - ä¿æŒå…ƒç´ é¦–æ¬¡å‡ºç°çš„é¡ºåº
        
        Args:
            items: åŸå§‹åˆ—è¡¨
            
        Returns:
            å»é‡åä¿æŒåŸå§‹é¡ºåºçš„åˆ—è¡¨
        """
        seen = set()
        result = []
        for item in items:
            if item not in seen:
                seen.add(item)
                result.append(item)
        return result
    
    def _extract_words_from_text(self, text: str) -> Tuple[List[str], List[str]]:
        """
        ä»æ–‡æœ¬ä¸­æå–å•è¯å¹¶è¿›è¡Œè¯å½¢è¿˜åŸå’Œè¿‡æ»¤
        
        Returns:
            (æœ‰æ•ˆå•è¯åˆ—è¡¨, è¢«è¿‡æ»¤å•è¯åˆ—è¡¨)
        """
        if not text.strip():
            return [], []
        
        # ä½¿ç”¨SpaCyå¤„ç†æ–‡æœ¬
        doc = self.nlp(text)
        
        valid_words = []
        filtered_words = []
        
        for token in doc:
            original = token.text.lower().strip()
            
            # åŸºç¡€è¿‡æ»¤
            if not original or not original.isalpha():
                filtered_words.append(original)
                continue
            
            # é•¿åº¦è¿‡æ»¤
            if len(original) < self.config.min_word_length or len(original) > self.config.max_word_length:
                filtered_words.append(original)
                continue
            
            # åº”ç”¨å„ç§è¿‡æ»¤è§„åˆ™
            # filter_reason = self._get_filter_reason(token, original)
            # if filter_reason:
            #     filtered_words.append(original)
            #     continue
            
            # åªå¯¹å¤æ•°å’Œç¬¬ä¸‰äººç§°å•æ•°è¿›è¡Œå½’ä¸€åŒ–
            normalized_word = self._normalize_word_selective(token, original)
            valid_words.append(normalized_word)
        
        return valid_words, filtered_words
    
    def _get_filter_reason(self, token, word: str) -> Optional[str]:
        """
        æ£€æŸ¥å•è¯æ˜¯å¦åº”è¯¥è¢«è¿‡æ»¤
        
        Returns:
            è¿‡æ»¤åŸå› ï¼Œå¦‚æœä¸éœ€è¦è¿‡æ»¤è¿”å›None
        """
        # åœç”¨è¯è¿‡æ»¤ - è‡ªå®šä¹‰å­¦ä¹ ç›¸å…³åœç”¨è¯ç™½åå•
        learning_stopwords_whitelist = {
            'through', 'during', 'between', 'among', 'within', 'without', 
            'around', 'across', 'above', 'below', 'under', 'over',
            'before', 'after', 'until', 'since', 'while'
        }
        # if (self.config.filter_stop_words and 
        #     word in self.stop_words and 
        #     word not in learning_stopwords_whitelist):
        #     return "stop_word"
        
        # ä¸“æœ‰åè¯è¿‡æ»¤
        # if self.config.filter_proper_nouns and token.pos_ == "PROPN":
        #     return "proper_noun"
        
        # äººåè¿‡æ»¤ - ä½¿ç”¨spaCy NERè¯†åˆ«äººå
        # if self.config.filter_names and token.ent_type_ == "PERSON":
        #     return "person_name"
        
        # æ•°å­—è¿‡æ»¤
        if self.config.filter_numbers and token.like_num:
            return "number"
        
        return None
    
    def _normalize_word_selective(self, token, word: str) -> str:
        """
        é€‰æ‹©æ€§è¯å½¢å½’ä¸€åŒ–ï¼šåªå½’ä¸€åŒ–å¤æ•°å’Œç¬¬ä¸‰äººç§°å•æ•°å½¢å¼
        
        Args:
            token: SpaCy tokenå¯¹è±¡
            word: åŸå§‹å•è¯
            
        Returns:
            å½’ä¸€åŒ–åçš„å•è¯
        """
        lemma = token.lemma_.lower().strip()
        
        # å¦‚æœåŸè¯å’Œè¯æ ¹ç›¸åŒï¼Œæ— éœ€å½’ä¸€åŒ–
        if word == lemma:
            return word
        
        # æ£€æŸ¥è¯æ€§å’Œå˜åŒ–ç±»å‹
        pos = token.pos_
        tag = token.tag_
        
        # åè¯å¤æ•°å½’ä¸€åŒ– (NNS -> NN)
        if pos == "NOUN" and tag == "NNS":
            return lemma
        
        # åŠ¨è¯ç¬¬ä¸‰äººç§°å•æ•°å½’ä¸€åŒ– (VBZ -> VB)
        if pos == "VERB" and tag == "VBZ":
            return lemma
        
        # å…¶ä»–æƒ…å†µä¿æŒåŸå½¢ï¼ˆåŒ…æ‹¬åŠ¨è¯æ—¶æ€VBD/VBG/VBNå’Œå½¢å®¹è¯æ¯”è¾ƒçº§JJR/JJSï¼‰
        return word
    
    def _format_words_with_info(self, words: List[str], vocab_dict: Dict[str, Dict]) -> List[str]:
        """
        å°†å•è¯åˆ—è¡¨æ ¼å¼åŒ–ä¸ºåŒ…å«è¯æ±‡ä¿¡æ¯çš„é€—å·åˆ†éš”æ ¼å¼
        
        Args:
            words: å•è¯åˆ—è¡¨
            vocab_dict: æ€»è¯æ±‡è¡¨å­—å…¸
            
        Returns:
            æ ¼å¼åŒ–åçš„å•è¯ä¿¡æ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º: word,tags,frq,collins,oxford
        """
        formatted_words = []
        unique_words = self._preserve_order_dedup(words)  # ä¿åºå»é‡
        
        for word in unique_words:
            word_info = vocab_dict.get(word, {})
            
            # æå–è¯æ±‡ä¿¡æ¯ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
            tags = ""
            if 'tags' in word_info:
                if isinstance(word_info['tags'], list):
                    tags = " ".join(word_info['tags'])
                elif isinstance(word_info['tags'], str):
                    tags = word_info['tags']
            
            frq = word_info.get('frq', 0)
            collins = word_info.get('collins', 0)
            oxford = word_info.get('oxford', 0)
            
            # æ ¼å¼åŒ–ä¸ºé€—å·åˆ†éš”å­—ç¬¦ä¸²: word,tags,frq,collins,oxford
            formatted_word = f"{word},{tags},{frq},{collins},{oxford}"
            formatted_words.append(formatted_word)
        
        # ä¿æŒåŸæ–‡ä¸­å‡ºç°çš„é¡ºåºï¼Œä¸è¿›è¡Œæ’åº
        return formatted_words
    
    def update_vocabulary_info(self, output_dir: str, master_vocab_path: str) -> bool:
        """
        ç¬¬äºŒé˜¶æ®µï¼šæ›´æ–°ç« èŠ‚è¯æ±‡æ–‡ä»¶ï¼Œå°†å•è¯åˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”ä¿¡æ¯æ ¼å¼
        
        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼ˆåŒ…å«vocabularyå­ç›®å½•ï¼‰
            master_vocab_path: æ€»è¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        try:
            # åŠ è½½æ€»è¯æ±‡è¡¨
            existing_vocab = load_master_vocabulary(master_vocab_path)
            if not existing_vocab:
                print("âš ï¸ æ€»è¯æ±‡è¡¨ä¸ºç©ºï¼Œæ— æ³•æ›´æ–°è¯æ±‡ä¿¡æ¯")
                return False
            
            # æŸ¥æ‰¾æ‰€æœ‰ç« èŠ‚è¯æ±‡æ–‡ä»¶
            vocab_dir = os.path.join(output_dir, self.config.vocabulary_subdir)
            if not os.path.exists(vocab_dir):
                print("âš ï¸ ç« èŠ‚è¯æ±‡ç›®å½•ä¸å­˜åœ¨")
                return False
            
            vocab_files = []
            for file in os.listdir(vocab_dir):
                if file.endswith('.json'):
                    vocab_files.append(os.path.join(vocab_dir, file))
            
            if not vocab_files:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç« èŠ‚è¯æ±‡æ–‡ä»¶")
                return False
            
            updated_count = 0
            
            print(f"ğŸ“ å¼€å§‹æ›´æ–° {len(vocab_files)} ä¸ªç« èŠ‚è¯æ±‡æ–‡ä»¶...")
            
            # å¤„ç†æ¯ä¸ªç« èŠ‚è¯æ±‡æ–‡ä»¶
            for vocab_file in vocab_files:
                if self._update_single_vocab_file(vocab_file, existing_vocab):
                    updated_count += 1
                    filename = os.path.basename(vocab_file)
                    print(f"  âœ… å·²æ›´æ–°: {filename}")
            
            print(f"\nğŸ“ è¯æ±‡ä¿¡æ¯æ›´æ–°å®Œæˆ: æˆåŠŸæ›´æ–° {updated_count}/{len(vocab_files)} ä¸ªæ–‡ä»¶")
            return updated_count > 0
            
        except Exception as e:
            print(f"âŒ æ›´æ–°è¯æ±‡ä¿¡æ¯å¤±è´¥: {e}")
            return False
    
    def _update_single_vocab_file(self, vocab_file: str, vocab_dict: Dict[str, Dict]) -> bool:
        """
        æ›´æ–°å•ä¸ªç« èŠ‚è¯æ±‡æ–‡ä»¶
        
        Args:
            vocab_file: ç« èŠ‚è¯æ±‡æ–‡ä»¶è·¯å¾„
            vocab_dict: æ€»è¯æ±‡è¡¨å­—å…¸
            
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        try:
            # è¯»å–ç°æœ‰æ–‡ä»¶
            with open(vocab_file, 'r', encoding='utf-8') as f:
                vocab_data = json.load(f)
            
            words = vocab_data.get('words', [])
            if not words:
                return False
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯æ ¼å¼åŒ–æ ¼å¼
            if isinstance(words[0], str) and ',' in words[0] and len(words[0].split(',')) == 5:
                print(f"  â­• å·²æ˜¯æ ¼å¼åŒ–æ ¼å¼ï¼Œè·³è¿‡: {os.path.basename(vocab_file)}")
                return False
            
            # æ ¼å¼åŒ–å•è¯ä¿¡æ¯
            formatted_words = self._format_words_with_info(words, vocab_dict)
            
            # æ›´æ–°æ•°æ®
            vocab_data['words'] = formatted_words
            
            # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
            self._save_json(vocab_data, vocab_file)
            return True
            
        except Exception as e:
            print(f"âŒ æ›´æ–°æ–‡ä»¶å¤±è´¥ {vocab_file}: {e}")
            return False
    
    def _save_json(self, data: dict, file_path: str):
        """ä¿å­˜JSONæ•°æ®åˆ°æ–‡ä»¶"""
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
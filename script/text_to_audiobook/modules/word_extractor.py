#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å•è¯æå–æ¨¡å—
ä»ç« èŠ‚æ–‡æœ¬ä¸­æå–å•è¯ï¼Œè¿›è¡Œè¯å½¢è¿˜åŸï¼Œè¿‡æ»¤å¤„ç†
"""

import os
import re
import json
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple
from dataclasses import dataclass
from collections import defaultdict

# éœ€è¦å®‰è£…: pip install spacy
# ä¸‹è½½æ¨¡å‹: python -m spacy download en_core_web_sm
try:
    import spacy
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {e}")
    print("è¯·å®‰è£…: pip install spacy")
    print("å¹¶ä¸‹è½½æ¨¡å‹: python -m spacy download en_core_web_sm")
    raise


@dataclass
class WordExtractionConfig:
    """å•è¯æå–é…ç½®"""
    
    # è¿‡æ»¤é…ç½®
    min_word_length: int = 2
    max_word_length: int = 20
    filter_numbers: bool = True
    filter_proper_nouns: bool = True
    filter_stop_words: bool = True
    filter_names: bool = True
    
    # è¾“å‡ºé…ç½®
    vocabulary_subdir: str = "vocabulary"
    chapters_subdir: str = "chapters"
    
    # SpaCyæ¨¡å‹
    spacy_model: str = "en_core_web_sm"


class WordExtractor:
    """å•è¯æå–å™¨ - ä»æ–‡æœ¬ä¸­æå–å’Œæ ‡å‡†åŒ–å•è¯"""
    
    def __init__(self, config: WordExtractionConfig):
        """
        åˆå§‹åŒ–å•è¯æå–å™¨
        
        Args:
            config: æå–é…ç½®
        """
        self.config = config
        
        # åŠ è½½SpaCyæ¨¡å‹
        try:
            self.nlp = spacy.load(self.config.spacy_model)
            print(f"âœ… SpaCyæ¨¡å‹åŠ è½½æˆåŠŸ: {self.config.spacy_model}")
        except OSError:
            raise RuntimeError(f"SpaCyæ¨¡å‹åŠ è½½å¤±è´¥: {self.config.spacy_model}ï¼Œè¯·è¿è¡Œ: python -m spacy download {self.config.spacy_model}")
        
        # è·å–spaCyåœç”¨è¯
        self.stop_words = self.nlp.Defaults.stop_words
        
        print(f"ğŸ“ å•è¯æå–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    
    def extract_subchapter_words(self, sentence_files: List[str], output_dir: str, master_vocab_path: str) -> Tuple[List[str], List[str]]:
        """
        ä»å¥å­æ–‡ä»¶ä¸­æå–å­ç« èŠ‚è¯æ±‡ï¼ˆä»¥å­ç« èŠ‚ä¸ºå•ä½ï¼‰
        
        Args:
            sentence_files: å¥å­æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            master_vocab_path: æ€»è¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„
            
        Returns:
            (å¤„ç†çš„å­ç« èŠ‚è¯æ±‡æ–‡ä»¶åˆ—è¡¨, æ‰€æœ‰æ–°è¯åˆ—è¡¨)
        """
        # åŠ è½½å·²æœ‰çš„æ€»è¯æ±‡è¡¨
        existing_vocab = self._load_master_vocabulary(master_vocab_path)
        print(f"ğŸ“ åŠ è½½ç°æœ‰è¯æ±‡è¡¨: {len(existing_vocab)} ä¸ªå•è¯")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        vocab_dir = os.path.join(output_dir, self.config.vocabulary_subdir)
        subchapters_dir = os.path.join(vocab_dir, "subchapters")
        os.makedirs(subchapters_dir, exist_ok=True)
        
        subchapter_vocab_files = []
        all_new_words = set()
        
        # æŒ‰å­ç« èŠ‚å¤„ç†å¥å­æ–‡ä»¶ï¼ˆæ¯ä¸ªå¥å­æ–‡ä»¶å¯¹åº”ä¸€ä¸ªå­ç« èŠ‚ï¼‰
        for sentence_file in sentence_files:
            # è·å–å­ç« èŠ‚åç§°
            filename = os.path.basename(sentence_file)
            subchapter_name = os.path.splitext(filename)[0]
            
            print(f"ğŸ“ å¤„ç†å­ç« èŠ‚: {subchapter_name}")
            
            # æå–å­ç« èŠ‚æ‰€æœ‰å•è¯
            all_words, filtered_words = self._extract_words_from_files([sentence_file])
            
            # æ”¶é›†æ‰€æœ‰æå–çš„å•è¯ï¼ˆä¸åŒºåˆ†æ–°æ—§ï¼‰
            all_new_words.update(all_words)
            
            # ä¿å­˜å­ç« èŠ‚è¯æ±‡æ–‡ä»¶ï¼ˆåŒ…å«æ‰€æœ‰æå–çš„å•è¯ï¼‰
            subchapter_vocab_data = {
                "subchapter_id": subchapter_name,
                "words": list(set(all_words)),  # æå–æ‰€æœ‰å•è¯
                "word_count": len(set(all_words)),
                "filtered_words": sorted(list(set(filtered_words)))
            }
            
            subchapter_vocab_file = os.path.join(subchapters_dir, f"{subchapter_name}.json")
            self._save_json(subchapter_vocab_data, subchapter_vocab_file)
            subchapter_vocab_files.append(subchapter_vocab_file)
            
            print(f"  ğŸ“„ å·²ä¿å­˜å­ç« èŠ‚è¯æ±‡: {subchapter_vocab_file}")
            print(f"  ğŸ“ˆ è¯æ±‡ç»Ÿè®¡: æ€»è®¡{len(set(all_words))}ä¸ª")
        
        print(f"\nğŸ“ å­ç« èŠ‚è¯æ±‡æå–å®Œæˆï¼Œå…±æå– {len(all_new_words)} ä¸ªå•è¯")
        return subchapter_vocab_files, list(all_new_words)
    
    def _group_sentence_files_by_chapter(self, sentence_files: List[str]) -> Dict[str, List[str]]:
        """æŒ‰ç« èŠ‚åç§°åˆ†ç»„å¥å­æ–‡ä»¶"""
        chapter_groups = defaultdict(list)
        
        for sentence_file in sentence_files:
            # ä»æ–‡ä»¶åæå–ç« èŠ‚åç§° (ä¾‹: 01_Down_the_Rabbit-Hole(1).txt -> 01_Down_the_Rabbit-Hole)
            filename = os.path.basename(sentence_file)
            base_name = os.path.splitext(filename)[0]
            
            # ç§»é™¤æ‹¬å·éƒ¨åˆ†ï¼Œè·å–ç« èŠ‚åŸºç¡€åç§°
            chapter_name = re.sub(r'\([^)]*\)$', '', base_name)
            chapter_groups[chapter_name].append(sentence_file)
        
        return dict(chapter_groups)
    
    def _extract_words_from_files(self, sentence_files: List[str]) -> Tuple[List[str], List[str]]:
        """
        ä»æ–‡ä»¶åˆ—è¡¨ä¸­æå–å•è¯
        
        Returns:
            (æœ‰æ•ˆå•è¯åˆ—è¡¨, è¢«è¿‡æ»¤å•è¯åˆ—è¡¨)
        """
        all_text = ""
        
        # è¯»å–æ‰€æœ‰æ–‡ä»¶å†…å®¹
        for sentence_file in sentence_files:
            try:
                with open(sentence_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # è·³è¿‡ç¬¬ä¸€è¡Œæ ‡é¢˜
                    lines = content.strip().split('\n')
                    if len(lines) > 1:
                        all_text += " " + " ".join(lines[1:])
            except Exception as e:
                print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥: {sentence_file}, {e}")
                continue
        
        return self._extract_words_from_text(all_text)
    
    def _extract_words_from_text(self, text: str) -> Tuple[List[str], List[str]]:
        """
        ä»æ–‡æœ¬ä¸­æå–å•è¯å¹¶è¿›è¡Œè¯å½¢è¿˜åŸå’Œè¿‡æ»¤
        
        Returns:
            (æœ‰æ•ˆå•è¯åˆ—è¡¨, è¢«è¿‡æ»¤å•è¯åˆ—è¡¨)
        """
        if not text.strip():
            return [], []
        
        # ä½¿ç”¨SpaCyå¤„ç†æ–‡æœ¬
        doc = self.nlp(text)
        
        valid_words = []
        filtered_words = []
        
        for token in doc:
            original = token.text.lower().strip()
            
            # åŸºç¡€è¿‡æ»¤
            if not original or not original.isalpha():
                filtered_words.append(original)
                continue
            
            # é•¿åº¦è¿‡æ»¤
            if len(original) < self.config.min_word_length or len(original) > self.config.max_word_length:
                filtered_words.append(original)
                continue
            
            # åº”ç”¨å„ç§è¿‡æ»¤è§„åˆ™
            # filter_reason = self._get_filter_reason(token, original)
            # if filter_reason:
            #     filtered_words.append(original)
            #     continue
            
            # åªå¯¹å¤æ•°å’Œç¬¬ä¸‰äººç§°å•æ•°è¿›è¡Œå½’ä¸€åŒ–
            normalized_word = self._normalize_word_selective(token, original)
            valid_words.append(normalized_word)
        
        return valid_words, filtered_words
    
    def _get_filter_reason(self, token, word: str) -> Optional[str]:
        """
        æ£€æŸ¥å•è¯æ˜¯å¦åº”è¯¥è¢«è¿‡æ»¤
        
        Returns:
            è¿‡æ»¤åŸå› ï¼Œå¦‚æœä¸éœ€è¦è¿‡æ»¤è¿”å›None
        """
        # åœç”¨è¯è¿‡æ»¤ - è‡ªå®šä¹‰å­¦ä¹ ç›¸å…³åœç”¨è¯ç™½åå•
        learning_stopwords_whitelist = {
            'through', 'during', 'between', 'among', 'within', 'without', 
            'around', 'across', 'above', 'below', 'under', 'over',
            'before', 'after', 'until', 'since', 'while'
        }
        # if (self.config.filter_stop_words and 
        #     word in self.stop_words and 
        #     word not in learning_stopwords_whitelist):
        #     return "stop_word"
        
        # ä¸“æœ‰åè¯è¿‡æ»¤
        # if self.config.filter_proper_nouns and token.pos_ == "PROPN":
        #     return "proper_noun"
        
        # äººåè¿‡æ»¤ - ä½¿ç”¨spaCy NERè¯†åˆ«äººå
        # if self.config.filter_names and token.ent_type_ == "PERSON":
        #     return "person_name"
        
        # æ•°å­—è¿‡æ»¤
        if self.config.filter_numbers and token.like_num:
            return "number"
        
        return None
    
    def _normalize_word_selective(self, token, word: str) -> str:
        """
        é€‰æ‹©æ€§è¯å½¢å½’ä¸€åŒ–ï¼šåªå½’ä¸€åŒ–å¤æ•°å’Œç¬¬ä¸‰äººç§°å•æ•°å½¢å¼
        
        Args:
            token: SpaCy tokenå¯¹è±¡
            word: åŸå§‹å•è¯
            
        Returns:
            å½’ä¸€åŒ–åçš„å•è¯
        """
        lemma = token.lemma_.lower().strip()
        
        # å¦‚æœåŸè¯å’Œè¯æ ¹ç›¸åŒï¼Œæ— éœ€å½’ä¸€åŒ–
        if word == lemma:
            return word
        
        # æ£€æŸ¥è¯æ€§å’Œå˜åŒ–ç±»å‹
        pos = token.pos_
        tag = token.tag_
        
        # åè¯å¤æ•°å½’ä¸€åŒ– (NNS -> NN)
        if pos == "NOUN" and tag == "NNS":
            return lemma
        
        # åŠ¨è¯ç¬¬ä¸‰äººç§°å•æ•°å½’ä¸€åŒ– (VBZ -> VB)
        if pos == "VERB" and tag == "VBZ":
            return lemma
        
        # å…¶ä»–æƒ…å†µä¿æŒåŸå½¢ï¼ˆåŒ…æ‹¬åŠ¨è¯æ—¶æ€VBD/VBG/VBNå’Œå½¢å®¹è¯æ¯”è¾ƒçº§JJR/JJSï¼‰
        return word
    
    def _load_master_vocabulary(self, master_vocab_path: str) -> Dict[str, Dict]:
        """åŠ è½½æ€»è¯æ±‡è¡¨"""
        from .vocabulary_enricher import load_master_vocabulary
        return load_master_vocabulary(master_vocab_path)
    
    def _save_json(self, data: dict, file_path: str):
        """ä¿å­˜JSONæ•°æ®åˆ°æ–‡ä»¶"""
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¥å­æ‹†åˆ†ä¸ç¿»è¯‘æ¨¡å—
ä½¿ç”¨AIåŒæ—¶è¿›è¡Œå¥å­æ‹†åˆ†å’Œç¿»è¯‘ï¼Œç¡®ä¿è¯­ä¹‰ä¸€è‡´æ€§
"""

import os
import re
import json
import time
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from infra import AIClient, FileManager
from infra.config_loader import AppConfig


class SentenceProcessor:
    """å¥å­æ‹†åˆ†ä¸ç¿»è¯‘å¤„ç†å™¨"""
    
    def __init__(self, config: AppConfig):
        """
        åˆå§‹åŒ–å¥å­å¤„ç†å™¨
        
        Args:
            config: åº”ç”¨é…ç½®
        """
        self.config = config
        self.ai_client = AIClient(config.api)
        self.file_manager = FileManager()
    
    def split_sub_chapters_to_sentences(self, input_files: List[str], output_dir: str) -> List[str]:
        """
        æ‹†åˆ†æ–‡ä»¶åˆ—è¡¨ä¸ºå¥å­çº§æ–‡ä»¶ï¼ˆJSONLæ ¼å¼ï¼‰
        
        Args:
            input_files: è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            ç”Ÿæˆçš„å¥å­æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        # åˆ›å»ºè¾“å‡ºç›®å½•
        sentences_dir = os.path.join(output_dir, "sentences")
        os.makedirs(sentences_dir, exist_ok=True)
        
        output_files = []
        
        print(f"ğŸ” å¼€å§‹AIæ‹†åˆ†ç¿»è¯‘ {len(input_files)} ä¸ªå­ç« èŠ‚æ–‡ä»¶...")
        
        for i, input_file in enumerate(input_files, 1):
            try:
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
                filename = os.path.basename(input_file)
                base_name = os.path.splitext(filename)[0]
                output_file = os.path.join(sentences_dir, f"{base_name}.jsonl")
                
                print(f"ğŸ“„ [{i}/{len(input_files)}] å¤„ç†æ–‡ä»¶: {filename}")
                
                # å¤„ç†å•ä¸ªæ–‡ä»¶
                success = self._process_file(input_file, output_file)
                if success:
                    output_files.append(output_file)
                    print(f"    âœ… å·²å®ŒæˆAIæ‹†åˆ†ç¿»è¯‘: {filename}")
                else:
                    print(f"    âŒ å¤„ç†å¤±è´¥: {filename}")
                    
            except Exception as e:
                print(f"    âŒ æ‹†åˆ†ç¿»è¯‘å¤±è´¥: {e}")
                continue
        
        print(f"\nğŸ“ å¥å­æ‹†åˆ†ç¿»è¯‘å®Œæˆï¼Œè¾“å‡ºåˆ°: {sentences_dir}")
        print(f"ğŸ“Š æˆåŠŸå¤„ç†: {len(output_files)}/{len(input_files)} ä¸ªæ–‡ä»¶")
        
        return output_files
    
    def _process_file(self, input_file: str, output_file: str) -> bool:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶çš„å¥å­æ‹†åˆ†å’Œç¿»è¯‘ï¼ˆå¢é‡å¤„ç†ï¼‰
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦å¤„ç†æˆåŠŸ
        """
        try:
            # è¯»å–è¾“å…¥æ–‡ä»¶
            with open(input_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–æ ‡é¢˜å’Œæ­£æ–‡
            title, body = self._extract_title_and_body(content)
            
            if not body.strip():
                print(f"    âš ï¸ æ–‡ä»¶å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                return False
            
            # æŒ‰æ®µè½åˆ†å‰²
            paragraphs = re.split(r'\n\n', body)
            paragraphs = [p.strip() for p in paragraphs if p.strip()]
            
            print(f"    ğŸ” å¤„ç† {len(paragraphs)} ä¸ªæ®µè½")
            
            # åŠ è½½å·²æœ‰å¤„ç†ç»“æœ
            existing_results = self._load_existing_paragraph_results(output_file)
            processed_indices = {result['paragraph_index'] for result in existing_results if result.get('success', False)}
            
            # è¯†åˆ«æœªå¤„ç†çš„æ®µè½
            unprocessed_paragraphs = []
            for para_idx, paragraph in enumerate(paragraphs, 1):
                if para_idx not in processed_indices:
                    unprocessed_paragraphs.append((para_idx, paragraph))
            
            if not unprocessed_paragraphs:
                print(f"    âœ… æ‰€æœ‰æ®µè½å·²å¤„ç†å®Œæ¯•ï¼Œè·³è¿‡")
                return True
            
            print(f"    ğŸ”„ éœ€è¦å¤„ç† {len(unprocessed_paragraphs)}/{len(paragraphs)} ä¸ªæ®µè½")
            
            # å¹¶å‘å¤„ç†æœªå®Œæˆçš„æ®µè½
            new_results = []
            max_workers = min(len(unprocessed_paragraphs), self.config.api.max_concurrent_workers)
            
            print(f"    ğŸš€ å¼€å§‹å¹¶å‘å¤„ç†ï¼Œä½¿ç”¨ {max_workers} ä¸ªworker")
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰æ®µè½ä»»åŠ¡
                future_to_paragraph = {}
                for para_idx, paragraph in unprocessed_paragraphs:
                    future = executor.submit(self._process_single_paragraph, para_idx, paragraph, len(paragraphs))
                    future_to_paragraph[future] = (para_idx, paragraph)
                
                # æ”¶é›†å¹¶å‘å¤„ç†ç»“æœ
                completed_count = 0
                for future in as_completed(future_to_paragraph):
                    para_idx, paragraph = future_to_paragraph[future]
                    try:
                        paragraph_result = future.result()
                        if paragraph_result:
                            new_results.append(paragraph_result)
                        else:
                            # åˆ›å»ºå¤±è´¥ç»“æœ
                            paragraph_result = {
                                "paragraph_index": para_idx,
                                "original_text": paragraph,
                                "segments": [],
                                "success": False
                            }
                            new_results.append(paragraph_result)
                        
                        completed_count += 1
                        if completed_count % 3 == 0 or completed_count == len(unprocessed_paragraphs):
                            print(f"    ğŸ“Š å¹¶å‘è¿›åº¦: {completed_count}/{len(unprocessed_paragraphs)} ä¸ªæ®µè½")
                            
                    except Exception as e:
                        print(f"    âŒ æ®µè½ {para_idx} å¹¶å‘å¤„ç†å¼‚å¸¸: {e}")
                        # è®°å½•å¤±è´¥çš„æ®µè½
                        paragraph_result = {
                            "paragraph_index": para_idx,
                            "original_text": paragraph,
                            "segments": [],
                            "success": False
                        }
                        new_results.append(paragraph_result)
            
            # ä¿å­˜ç»“æœï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
            if new_results:
                self._save_paragraph_results(output_file, new_results, existing_results)
                
                success_count = sum(1 for result in new_results if result['success'])
                print(f"    ğŸ’¾ å·²ä¿å­˜ {len(new_results)} ä¸ªæ®µè½ç»“æœï¼ŒæˆåŠŸ {success_count} ä¸ª")
                return success_count > 0
            else:
                print(f"    âš ï¸ æœªç”Ÿæˆæ–°çš„æ®µè½ç»“æœ")
                return False
                
        except Exception as e:
            print(f"    âŒ æ–‡ä»¶å¤„ç†å¼‚å¸¸: {e}")
            return False
    
    def _extract_title_and_body(self, content: str) -> tuple[str, str]:
        """
        æå–æ ‡é¢˜å’Œæ­£æ–‡å†…å®¹
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            
        Returns:
            (æ ‡é¢˜, æ­£æ–‡å†…å®¹)
        """
        lines = content.split('\n')
        
        # ç¬¬ä¸€è¡Œæ˜¯æ ‡é¢˜
        title = lines[0].strip() if lines else "Unknown Title"
        
        # å…¶ä½™æ˜¯æ­£æ–‡ï¼ˆå»é™¤å¼€å¤´çš„ç©ºè¡Œï¼‰
        body_lines = lines[1:]
        while body_lines and not body_lines[0].strip():
            body_lines.pop(0)
        
        body = '\n'.join(body_lines)
        return title, body
    
    def _process_single_paragraph(self, para_idx: int, paragraph: str, total_paragraphs: int) -> Optional[Dict]:
        """
        å¤„ç†å•ä¸ªæ®µè½ï¼ˆç”¨äºå¹¶å‘ï¼‰
        
        Args:
            para_idx: æ®µè½ç´¢å¼•
            paragraph: æ®µè½å†…å®¹
            total_paragraphs: æ€»æ®µè½æ•°
            
        Returns:
            æ®µè½å¤„ç†ç»“æœï¼Œå¤±è´¥è¿”å›None
        """
        try:
            print(f"      ğŸ“ æ®µè½ {para_idx}/{total_paragraphs}: {len(paragraph)} å­—ç¬¦")
            
            # ä½¿ç”¨AIè¿›è¡Œæ‹†åˆ†å’Œç¿»è¯‘
            segments = self._split_and_translate_with_ai(paragraph)
            
            # æ„å»ºæ®µè½ç»“æœ
            paragraph_result = {
                "paragraph_index": para_idx,
                "original_text": paragraph,
                "segments": segments if segments else [],
                "success": bool(segments)
            }
            
            if segments:
                print(f"      âœ… æ®µè½ {para_idx} ç”Ÿæˆ {len(segments)} ä¸ªå¥å­ç‰‡æ®µ")
            else:
                print(f"      âŒ æ®µè½ {para_idx} å¤„ç†å¤±è´¥")
            
            return paragraph_result
            
        except Exception as e:
            print(f"      âŒ æ®µè½ {para_idx} å¤„ç†å¼‚å¸¸: {e}")
            return None
    
    def _split_and_translate_with_ai(self, paragraph: str) -> List[Dict[str, str]]:
        """
        ä½¿ç”¨AIåŒæ—¶è¿›è¡Œå¥å­æ‹†åˆ†å’Œç¿»è¯‘
        
        Args:
            paragraph: è¾“å…¥æ®µè½
            
        Returns:
            æ‹†åˆ†ç¿»è¯‘ç»“æœåˆ—è¡¨ [{"original": "è‹±æ–‡", "translation": "ä¸­æ–‡"}, ...]
        """
        try:
            system_prompt = """ä½œä¸ºè‹±è¯­å¥å­æ‹†åˆ†ä¸ç¿»è¯‘ä¸“å®¶ï¼Œè¯·å°†è‹±æ–‡æ®µè½æŒ‰è¯­ä¹‰å®Œæ•´æ€§æ‹†åˆ†å¹¶ç¿»è¯‘æˆä¸­æ–‡ã€‚

## è¾“å‡ºæ ¼å¼
æ¯è¡Œä¸€ä¸ªå¥å­å¯¹ï¼Œæ ¼å¼ï¼šè‹±æ–‡ || ä¸­æ–‡
ä¸è¦ä»»ä½•å…¶ä»–è¯´æ˜æˆ–æ ¼å¼æ ‡è®°ã€‚

## æ‹†åˆ†è§„åˆ™
1. **é•¿åº¦æ§åˆ¶**ï¼š
   - çŸ­å¥ï¼ˆâ‰¤15ä¸ªå•è¯ï¼‰ï¼šä¿æŒå®Œæ•´ï¼Œä¸æ‹†åˆ†
   - é•¿å¥ï¼ˆ>15ä¸ªå•è¯ï¼‰ï¼šæ‹†åˆ†ä¸º8-15ä¸ªå•è¯çš„ç‰‡æ®µ
   - ä¸¥ç¦è¶…è¿‡15ä¸ªå•è¯çš„ç‰‡æ®µ

2. **æ‹†åˆ†åŸåˆ™**ï¼š
   - åœ¨è‡ªç„¶åœé¡¿å¤„æ‹†åˆ†ï¼ˆä»å¥è¾¹ç•Œã€è¿è¯ã€æ ‡ç‚¹ï¼‰
   - ä¿æŒè¯­ä¹‰å®Œæ•´æ€§å’Œé€»è¾‘è¿è´¯æ€§
   - ä¸ç ´åä¹ è¯­å’Œå›ºå®šæ­é…
   - ä¸¥æ ¼ä¿ç•™åŸæ–‡æ‰€æœ‰å†…å®¹ï¼ˆæ ‡ç‚¹ã€å¤§å°å†™ã€æ–œä½“ç­‰ï¼‰

## ç¿»è¯‘è¦æ±‚
- **ä¿¡è¾¾é›…**ï¼šå‡†ç¡®ä¼ æ„ï¼Œä¸­æ–‡æµç•…ï¼Œä¿æŒæ–‡å­¦æ€§
- æ°å½“å¤„ç†æ–œä½“å¼ºè°ƒå’Œå¯¹è¯å¼•è¯­
- è‡ªç„¶å¤„ç†æ‹¬å·è¡¥å……è¯´æ˜

## ç¤ºä¾‹
è¾“å…¥ï¼šAlice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do.

è¾“å‡ºï¼š
Alice was beginning to get very tired of sitting by her sister on the bank, || çˆ±ä¸½ä¸å¼€å§‹å¯¹ååœ¨å§å§èº«è¾¹çš„æ²³å²¸ä¸Šæ„Ÿåˆ°éå¸¸ç–²å€¦ï¼Œ
and of having nothing to do. || ä¹ŸåŒå€¦äº†æ— æ‰€äº‹äº‹ã€‚"""
            
            user_prompt = f"è¯·å¯¹ä»¥ä¸‹è‹±æ–‡æ®µè½è¿›è¡Œæ‹†åˆ†å’Œç¿»è¯‘ï¼š\n\n{paragraph}"
            
            # è°ƒç”¨AI API
            response = self.ai_client.chat_completion(
                user_prompt, 
                system_prompt,
                temperature=0.8, 
                max_tokens=4000
            )
            
            if not response or not response.strip():
                print(f"      âš ï¸ AIè¿”å›ç©ºç»“æœ")
                return []
            
            # è§£æå¥å­å¯¹å“åº”
            try:
                # ä½¿ç”¨æ–°çš„å¥å­å¯¹è§£ææ–¹æ³•
                sentences = self._parse_sentence_pairs(response)
                
                if sentences:
                    return sentences
                else:
                    print(f"      âš ï¸ æœªè§£æåˆ°æœ‰æ•ˆçš„å¥å­å¯¹")
                    return []
                    
            except Exception as e:
                print(f"      âš ï¸ å¥å­å¯¹è§£æå¤±è´¥: {e}")
                return []
                
        except Exception as e:
            print(f"      âŒ AIæ‹†åˆ†ç¿»è¯‘å¼‚å¸¸: {e}")
            return []
    
    def _parse_sentence_pairs(self, response: str) -> List[Dict[str, str]]:
        """
        ä»AIå“åº”ä¸­è§£æåŒç«–çº¿åˆ†éš”çš„å¥å­å¯¹
        
        Args:
            response: AIè¿”å›çš„åŸå§‹å“åº”
            
        Returns:
            è§£æçš„å¥å­å¯¹åˆ—è¡¨ [{"original": "è‹±æ–‡", "translation": "ä¸­æ–‡"}, ...]
        """
        if not response:
            return []
        
        sentences = []
        try:
            # æŒ‰è¡Œåˆ†å‰²å“åº”
            lines = response.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                # è·³è¿‡ç©ºè¡Œ
                if not line:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«åŒç«–çº¿åˆ†éš”ç¬¦
                if '||' in line:
                    # åªåˆ†å‰²ç¬¬ä¸€ä¸ª||ï¼Œé˜²æ­¢ç¿»è¯‘æ–‡æœ¬ä¸­çš„||è¢«è¯¯åˆ†å‰²
                    parts = line.split('||', 1)
                    if len(parts) == 2:
                        original = parts[0].strip()
                        translation = parts[1].strip()
                        
                        # éªŒè¯å†…å®¹ä¸ä¸ºç©º
                        if original and translation:
                            sentences.append({
                                'original': original,
                                'translation': translation
                            })
            
            return sentences
            
        except Exception as e:
            print(f"      âš ï¸ è§£æå¥å­å¯¹å¤±è´¥: {e}")
            return []
    
    def _load_existing_paragraph_results(self, output_file: str) -> List[Dict]:
        """
        åŠ è½½å·²æœ‰çš„æ®µè½å¤„ç†ç»“æœ
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            å·²æœ‰çš„æ®µè½ç»“æœåˆ—è¡¨
        """
        if not os.path.exists(output_file):
            return []
        
        results = []
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    
                    try:
                        result = json.loads(line)
                        if isinstance(result, dict) and 'paragraph_index' in result:
                            results.append(result)
                    except json.JSONDecodeError as e:
                        print(f"      âš ï¸ è§£æJSONè¡Œ {line_num} å¤±è´¥: {e}")
                        continue
                        
        except Exception as e:
            print(f"      âš ï¸ è¯»å–å·²æœ‰ç»“æœæ–‡ä»¶å¤±è´¥ {output_file}: {e}")
            
        return results
    
    def _save_paragraph_results(self, output_file: str, new_results: List[Dict], existing_results: List[Dict]):
        """
        ä¿å­˜æ®µè½å¤„ç†ç»“æœï¼ˆåˆå¹¶æ–°æ—§ç»“æœï¼‰
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            new_results: æ–°çš„å¤„ç†ç»“æœ
            existing_results: å·²æœ‰çš„å¤„ç†ç»“æœ
        """
        try:
            # åˆå¹¶ç»“æœï¼šç”¨æ–°ç»“æœè¦†ç›–ç›¸åŒæ®µè½ç´¢å¼•çš„æ—§ç»“æœ
            all_results = {}
            
            # å…ˆæ·»åŠ å·²æœ‰ç»“æœ
            for result in existing_results:
                para_idx = result.get('paragraph_index')
                if para_idx is not None:
                    all_results[para_idx] = result
            
            # å†æ·»åŠ æ–°ç»“æœï¼ˆè¦†ç›–ç›¸åŒç´¢å¼•ï¼‰
            for result in new_results:
                para_idx = result.get('paragraph_index')
                if para_idx is not None:
                    all_results[para_idx] = result
            
            # æŒ‰æ®µè½ç´¢å¼•æ’åºå¹¶å†™å…¥æ–‡ä»¶
            sorted_results = sorted(all_results.values(), key=lambda x: x.get('paragraph_index', 0))
            
            with open(output_file, 'w', encoding='utf-8') as f:
                for result in sorted_results:
                    f.write(json.dumps(result, ensure_ascii=False) + '\n')
                    
        except Exception as e:
            print(f"      âŒ ä¿å­˜æ®µè½ç»“æœå¤±è´¥: {e}")
            raise
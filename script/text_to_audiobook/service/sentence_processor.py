#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¥å­æ‹†åˆ†ä¸ç¿»è¯‘æ¨¡å—
ä½¿ç”¨AIåŒæ—¶è¿›è¡Œå¥å­æ‹†åˆ†å’Œç¿»è¯‘ï¼Œç¡®ä¿è¯­ä¹‰ä¸€è‡´æ€§
"""

import os
import re
import json
import time
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from infra import AIClient, FileManager
from infra.config_loader import AppConfig


class SentenceProcessor:
    """å¥å­æ‹†åˆ†ä¸ç¿»è¯‘å¤„ç†å™¨"""
    
    def __init__(self, config: AppConfig):
        """
        åˆå§‹åŒ–å¥å­å¤„ç†å™¨
        
        Args:
            config: åº”ç”¨é…ç½®
        """
        self.config = config
        self.ai_client = AIClient(config.api)
        self.file_manager = FileManager()
    
    def split_sub_chapters_to_sentences(self, input_files: List[str], output_dir: str) -> List[str]:
        """
        æ‹†åˆ†æ–‡ä»¶åˆ—è¡¨ä¸ºå¥å­çº§æ–‡ä»¶ï¼ˆJSONLæ ¼å¼ï¼‰
        
        Args:
            input_files: è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            ç”Ÿæˆçš„å¥å­æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        # åˆ›å»ºè¾“å‡ºç›®å½•
        sentences_dir = os.path.join(output_dir, "sentences")
        os.makedirs(sentences_dir, exist_ok=True)
        
        output_files = []
        
        print(f"ğŸ” å¼€å§‹AIæ‹†åˆ†ç¿»è¯‘ {len(input_files)} ä¸ªå­ç« èŠ‚æ–‡ä»¶...")
        
        for i, input_file in enumerate(input_files, 1):
            try:
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
                filename = os.path.basename(input_file)
                base_name = os.path.splitext(filename)[0]
                output_file = os.path.join(sentences_dir, f"{base_name}.jsonl")
                
                print(f"ğŸ“„ [{i}/{len(input_files)}] å¤„ç†æ–‡ä»¶: {filename}")
                
                # å¤„ç†å•ä¸ªæ–‡ä»¶
                success = self._process_file(input_file, output_file)
                if success:
                    output_files.append(output_file)
                    print(f"    âœ… å·²å®ŒæˆAIæ‹†åˆ†ç¿»è¯‘: {filename}")
                else:
                    print(f"    âŒ å¤„ç†å¤±è´¥: {filename}")
                    
            except Exception as e:
                print(f"    âŒ æ‹†åˆ†ç¿»è¯‘å¤±è´¥: {e}")
                continue
        
        print(f"\nğŸ“ å¥å­æ‹†åˆ†ç¿»è¯‘å®Œæˆï¼Œè¾“å‡ºåˆ°: {sentences_dir}")
        print(f"ğŸ“Š æˆåŠŸå¤„ç†: {len(output_files)}/{len(input_files)} ä¸ªæ–‡ä»¶")
        
        return output_files
    
    def _process_file(self, input_file: str, output_file: str) -> bool:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶çš„å¥å­æ‹†åˆ†å’Œç¿»è¯‘ï¼ˆå¢é‡å¤„ç†ï¼‰
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦å¤„ç†æˆåŠŸ
        """
        try:
            # è¯»å–è¾“å…¥æ–‡ä»¶
            with open(input_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–æ ‡é¢˜å’Œæ­£æ–‡
            title, body = self._extract_title_and_body(content)
            
            if not body.strip():
                print(f"    âš ï¸ æ–‡ä»¶å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                return False
            
            # æŒ‰æ®µè½åˆ†å‰²
            paragraphs = re.split(r'\n\n', body)
            paragraphs = [p.strip() for p in paragraphs if p.strip()]
            
            print(f"    ğŸ” å¤„ç† {len(paragraphs)} ä¸ªæ®µè½")
            
            # åŠ è½½å·²æœ‰å¤„ç†ç»“æœ
            existing_results = self._load_existing_paragraph_results(output_file)
            processed_indices = {result['paragraph_index'] for result in existing_results if result.get('success', False)}
            
            # è¯†åˆ«æœªå¤„ç†çš„æ®µè½
            unprocessed_paragraphs = []
            for para_idx, paragraph in enumerate(paragraphs, 1):
                if para_idx not in processed_indices:
                    unprocessed_paragraphs.append((para_idx, paragraph))
            
            if not unprocessed_paragraphs:
                print(f"    âœ… æ‰€æœ‰æ®µè½å·²å¤„ç†å®Œæ¯•ï¼Œè·³è¿‡")
                return True
            
            print(f"    ğŸ”„ éœ€è¦å¤„ç† {len(unprocessed_paragraphs)}/{len(paragraphs)} ä¸ªæ®µè½")
            
            # å¹¶å‘å¤„ç†æœªå®Œæˆçš„æ®µè½
            new_results = []
            max_workers = min(len(unprocessed_paragraphs), self.config.api.max_concurrent_workers)
            
            print(f"    ğŸš€ å¼€å§‹å¹¶å‘å¤„ç†ï¼Œä½¿ç”¨ {max_workers} ä¸ªworker")
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰æ®µè½ä»»åŠ¡
                future_to_paragraph = {}
                for para_idx, paragraph in unprocessed_paragraphs:
                    future = executor.submit(self._process_single_paragraph, para_idx, paragraph, len(paragraphs))
                    future_to_paragraph[future] = (para_idx, paragraph)
                
                # æ”¶é›†å¹¶å‘å¤„ç†ç»“æœ
                completed_count = 0
                for future in as_completed(future_to_paragraph):
                    para_idx, paragraph = future_to_paragraph[future]
                    try:
                        paragraph_result = future.result()
                        if paragraph_result:
                            new_results.append(paragraph_result)
                        else:
                            # åˆ›å»ºå¤±è´¥ç»“æœ
                            paragraph_result = {
                                "paragraph_index": para_idx,
                                "original_text": paragraph,
                                "segments": [],
                                "success": False
                            }
                            new_results.append(paragraph_result)
                        
                        completed_count += 1
                        if completed_count % 3 == 0 or completed_count == len(unprocessed_paragraphs):
                            print(f"    ğŸ“Š å¹¶å‘è¿›åº¦: {completed_count}/{len(unprocessed_paragraphs)} ä¸ªæ®µè½")
                            
                    except Exception as e:
                        print(f"    âŒ æ®µè½ {para_idx} å¹¶å‘å¤„ç†å¼‚å¸¸: {e}")
                        # è®°å½•å¤±è´¥çš„æ®µè½
                        paragraph_result = {
                            "paragraph_index": para_idx,
                            "original_text": paragraph,
                            "segments": [],
                            "success": False
                        }
                        new_results.append(paragraph_result)
            
            # ä¿å­˜ç»“æœï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
            if new_results:
                self._save_paragraph_results(output_file, new_results, existing_results)
                
                success_count = sum(1 for result in new_results if result['success'])
                print(f"    ğŸ’¾ å·²ä¿å­˜ {len(new_results)} ä¸ªæ®µè½ç»“æœï¼ŒæˆåŠŸ {success_count} ä¸ª")
                return success_count > 0
            else:
                print(f"    âš ï¸ æœªç”Ÿæˆæ–°çš„æ®µè½ç»“æœ")
                return False
                
        except Exception as e:
            print(f"    âŒ æ–‡ä»¶å¤„ç†å¼‚å¸¸: {e}")
            return False
    
    def _extract_title_and_body(self, content: str) -> tuple[str, str]:
        """
        æå–æ ‡é¢˜å’Œæ­£æ–‡å†…å®¹
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            
        Returns:
            (æ ‡é¢˜, æ­£æ–‡å†…å®¹)
        """
        lines = content.split('\n')
        
        # ç¬¬ä¸€è¡Œæ˜¯æ ‡é¢˜
        title = lines[0].strip() if lines else "Unknown Title"
        
        # å…¶ä½™æ˜¯æ­£æ–‡ï¼ˆå»é™¤å¼€å¤´çš„ç©ºè¡Œï¼‰
        body_lines = lines[1:]
        while body_lines and not body_lines[0].strip():
            body_lines.pop(0)
        
        body = '\n'.join(body_lines)
        return title, body
    
    def _process_single_paragraph(self, para_idx: int, paragraph: str, total_paragraphs: int) -> Optional[Dict]:
        """
        å¤„ç†å•ä¸ªæ®µè½ï¼ˆç”¨äºå¹¶å‘ï¼‰
        
        Args:
            para_idx: æ®µè½ç´¢å¼•
            paragraph: æ®µè½å†…å®¹
            total_paragraphs: æ€»æ®µè½æ•°
            
        Returns:
            æ®µè½å¤„ç†ç»“æœï¼Œå¤±è´¥è¿”å›None
        """
        try:
            print(f"      ğŸ“ æ®µè½ {para_idx}/{total_paragraphs}: {len(paragraph)} å­—ç¬¦")
            
            # ä½¿ç”¨AIè¿›è¡Œæ‹†åˆ†å’Œç¿»è¯‘
            segments = self._split_and_translate_with_ai(paragraph)
            
            # æ„å»ºæ®µè½ç»“æœ
            paragraph_result = {
                "paragraph_index": para_idx,
                "original_text": paragraph,
                "segments": segments if segments else [],
                "success": bool(segments)
            }
            
            if segments:
                print(f"      âœ… æ®µè½ {para_idx} ç”Ÿæˆ {len(segments)} ä¸ªå¥å­ç‰‡æ®µ")
            else:
                print(f"      âŒ æ®µè½ {para_idx} å¤„ç†å¤±è´¥")
            
            return paragraph_result
            
        except Exception as e:
            print(f"      âŒ æ®µè½ {para_idx} å¤„ç†å¼‚å¸¸: {e}")
            return None
    
    def _split_and_translate_with_ai(self, paragraph: str) -> List[Dict[str, str]]:
        """
        ä½¿ç”¨AIåŒæ—¶è¿›è¡Œå¥å­æ‹†åˆ†å’Œç¿»è¯‘
        
        Args:
            paragraph: è¾“å…¥æ®µè½
            
        Returns:
            æ‹†åˆ†ç¿»è¯‘ç»“æœåˆ—è¡¨ [{"original": "è‹±æ–‡", "translation": "ä¸­æ–‡"}, ...]
        """
        try:
            system_prompt = """âš ï¸ ä¸¥æ ¼è¦æ±‚ï¼šå¿…é¡»ä¸”åªèƒ½è¿”å›JSONæ•°ç»„æ ¼å¼ï¼

# å¥å­æ‹†åˆ†ä¸ç¿»è¯‘ä¸“å®¶

## âŒ ç»å¯¹ç¦æ­¢è¿”å›çš„å†…å®¹
- ä»»ä½•æ–‡å­—è¯´æ˜ã€è§£é‡Šã€æ³¨é‡Š
- ä»£ç å—æ ‡è®°ï¼ˆå¦‚```json```ï¼‰
- å‰è¨€ã€æ€»ç»“ã€æç¤ºæ€§æ–‡å­—
- é™¤JSONæ•°ç»„å¤–çš„ä»»ä½•å…¶ä»–æ ¼å¼

## âœ… æ­£ç¡®è¾“å‡ºæ ¼å¼ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šçŸ­å¥ä¿æŒå®Œæ•´ï¼ˆä¸æ‹†åˆ†ï¼‰
è¾“å…¥ï¼šAlice was beginning to get very tired.
è¾“å‡ºï¼š
[
  {"original": "Alice was beginning to get very tired.", "translation": "çˆ±ä¸½ä¸å¼€å§‹æ„Ÿåˆ°éå¸¸ç–²å€¦ã€‚"}
]

### ç¤ºä¾‹2ï¼šé•¿å¥åˆç†æ‹†åˆ†
è¾“å…¥ï¼šAlice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do.
è¾“å‡ºï¼š
[
  {"original": "Alice was beginning to get very tired of sitting by her sister on the bank,", "translation": "çˆ±ä¸½ä¸å¼€å§‹å¯¹ååœ¨å§å§èº«è¾¹çš„æ²³å²¸ä¸Šæ„Ÿåˆ°éå¸¸ç–²å€¦ï¼Œ"},
  {"original": "and of having nothing to do.", "translation": "ä¹ŸåŒå€¦äº†æ— æ‰€äº‹äº‹ã€‚"}
]

### ç¤ºä¾‹3ï¼šè¶…é•¿å¥å¿…é¡»å……åˆ†æ‹†åˆ†
è¾“å…¥ï¼šAlice had learnt several things of this sort in her lessons in the schoolroom, and though this was not a very good opportunity for showing off her knowledge, as there was no one to listen to her, still it was good practice.
è¾“å‡ºï¼š
[
  {"original": "Alice had learnt several things of this sort in her lessons in the schoolroom,", "translation": "çˆ±ä¸½ä¸åœ¨å­¦æ ¡é‡Œä¸Šè¯¾æ—¶å­¦è¿‡å¾ˆå¤šè¿™ç±»ä¸œè¥¿ï¼Œ"},
  {"original": "and though this was not a very good opportunity for showing off her knowledge,", "translation": "è™½ç„¶è¿™ä¸æ˜¯ç‚«è€€å¥¹çŸ¥è¯†çš„å¥½æœºä¼šï¼Œ"},
  {"original": "as there was no one to listen to her,", "translation": "å› ä¸ºæ²¡æœ‰äººå¬å¥¹è¯´è¯ï¼Œ"},
  {"original": "still it was good practice.", "translation": "ä½†è¿™ä»ç„¶æ˜¯å¾ˆå¥½çš„ç»ƒä¹ ã€‚"}
]

## ä»»åŠ¡æè¿°
è¯·å°†ç»™å®šçš„è‹±æ–‡é•¿å¥æŒ‰ç…§**è¯­ä¹‰å®Œæ•´æ€§**æ‹†åˆ†æˆåˆé€‚çš„ç‰‡æ®µï¼Œç„¶åå¯¹æ¯ä¸ªç‰‡æ®µè¿›è¡Œä¿¡è¾¾é›…çš„ä¸­æ–‡ç¿»è¯‘ã€‚

## æ ¸å¿ƒåŸåˆ™
**ä¸¥æ ¼ä¿æŒåŸæ–‡å®Œæ•´æ€§**ï¼šä¸å¾—ä»¥ä»»ä½•æ–¹å¼ä¿®æ”¹ã€é‡ç»„ã€åˆ å‡æˆ–æ·»åŠ åŸæ–‡å†…å®¹ï¼ŒåŒ…æ‹¬æ‰€æœ‰æ ‡ç‚¹ç¬¦å·ã€å¤§å°å†™ã€æ–œä½“ç­‰æ ¼å¼æ ‡è®°ã€‚

## æ‹†åˆ†è§„åˆ™
1. **é•¿åº¦æ§åˆ¶**ï¼š
   - çŸ­å¥ï¼ˆâ‰¤15ä¸ªå•è¯ï¼‰ï¼šä¿æŒåŸæ ·ï¼Œä¸æ‹†åˆ†
   - é•¿å¥ï¼ˆ>15ä¸ªå•è¯ï¼‰ï¼šå¿…é¡»æ‹†åˆ†ä¸º8-15ä¸ªå•è¯çš„ç‰‡æ®µ
   - ä¸¥ç¦ç”Ÿæˆè¶…è¿‡15ä¸ªå•è¯çš„ç‰‡æ®µ
2. **æ‹†åˆ†åˆ¤æ–­**ï¼š
   - ä¼˜å…ˆè€ƒè™‘å¥å­æ˜¯å¦å·²ç»è¶³å¤Ÿç®€æ´å®Œæ•´
   - é¿å…ä¸å¿…è¦çš„è¿‡åº¦æ‹†åˆ†çŸ­å¥
   - ç¡®ä¿é•¿å¥å……åˆ†æ‹†åˆ†ï¼Œä¸ç•™è¿‡é•¿ç‰‡æ®µ
3. **æ‹†åˆ†åŸåˆ™**ï¼š
   - ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼Œåœ¨è‡ªç„¶åœé¡¿å¤„æ‹†åˆ†
   - ä¸¥æ ¼éµå¾ªåŸæ–‡çš„è¯­æ³•ç»“æ„å’Œæ ‡ç‚¹ç¬¦å·è¿›è¡Œæ‹†åˆ†
   - ä¼˜å…ˆåœ¨ä»å¥è¾¹ç•Œã€è¿è¯ã€æ ‡ç‚¹å¤„æ‹†åˆ†
   - ä¿æŒä¿®è¾ç»“æ„å’Œé€»è¾‘è¿è´¯æ€§
   - é¿å…ç ´åä¹ è¯­å’Œå›ºå®šæ­é…
   - é•¿å¥å¿…é¡»å……åˆ†æ‹†åˆ†ï¼Œç¡®ä¿æ¯ä¸ªç‰‡æ®µéƒ½åœ¨åˆç†é•¿åº¦èŒƒå›´å†…
3. **æ ¼å¼ä¿ç•™**ï¼š
   - å®Œæ•´ä¿ç•™æ‰€æœ‰æ ‡ç‚¹ç¬¦å·ï¼ˆé€—å·ã€åˆ†å·ã€å¼•å·ã€æ‹¬å·ç­‰ï¼‰
   - ä¿ç•™æ–œä½“æ ‡è®° `_word_` ä¸ä½œä»»ä½•æ”¹åŠ¨
   - ä¿ç•™å¯¹è¯çš„ç›´æ¥å¼•è¯­å½¢å¼
   - ä¿æŒæ‹¬å·å†…å®¹çš„å®Œæ•´æ€§

## ç¿»è¯‘è¦æ±‚
- **ä¿¡**ï¼šå‡†ç¡®ä¼ è¾¾åŸæ„ï¼Œä¸é—æ¼ä»»ä½•ç»†èŠ‚
- **è¾¾**ï¼šä¸­æ–‡æµç•…è‡ªç„¶ï¼Œç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯
- **é›…**ï¼šæ–‡å­¦æ€§è¡¨è¾¾ï¼Œä¿æŒåŸæ–‡é£æ ¼éŸµå‘³
   - æ°å½“å¤„ç†æ–œä½“å¼ºè°ƒï¼ˆåœ¨ç¿»è¯‘ä¸­ä½¿ç”¨ä¸­æ–‡å¼ºè°ƒè¡¨è¾¾ï¼‰
   - ä¿æŒå¯¹è¯çš„ç›´æ¥å¼•è¯­å½¢å¼
   - è‡ªç„¶å¤„ç†æ‹¬å·å†…çš„è¡¥å……è¯´æ˜

## è¾“å‡ºæ ¼å¼è¦æ±‚
- å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ•°ç»„
- æ•°ç»„ä¸­æ¯ä¸ªå¯¹è±¡å¿…é¡»åŒ…å«"original"å’Œ"translation"ä¸¤ä¸ªå­—æ®µ
- ä¸å…è®¸æœ‰ä»»ä½•é¢å¤–çš„æ–‡å­—æˆ–æ ¼å¼

## ğŸ”¥ æœ€ç»ˆå¼ºè°ƒï¼š
- åªè¿”å›çº¯JSONæ•°ç»„ï¼ç»ä¸å…è®¸ä»»ä½•å…¶ä»–å†…å®¹ï¼
- çŸ­å¥ï¼ˆâ‰¤15è¯ï¼‰ä¿æŒå®Œæ•´ï¼Œé¿å…è¿‡åº¦æ‹†åˆ†ï¼
- é•¿å¥ï¼ˆ>15è¯ï¼‰å¿…é¡»å……åˆ†æ‹†åˆ†ä¸º8-15è¯ç‰‡æ®µï¼
- ä¸¥ç¦ç”Ÿæˆè¶…è¿‡15ä¸ªå•è¯çš„ç‰‡æ®µï¼
- æ¯ä¸ªç‰‡æ®µå¿…é¡»åœ¨åˆç†é•¿åº¦èŒƒå›´å†…ï¼ˆ8-15è¯ï¼‰ï¼"""
            
            user_prompt = f"è¯·å¯¹ä»¥ä¸‹è‹±æ–‡æ®µè½è¿›è¡Œæ‹†åˆ†å’Œç¿»è¯‘ï¼š\n\n{paragraph}"
            
            # è°ƒç”¨AI API
            response = self.ai_client.chat_completion(
                user_prompt, 
                system_prompt,
                temperature=0.8, 
                max_tokens=4000
            )
            
            if not response or not response.strip():
                print(f"      âš ï¸ AIè¿”å›ç©ºç»“æœ")
                return []
            
            # è§£æJSONå“åº”
            try:
                # æ™ºèƒ½æå–JSONå†…å®¹
                json_str = self._extract_json_from_response(response)
                if not json_str:
                    print(f"      âš ï¸ æ— æ³•ä»å“åº”ä¸­æå–JSON")
                    return []
                
                sentences = json.loads(json_str)
                
                # éªŒè¯ç»“æœæ ¼å¼
                if not isinstance(sentences, list):
                    print(f"      âš ï¸ JSONæ ¼å¼é”™è¯¯ï¼Œä¸æ˜¯æ•°ç»„")
                    return []
                
                valid_sentences = []
                for sentence in sentences:
                    if isinstance(sentence, dict) and 'original' in sentence and 'translation' in sentence:
                        valid_sentences.append({
                            'original': sentence['original'].strip(),
                            'translation': sentence['translation'].strip()
                        })
                
                if valid_sentences:
                    return valid_sentences
                else:
                    print(f"      âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å¥å­å¯¹è±¡")
                    return []
                    
            except json.JSONDecodeError as e:
                print(f"      âš ï¸ JSONè§£æå¤±è´¥: {e}")
                return []
                
        except Exception as e:
            print(f"      âŒ AIæ‹†åˆ†ç¿»è¯‘å¼‚å¸¸: {e}")
            return []
    
    def _extract_json_from_response(self, response: str) -> str:
        """
        ä»AIå“åº”ä¸­æ™ºèƒ½æå–JSONå†…å®¹
        
        Args:
            response: AIè¿”å›çš„åŸå§‹å“åº”
            
        Returns:
            æå–çš„JSONå­—ç¬¦ä¸²ï¼Œå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        if not response:
            return ""
        
        # æ¸…ç†å“åº”å†…å®¹
        response = response.strip()
        
        # 1. å°è¯•ç§»é™¤ä»£ç å—æ ‡è®°
        if response.startswith('```json'):
            response = response[7:]  # ç§»é™¤ ```json
        if response.startswith('```'):
            response = response[3:]  # ç§»é™¤ ```
        if response.endswith('```'):
            response = response[:-3]  # ç§»é™¤ç»“å°¾çš„ ```
        
        response = response.strip()
        
        # 2. å¯»æ‰¾ç¬¬ä¸€ä¸ª[å’Œæœ€åä¸€ä¸ª]
        first_bracket = response.find('[')
        last_bracket = response.rfind(']')
        
        if first_bracket != -1 and last_bracket != -1 and last_bracket > first_bracket:
            json_str = response[first_bracket:last_bracket + 1]
            return json_str
        
        # 3. å¦‚æœå·²ç»æ˜¯å®Œæ•´JSONæ ¼å¼ï¼Œç›´æ¥è¿”å›
        if response.startswith('[') and response.endswith(']'):
            return response
        
        return ""
    
    def _load_existing_paragraph_results(self, output_file: str) -> List[Dict]:
        """
        åŠ è½½å·²æœ‰çš„æ®µè½å¤„ç†ç»“æœ
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            å·²æœ‰çš„æ®µè½ç»“æœåˆ—è¡¨
        """
        if not os.path.exists(output_file):
            return []
        
        results = []
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    
                    try:
                        result = json.loads(line)
                        if isinstance(result, dict) and 'paragraph_index' in result:
                            results.append(result)
                    except json.JSONDecodeError as e:
                        print(f"      âš ï¸ è§£æJSONè¡Œ {line_num} å¤±è´¥: {e}")
                        continue
                        
        except Exception as e:
            print(f"      âš ï¸ è¯»å–å·²æœ‰ç»“æœæ–‡ä»¶å¤±è´¥ {output_file}: {e}")
            
        return results
    
    def _save_paragraph_results(self, output_file: str, new_results: List[Dict], existing_results: List[Dict]):
        """
        ä¿å­˜æ®µè½å¤„ç†ç»“æœï¼ˆåˆå¹¶æ–°æ—§ç»“æœï¼‰
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            new_results: æ–°çš„å¤„ç†ç»“æœ
            existing_results: å·²æœ‰çš„å¤„ç†ç»“æœ
        """
        try:
            # åˆå¹¶ç»“æœï¼šç”¨æ–°ç»“æœè¦†ç›–ç›¸åŒæ®µè½ç´¢å¼•çš„æ—§ç»“æœ
            all_results = {}
            
            # å…ˆæ·»åŠ å·²æœ‰ç»“æœ
            for result in existing_results:
                para_idx = result.get('paragraph_index')
                if para_idx is not None:
                    all_results[para_idx] = result
            
            # å†æ·»åŠ æ–°ç»“æœï¼ˆè¦†ç›–ç›¸åŒç´¢å¼•ï¼‰
            for result in new_results:
                para_idx = result.get('paragraph_index')
                if para_idx is not None:
                    all_results[para_idx] = result
            
            # æŒ‰æ®µè½ç´¢å¼•æ’åºå¹¶å†™å…¥æ–‡ä»¶
            sorted_results = sorted(all_results.values(), key=lambda x: x.get('paragraph_index', 0))
            
            with open(output_file, 'w', encoding='utf-8') as f:
                for result in sorted_results:
                    f.write(json.dumps(result, ensure_ascii=False) + '\n')
                    
        except Exception as e:
            print(f"      âŒ ä¿å­˜æ®µè½ç»“æœå¤±è´¥: {e}")
            raise
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¥å­æ‹†åˆ†æ¨¡å—
å°†å­ç« èŠ‚çš„æ®µè½æ‹†åˆ†ä¸ºå¥å­æ¯ä¸ªå¥å­å ä¸€è¡Œä¿ç•™æ®µè½é—´éš”
ä½¿ç”¨å¼•å·ä¼˜å…ˆçš„è¯­ä¹‰æ„ŸçŸ¥åˆ†å‰²æ–¹æ³•
"""

import os
import re
import nltk
import pysbd
from typing import List
from infra.config_loader import AppConfig

# è°ƒè¯•å¼€å…³
DEBUG_SENTENCE_PROCESSING = False

def debug_print(stage, content):
    """è°ƒè¯•è¾“å‡ºå‡½æ•°"""
    if DEBUG_SENTENCE_PROCESSING:
        if isinstance(content, list):
            print(f"[DEBUG] {stage}: {len(content)} items")
            for i, item in enumerate(content):
                print(f"  [{i}]: {repr(item)}")
        else:
            print(f"[DEBUG] {stage}: {repr(content)}")
        print()  # ç©ºè¡Œåˆ†éš”

# é…ç½®ï¼šæ‹¬å·ç±»ç¬¦å·ï¼ˆæ•´ä½“ä¿ç•™ï¼‰
PAIR_SYMBOLS_PARENS = [
    ("(", ")"),
    ("[", "]"),
    ("{", "}"),
    ("ï¼ˆ", "ï¼‰"),
    ("ã€", "ã€‘"),
    ("ã€Š", "ã€‹"),
]

# é…ç½®ï¼šå¼•å·ç±»ç¬¦å·ï¼ˆå…è®¸å†…éƒ¨ç»§ç»­æ‹†åˆ†ï¼‰
PAIR_SYMBOLS_QUOTES = [
    ("â€˜", "â€™"),
    ("â€œ", "â€"),
    ('"', '"'),  # æ¢å¤æ ‡å‡†åŒå¼•å·ç”¨äºè°ƒè¯•
]

# é…ç½®ï¼šå¥ä¸­åˆ†éš”ç¬¦ï¼ˆå¯ä»¥å†æ‰©å±•ï¼‰
SPLIT_PUNCT = [",", "ï¼Œ", ":", "ï¼š", ";", "ï¼›", "!", "?", "."]

# é…ç½®ï¼šè‹±è¯­å¸¸è§ç¼©å†™è¯ï¼ˆä¸åº”åœ¨å¥å·å¤„æ‹†åˆ†ï¼‰
ENGLISH_ABBREVIATIONS = ["Dr", "Mrs", "Ms", "Mr", "Prof", "St", "Ave", "etc", "vs", "Jr", "Sr", "Co", "Inc", "Ltd", "Corp"]

# é…ç½®ï¼šè‹±è¯­ç¼©å†™è¯æ¨¡å¼ï¼ˆå•å¼•å·åœ¨è¿™äº›æƒ…å†µä¸‹ä¸åº”ä½œä¸ºå¼•å·åˆ†éš”ç¬¦ï¼‰
CONTRACTION_PATTERNS = [
    "'t",   # don't, can't, won't, isn't, aren't, haven't, hasn't
    "'m",    # I'm, we'm  
    "'am",
    "'re",   # you're, we're, they're
    "'ve",   # I've, you've, we've, they've
    "'d",    # I'd, you'd, he'd, she'd, we'd, they'd
    "'ll",   # I'll, you'll, he'll, she'll, we'll, they'll
    "'s",    # possessive: John's, Mary's, æˆ– is/has: he's, she's
]

# å¥æœ«åˆ†éš”ç¬¦ï¼ˆä¸åº”åœ¨æ­¤å¤„åˆå¹¶å­å¥ï¼‰
SENTENCE_TERMINATORS = [".", "?", ";"]

# å¯åˆå¹¶çš„åˆ†éš”ç¬¦ + æˆå¯¹ç¬¦å·çš„ç»“æŸéƒ¨åˆ†
PREV_MERGEABLE_SEPARATORS = [".", "!", "?", ";"]
NEXT_SYMBOL_ENDINGS = ["â€", "â€™", ")", "]", "}", "ï¼‰", "ã€‘", "ã€‹"]

PREV_SYMBOL_ENDINGS = ["â€", "â€™", ")", "]", "}", "ï¼‰", "ã€‘", "ã€‹"]
NEXT_MERGEABLE_SEPARATORS = [".", "!", "?", ";",  ",", "ï¼Œ"]


# é•¿åº¦æ§åˆ¶å¸¸é‡ - é’ˆå¯¹è¯­éŸ³åˆæˆä¼˜åŒ–
MAX_SENTENCE_LENGTH = 80      # ç›®æ ‡æœ€å¤§é•¿åº¦ï¼ˆé€‚åˆè¯­éŸ³åˆæˆï¼‰
MIN_MERGE_LENGTH = 30      # æœ€å°åˆå¹¶é•¿åº¦
MAX_MERGE_LENGTH = 100      # æœ€å¤§åˆå¹¶é•¿åº¦


class SentenceProcessor:
    """å¥å­æ‹†åˆ†å™¨"""
    
    def __init__(self, config: AppConfig):
        """
        åˆå§‹åŒ–å¥å­æ‹†åˆ†å™¨
        """
        self._ensure_nltk_data()
        self._init_pysbd()
    
    def _ensure_nltk_data(self):
        """
        ç¡®ä¿NLTKæ•°æ®åŒ…å¯ç”¨
        """
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            print("ä¸‹è½½NLTK punktæ•°æ®åŒ…...")
            nltk.download('punkt')
    
    def _init_pysbd(self):
        """
        åˆå§‹åŒ–pySBDåˆ†æ®µå™¨
        """
        self.segmenter = pysbd.Segmenter(language="en", clean=False)
        print("âœ… pySBDåˆ†æ®µå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def split_sub_chapters_to_sentences(self, input_files: List[str], output_dir: str) -> List[str]:
        """
        æ‹†åˆ†æ–‡ä»¶åˆ—è¡¨ä¸ºå¥å­çº§æ–‡ä»¶
        
        Args:
            input_files: è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            ç”Ÿæˆçš„å¥å­æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        # åˆ›å»ºè¾“å‡ºç›®å½•
        sentences_dir = os.path.join(output_dir, "sentences")
        os.makedirs(sentences_dir, exist_ok=True)
        
        output_files = []
        
        for input_file in input_files:
            try:
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
                filename = os.path.basename(input_file)
                output_file = os.path.join(sentences_dir, filename)
                
                # å¤„ç†å•ä¸ªæ–‡ä»¶
                self._process_file(input_file, output_file)
                output_files.append(output_file)
                
                print(f"ğŸ“ å·²å¤„ç†å¥å­æ‹†åˆ†: {filename}")
            except Exception as e:
                print(f"âŒ æ‹†åˆ†å¤±è´¥: {e}")
                continue
        
        print(f"\nğŸ“ å¥å­æ‹†åˆ†å®Œæˆï¼Œè¾“å‡ºåˆ°: {sentences_dir}")
        return output_files
    
    def _process_file(self, input_file: str, output_file: str):
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶çš„å¥å­æ‹†åˆ†
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # è¯»å–è¾“å…¥æ–‡ä»¶
        with open(input_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–æ ‡é¢˜å’Œæ­£æ–‡
        title, body = self._extract_title_and_body(content)
        
        # å¤„ç†æ®µè½å¥å­æ‹†åˆ†ï¼ŒåŒæ—¶ä¿å­˜ä¸­é—´ç»“æœ
        processed_content, pysbd_content = self._split_paragraphs_to_sentences(body)
        
        # æ„å»ºæœ€ç»ˆå†…å®¹
        final_content = f"{title}\n\n{processed_content}"
        
        # å†™å…¥è¾“å‡ºæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(final_content)        
        
        # pySBDåŸå§‹ç»“æœ
        # base_name = os.path.splitext(output_file)[0]
        # pysbd_file = f"{base_name}_pysbd.txt"
        # pysbd_final_content = f"{title}\n\n{pysbd_content}"
        # with open(pysbd_file, 'w', encoding='utf-8') as f:
        #     f.write(pysbd_final_content)
    
    def _extract_title_and_body(self, content: str) -> tuple[str, str]:
        """
        æå–æ ‡é¢˜å’Œæ­£æ–‡å†…å®¹
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            
        Returns:
            (æ ‡é¢˜, æ­£æ–‡å†…å®¹)
        """
        lines = content.split('\n')
        
        # ç¬¬ä¸€è¡Œæ˜¯æ ‡é¢˜
        title = lines[0].strip() if lines else "Unknown Title"
        
        # å…¶ä½™æ˜¯æ­£æ–‡ï¼ˆå»é™¤å¼€å¤´çš„ç©ºè¡Œï¼‰
        body_lines = lines[1:]
        while body_lines and not body_lines[0].strip():
            body_lines.pop(0)
        
        body = '\n'.join(body_lines)
        return title, body

    def _split_paragraphs_to_sentences(self, content: str) -> tuple[str, str]:
        """
        å°†å†…å®¹æŒ‰æ®µè½æ‹†åˆ†å†å°†æ¯ä¸ªæ®µè½æ‹†åˆ†ä¸ºå¥å­åŒæ—¶è¿”å›ä¸­é—´å¤„ç†ç»“æœ
        
        Args:
            content: æ­£æ–‡å†…å®¹
            
        Returns:
            (æœ€ç»ˆå¤„ç†å†…å®¹, pySBDåŸå§‹å†…å®¹)
        """
        # æŒ‰æ®µè½åˆ†å‰²ï¼ˆåŒæ¢è¡Œåˆ†å‰²ï¼‰
        paragraphs = re.split(r'\n\n', content)
        
        # è¿‡æ»¤ç©ºæ®µè½
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        
        final_paragraphs = []
        pysbd_paragraphs = []
        
        for paragraph in paragraphs:
            # å¯¹æ®µè½è¿›è¡Œå¥å­æ‹†åˆ†
            final_sentences, pysbd_sentences = self._split_sentences(paragraph)
            
            # å°†å¥å­åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆæ¯å¥ä¸€è¡Œï¼‰
            if final_sentences:
                final_paragraphs.append('\n'.join(final_sentences))
            if pysbd_sentences:
                pysbd_paragraphs.append('\n'.join(pysbd_sentences))
        
        # æ®µè½é—´ç”¨ç©ºè¡Œåˆ†éš”
        return (
            '\n\n'.join(final_paragraphs),
            '\n\n'.join(pysbd_paragraphs),
        )

    def _split_sentences(self, text: str) -> tuple[List[str], List[str]]:
        """
        å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå¥å­è¿”å›
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            (æœ€ç»ˆå¥å­åˆ—è¡¨, pySBDåŸå§‹å¥å­åˆ—è¡¨)
        """
        # æ¸…ç†æ–‡æœ¬ï¼ˆç§»é™¤å¤šä½™ç©ºç™½ï¼‰
        text = re.sub(r'\s+', ' ', text.strip())
        
        if not text:
            return [], []
        
        # ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨pySBDè¿›è¡ŒåŸºç¡€å¥å­åˆ†å‰²
        if len(text) <= MAX_SENTENCE_LENGTH:
            return [text], [text] 

        pysbd_sentences = self._split_with_pysbd(text)
        pysbd_sentences = [s.strip() for s in pysbd_sentences if s.strip()]
        
        # ç¬¬äºŒé˜¶æ®µï¼šé•¿å¥æ‹†åˆ†é€»è¾‘
        final_sentences = self._split_long_sentences(pysbd_sentences)
        final_sentences = [s.strip() for s in final_sentences if s.strip()]
        
        return final_sentences, pysbd_sentences
    
    def _split_with_nltk(self, text: str) -> List[str]:
        """
        ä½¿ç”¨NLTKè¿›è¡Œå¥å­åˆ†å‰²
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å¥å­åˆ—è¡¨
        """
        return nltk.sent_tokenize(text)

    def _split_with_pysbd(self, text: str) -> List[str]:
        """
        ä½¿ç”¨pySBDè¿›è¡Œå¥å­åˆ†å‰²
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å¥å­åˆ—è¡¨
        """
        debug_print("pySBDè¾“å…¥", text)
        result = self.segmenter.segment(text)
        result = [sent.strip() for sent in result if sent.strip()]
        debug_print("pySBDè¾“å‡º", result)
        return result
    
    
    def _split_long_sentences(self, sentences: List[str]) -> List[str]:
        """
        æ–°çš„é•¿å¥æ‹†åˆ†ç­–ç•¥: æˆå¯¹ç¬¦å·ä¿æŠ¤ + åˆ†éš”ç¬¦æ‹†åˆ† + æ™ºèƒ½åˆå¹¶
        
        Args:
            sentences: åŸå§‹å¥å­åˆ—è¡¨
            
        Returns:
            å¤„ç†åçš„å¥å­åˆ—è¡¨
        """
        result = []
        
        for sentence in sentences:
            if len(sentence) <= MAX_SENTENCE_LENGTH:
                result.append(sentence)
                continue
            
            # å¯¹é•¿å¥è¿›è¡Œæ‹†åˆ†-åˆå¹¶å¤„ç†
            split_result = self.split_into_clauses(sentence)
            result.extend(split_result)
        
        return result
    
    def _is_abbreviation(self, position: int, text: str) -> bool:
        """æ£€æµ‹æŒ‡å®šä½ç½®çš„å¥å·å‰æ˜¯å¦ä¸ºè‹±è¯­ç¼©å†™è¯"""
        if position == 0:
            return False
        
        # å‘å‰æŸ¥æ‰¾å•è¯è¾¹ç•Œ
        word_start = position - 1
        while word_start > 0 and text[word_start - 1].isalpha():
            word_start -= 1
        
        if word_start == position:
            return False
        
        # æå–å¯èƒ½çš„ç¼©å†™è¯
        word = text[word_start:position]
        return word in ENGLISH_ABBREVIATIONS
    
    def _get_quote_type(self, ch: str) -> tuple[str, str] | None:
        """è·å–å¼•å·å­—ç¬¦çš„å¼€å§‹å’Œç»“æŸç¬¦å·ï¼Œå¦‚æœä¸æ˜¯å¼•å·è¿”å›None"""
        for open_quote, close_quote in PAIR_SYMBOLS_QUOTES:
            if ch == open_quote or ch == close_quote:
                return open_quote, close_quote
        return None
    
    def _get_paren_type(self, ch: str) -> tuple[str, str] | None:
        """è·å–æ‹¬å·å­—ç¬¦çš„å¼€å§‹å’Œç»“æŸç¬¦å·ï¼Œå¦‚æœä¸æ˜¯æ‹¬å·è¿”å›None"""
        for open_paren, close_paren in PAIR_SYMBOLS_PARENS:
            if ch == open_paren or ch == close_paren:
                return open_paren, close_paren
        return None
    
    def _split_by_quotes_and_parens(self, text: str) -> List[tuple[str, int]]:
        """
        ç¬¬1æ­¥ï¼šæŒ‰å¼•å·å’Œæ‹¬å·æ‹†åˆ†æ–‡æœ¬ï¼Œä¸ºæ¯ä¸ªç‰‡æ®µåˆ†é…åºå·
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            List of (æ–‡æœ¬ç‰‡æ®µ, æºåºå·) å…ƒç»„
        """
        segments = []
        buf = []
        current_segment_index = 0
        
        # ç»Ÿä¸€çš„ç¬¦å·çŠ¶æ€è·Ÿè¸ª
        quote_stack = []  # å¼•å·æ ˆï¼Œè®°å½•å½“å‰æ‰“å¼€çš„å¼•å·ç±»å‹
        paren_count = 0   # æ‹¬å·åµŒå¥—å±‚çº§
        
        def add_segment_if_not_empty():
            """æ·»åŠ å½“å‰ç¼“å†²åŒºå†…å®¹ä¸ºæ–°ç‰‡æ®µ"""
            nonlocal current_segment_index
            if buf:
                clause = ''.join(buf).strip()
                if clause:
                    segments.append((clause, current_segment_index))
                    current_segment_index += 1
                buf.clear()
        
        i = 0
        while i < len(text):
            ch = text[i]
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¼•å·å­—ç¬¦
            quote_info = self._get_quote_type(ch)
            if quote_info:
                open_quote, close_quote = quote_info
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºç¼©å†™è¯ä¸­çš„æ’‡å·ï¼Œå¦‚æœæ˜¯åˆ™ä¸ä½œä¸ºå¼•å·å¤„ç†
                if ch in ["'", "'"] and self._is_contraction_apostrophe(i, text):
                    buf.append(ch)
                else:
                    # åªåœ¨æ²¡æœ‰æ‹¬å·åµŒå¥—æ—¶å¤„ç†å¼•å·
                    if paren_count == 0:
                        if ch == open_quote:
                            # æ£€æŸ¥æ˜¯å¦ä¸ºå¼€å§‹å¼•å·
                            if not quote_stack or quote_stack[-1] != (open_quote, close_quote):
                                # å¼€å§‹æ–°çš„å¼•å·åŒºåŸŸ - å…ˆä¿å­˜å½“å‰ç¼“å†²åŒº
                                add_segment_if_not_empty()
                                buf.append(ch)
                                quote_stack.append((open_quote, close_quote))
                            else:
                                buf.append(ch)
                        elif ch == close_quote:
                            # æ£€æŸ¥æ˜¯å¦ä¸ºç»“æŸå¼•å·
                            if quote_stack and quote_stack[-1] == (open_quote, close_quote):
                                buf.append(ch)
                                # ç»“æŸå½“å‰å¼•å·åŒºåŸŸ
                                quote_stack.pop()
                                add_segment_if_not_empty()
                            else:
                                buf.append(ch)
                        else:
                            buf.append(ch)
                    else:
                        buf.append(ch)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ‹¬å·å­—ç¬¦
            elif self._get_paren_type(ch):
                open_paren, close_paren = self._get_paren_type(ch)
                
                # åªåœ¨æ²¡æœ‰å¼•å·åµŒå¥—æ—¶è®¡ç®—æ‹¬å·å±‚çº§
                if not quote_stack:
                    if ch == open_paren:
                        # å¼€å§‹æ–°çš„æ‹¬å·åŒºåŸŸ - å…ˆä¿å­˜å½“å‰ç¼“å†²åŒº
                        add_segment_if_not_empty()
                        buf.append(ch)
                        paren_count += 1
                    elif ch == close_paren and paren_count > 0:
                        buf.append(ch)
                        paren_count -= 1
                        # å¦‚æœæ‹¬å·å®Œå…¨é—­åˆï¼Œç»“æŸå½“å‰ç‰‡æ®µ
                        if paren_count == 0:
                            add_segment_if_not_empty()
                    else:
                        buf.append(ch)
                else:
                    buf.append(ch)
            
            # æ™®é€šå­—ç¬¦
            else:
                buf.append(ch)
            
            i += 1
        
        # æ”¶å°¾å¤„ç†
        add_segment_if_not_empty()
        
        debug_print("ç¬¬1æ­¥-å¼•å·æ‹¬å·æ‹†åˆ†", [(seg[0], seg[1]) for seg in segments])
        return segments

    def _is_contraction_apostrophe(self, position: int, text: str) -> bool:
        """æ£€æµ‹æŒ‡å®šä½ç½®çš„å•å¼•å·æ˜¯å¦ä¸ºè‹±è¯­ç¼©å†™è¯ä¸­çš„æ’‡å·"""
        if position == 0 or position >= len(text) - 1:
            return False
        
        # æ£€æŸ¥æ˜¯å¦ç¬¦åˆç¼©å†™è¯æ¨¡å¼
        for pattern in CONTRACTION_PATTERNS:
            pattern_start = position
            pattern_end = position + len(pattern)
            
            if pattern_end <= len(text):
                if text[pattern_start:pattern_end] == pattern:
                    # æ£€æŸ¥å‰é¢æ˜¯å¦æœ‰å­—æ¯ï¼ˆç¡®ä¿æ˜¯å•è¯çš„ä¸€éƒ¨åˆ†ï¼‰
                    if position > 0 and text[position-1].isalpha():
                        # æ£€æŸ¥åé¢æ˜¯å¦æ˜¯å•è¯è¾¹ç•Œï¼ˆç©ºæ ¼ã€æ ‡ç‚¹ã€æ–‡æœ¬ç»“å°¾ï¼‰
                        if (pattern_end >= len(text) or 
                            text[pattern_end].isspace() or 
                            text[pattern_end] in SPLIT_PUNCT or
                            text[pattern_end] in '"",ï¼Œ"'):
                            return True
        
        return False
    
    def _split_by_punctuation(self, segments: List[tuple[str, int]]) -> List[tuple[str, int]]:
        """
        ç¬¬2æ­¥ï¼šæŒ‰åˆ†éš”ç¬¦æ‹†åˆ†é•¿åº¦è¶…è¿‡é˜ˆå€¼çš„ç‰‡æ®µ
        
        Args:
            segments: ç¬¬1æ­¥çš„è¾“å‡º - (æ–‡æœ¬, æºåºå·) å…ƒç»„åˆ—è¡¨
            
        Returns:
            è¿›ä¸€æ­¥æ‹†åˆ†çš„ (æ–‡æœ¬, æºåºå·) å…ƒç»„åˆ—è¡¨
        """
        result = []
        
        for text, source_idx in segments:
            if len(text) <= MAX_SENTENCE_LENGTH:
                # çŸ­ç‰‡æ®µç›´æ¥ä¿ç•™
                result.append((text, source_idx))
            else:
                # é•¿ç‰‡æ®µæŒ‰åˆ†éš”ç¬¦æ‹†åˆ†
                sub_parts = self._split_text_by_punct(text)
                # ä¿æŒç›¸åŒçš„æºåºå·
                result.extend([(part, source_idx) for part in sub_parts])
        
        debug_print("ç¬¬2æ­¥-åˆ†éš”ç¬¦æ‹†åˆ†", [(seg[0], seg[1]) for seg in result])
        return result
    
    def _split_text_by_punct(self, text: str) -> List[str]:
        """
        æŒ‰åˆ†éš”ç¬¦æ‹†åˆ†æ–‡æœ¬ï¼ˆä¸å¤„ç†å¼•å·æ‹¬å·é€»è¾‘ï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æ‹†åˆ†åçš„æ–‡æœ¬ç‰‡æ®µåˆ—è¡¨
        """
        clauses = []
        buf = []
        
        i = 0
        while i < len(text):
            ch = text[i]
            buf.append(ch)
            
            # å¤„ç†åˆ†éš”ç¬¦
            if ch in SPLIT_PUNCT:
                # ç‰¹æ®Šå¤„ç†å¥å·ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºç¼©å†™è¯
                if ch == '.' and self._is_abbreviation(i, text):
                    pass  # ä¸æ‹†åˆ†ç¼©å†™è¯
                else:
                    clause = ''.join(buf).strip()
                    if clause and len(clause) > 1:  # é¿å…å•ä¸ªåˆ†éš”ç¬¦æˆä¸ºç‹¬ç«‹å­å¥
                        clauses.append(clause)
                        buf = []
            
            i += 1
        
        # æ”¶å°¾å¤„ç†
        if buf:
            clause = ''.join(buf).strip()
            if clause:
                clauses.append(clause)
        
        # æ¸…ç†å¹¶è¿‡æ»¤ç©ºå­å¥
        clauses = [clause.strip() for clause in clauses if clause.strip()]
        return clauses
    
    def _has_adjacent_same_source(self, segments: List[tuple[str, int]], current_index: int, source_idx: int) -> bool:
        """
        æ£€æŸ¥å½“å‰ä½ç½®æ˜¯å¦æœ‰ç›¸é‚»çš„ç›¸åŒæºåºå·ç‰‡æ®µ
        
        Args:
            segments: å®Œæ•´çš„ç‰‡æ®µåˆ—è¡¨
            current_index: å½“å‰ç‰‡æ®µåœ¨åˆ—è¡¨ä¸­çš„ä½ç½®
            source_idx: è¦æ£€æŸ¥çš„æºåºå·
            
        Returns:
            æ˜¯å¦å­˜åœ¨ç›¸é‚»çš„ç›¸åŒæºåºå·ç‰‡æ®µ
        """
        # æ£€æŸ¥ä¸‹ä¸€ä¸ªç‰‡æ®µæ˜¯å¦ä¸ºç›¸åŒåºå·
        if (current_index + 1 < len(segments) and 
            segments[current_index + 1][1] == source_idx):
            debug_print("é‚»è¿‘æ£€æŸ¥", f"åºå·{source_idx}åœ¨ä½ç½®{current_index}æœ‰åç»­ç›¸åŒåºå·ç‰‡æ®µ")
            return True
        
        # æ£€æŸ¥å‰ä¸€ä¸ªç‰‡æ®µæ˜¯å¦ä¸ºç›¸åŒåºå·
        # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦æ£€æŸ¥å‰ä¸€ä¸ªï¼Œå› ä¸ºå¦‚æœå‰ä¸€ä¸ªæ˜¯ç›¸åŒåºå·ï¼Œåœ¨å¤„ç†å‰ä¸€ä¸ªç‰‡æ®µæ—¶å°±ä¼šåˆå¹¶
        # è¿™é‡Œä¸»è¦å…³å¿ƒæ˜¯å¦æœ‰åç»­çš„ç›¸åŒåºå·ç‰‡æ®µéœ€è¦ç­‰å¾…
        
        return False
    
    def _merge_short_segments(self, segments: List[tuple[str, int]]) -> List[str]:
        """
        ç¬¬3æ­¥ï¼šæ™ºèƒ½åˆå¹¶çŸ­ç‰‡æ®µï¼Œä¼˜å…ˆä¸ç›¸åŒæºåºå·çš„ç‰‡æ®µåˆå¹¶
        
        Args:
            segments: ç¬¬2æ­¥çš„è¾“å‡º - (æ–‡æœ¬, æºåºå·) å…ƒç»„åˆ—è¡¨
            
        Returns:
            æœ€ç»ˆçš„å­å¥åˆ—è¡¨
        """
        merged = []
        merged_indices = []  # è¿½è¸ªå·²åˆå¹¶æ–‡æœ¬çš„æºåºå·
        
        for i, (text, source_idx) in enumerate(segments):
            # ä¼˜å…ˆçº§1ï¼šä¸ç›¸åŒæºåºå·çš„å‰ä¸€ç‰‡æ®µåˆå¹¶
            if (merged and merged_indices and merged_indices[-1] == source_idx and 
                self._can_merge_segments(merged[-1], text)):
                merged[-1] += " " + text
                debug_print("ç›¸åŒåºå·åˆå¹¶", f"åºå·{source_idx}: {merged[-1]}")
            
            # ä¼˜å…ˆçº§2ï¼šä¸ä¸åŒåºå·çš„å‰ä¸€ç‰‡æ®µåˆå¹¶
            # æ–°å¢æ¡ä»¶ï¼šåªæœ‰å½“å‰ç‰‡æ®µæ²¡æœ‰ç›¸é‚»çš„ç›¸åŒåºå·ç‰‡æ®µæ—¶æ‰å…è®¸
            elif (merged and self._can_merge_segments(merged[-1], text) and
                  not self._has_adjacent_same_source(segments, i, source_idx)):
                merged[-1] += " " + text
                merged_indices[-1] = source_idx  # æ›´æ–°åºå·ä¸ºæ–°ç‰‡æ®µçš„åºå·
                debug_print("è·¨åºå·åˆå¹¶", f"åºå·{merged_indices[-1]}: {merged[-1]}")
            
            else:
                # æ— æ³•åˆå¹¶ï¼Œæ·»åŠ ä¸ºæ–°ç‰‡æ®µ
                merged.append(text)
                merged_indices.append(source_idx)
                debug_print("ç‹¬ç«‹ç‰‡æ®µ", f"åºå·{source_idx}: {text}")
        
        debug_print("ç¬¬3æ­¥-æ™ºèƒ½åˆå¹¶", merged)
        return merged
    
    def _can_merge_segments(self, prev_text: str, current_text: str) -> bool:
        """
        æ£€æŸ¥ä¸¤ä¸ªç‰‡æ®µæ˜¯å¦å¯ä»¥åˆå¹¶
        
        Args:
            prev_text: å‰ä¸€ä¸ªç‰‡æ®µæ–‡æœ¬
            current_text: å½“å‰ç‰‡æ®µæ–‡æœ¬
            
        Returns:
            æ˜¯å¦å¯ä»¥åˆå¹¶
        """
        # åˆå¹¶æ¡ä»¶ï¼š
        # 1. å‰ä¸€ä¸ªç‰‡æ®µæˆ–å½“å‰ç‰‡æ®µè¿‡çŸ­
        # 2. åˆå¹¶åä¸è¶…è¿‡æœ€å¤§é•¿åº¦
        # 3. å‰ä¸€ä¸ªç‰‡æ®µä¸ä»¥å¥æœ«åˆ†éš”ç¬¦ç»“å°¾
        return (
            (len(prev_text) < MIN_MERGE_LENGTH or len(current_text) < MIN_MERGE_LENGTH) and 
            len(prev_text) + len(current_text) < MAX_MERGE_LENGTH and
            not self._ends_with_sentence_terminator(prev_text)
        )
    
    def _parse_text_into_clauses(self, text: str):
        """
        å°†æ–‡æœ¬æ‹†åˆ†æˆå­å¥çš„æ ¸å¿ƒé€»è¾‘:
        ä½¿ç”¨ç»Ÿä¸€çš„ç¬¦å·å¤„ç†å’ŒçŠ¶æ€è·Ÿè¸ª
        """
        clauses = []
        buf = []
        
        # ç»Ÿä¸€çš„ç¬¦å·çŠ¶æ€è·Ÿè¸ª
        quote_stack = []  # å¼•å·æ ˆï¼Œè®°å½•å½“å‰æ‰“å¼€çš„å¼•å·ç±»å‹
        paren_count = 0   # æ‹¬å·åµŒå¥—å±‚çº§
        
        i = 0
        while i < len(text):
            ch = text[i]
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¼•å·å­—ç¬¦
            quote_info = self._get_quote_type(ch)
            if quote_info:
                open_quote, close_quote = quote_info
                buf.append(ch)
                
                # åªåœ¨æ²¡æœ‰æ‹¬å·åµŒå¥—æ—¶å¤„ç†å¼•å·
                if paren_count == 0:
                    if ch == open_quote:
                        # æ£€æŸ¥æ˜¯å¦ä¸ºå¼€å§‹å¼•å·
                        if not quote_stack or quote_stack[-1] != (open_quote, close_quote):
                            # å¼€å§‹æ–°çš„å¼•å·åŒºåŸŸ
                            if buf[:-1]:  # å¦‚æœç¼“å†²åŒºæœ‰å†…å®¹ï¼ˆé™¤äº†åˆšåŠ å…¥çš„å¼•å·ï¼‰
                                clause = ''.join(buf[:-1]).strip()
                                if clause:
                                    clauses.append(clause)
                                buf = [ch]  # é‡æ–°å¼€å§‹ï¼Œåªä¿ç•™å¼•å·
                            quote_stack.append((open_quote, close_quote))
                    elif ch == close_quote:
                        # æ£€æŸ¥æ˜¯å¦ä¸ºç»“æŸå¼•å·
                        if quote_stack and quote_stack[-1] == (open_quote, close_quote):
                            # ç»“æŸå½“å‰å¼•å·åŒºåŸŸ
                            quote_stack.pop()
                            clause = ''.join(buf).strip()
                            if clause:
                                clauses.append(clause)
                            buf = []
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ‹¬å·å­—ç¬¦
            elif self._get_paren_type(ch):
                open_paren, close_paren = self._get_paren_type(ch)
                buf.append(ch)
                
                # åªåœ¨æ²¡æœ‰å¼•å·åµŒå¥—æ—¶è®¡ç®—æ‹¬å·å±‚çº§
                if not quote_stack:
                    if ch == open_paren:
                        paren_count += 1
                    elif ch == close_paren and paren_count > 0:
                        paren_count -= 1
            
            # å¤„ç†åˆ†éš”ç¬¦
            elif ch in SPLIT_PUNCT:
                buf.append(ch)
                
                # åªåœ¨æ²¡æœ‰å¼•å·æˆ–æ‹¬å·åµŒå¥—æ—¶è¿›è¡Œæ‹†åˆ†
                if not quote_stack and paren_count == 0:
                    # ç‰¹æ®Šå¤„ç†å¥å·ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºç¼©å†™è¯
                    if ch == '.' and self._is_abbreviation(i, text):
                        pass  # ä¸æ‹†åˆ†ç¼©å†™è¯
                    else:
                        clause = ''.join(buf).strip()
                        if clause and len(clause) > 1:  # é¿å…å•ä¸ªåˆ†éš”ç¬¦æˆä¸ºç‹¬ç«‹å­å¥
                            clauses.append(clause)
                            buf = []
            
            # æ™®é€šå­—ç¬¦
            else:
                buf.append(ch)
            
            i += 1
        
        # æ”¶å°¾å¤„ç†
        if buf:
            clause = ''.join(buf).strip()
            if clause:
                clauses.append(clause)
        
        # æ¸…ç†å¹¶è¿‡æ»¤ç©ºå­å¥
        clauses = [clause.strip() for clause in clauses if clause.strip()]
        debug_print("_parse_text_into_clausesè¾“å‡º", clauses)
        
        return clauses

    def split_into_clauses(self, text: str):
        """
        ä¸‰æ­¥ä¼˜åŒ–çš„å­å¥æ‹†åˆ†:
        1. æŒ‰æ‹¬å·å’Œå¼•å·æ‹†åˆ†ï¼Œè®°å½•åºå·
        2. æŒ‰åˆ†éš”ç¬¦æ‹†åˆ†é•¿ç‰‡æ®µï¼Œä¿æŒåºå·
        3. æ™ºèƒ½åˆå¹¶ï¼Œä¼˜å…ˆç›¸åŒåºå·ç‰‡æ®µ
        """
        debug_print("split_into_clausesè¾“å…¥", text)
        
        # ç¬¬1æ­¥ï¼šæŒ‰å¼•å·å’Œæ‹¬å·æ‹†åˆ†ï¼Œåˆ†é…åºå·
        segments = self._split_by_quotes_and_parens(text)
        
        # ç¬¬2æ­¥ï¼šæŒ‰åˆ†éš”ç¬¦æ‹†åˆ†é•¿ç‰‡æ®µ  
        segments = self._split_by_punctuation(segments)
        
        # ç¬¬3æ­¥ï¼šæ™ºèƒ½åˆå¹¶
        result = self._merge_short_segments(segments)
        
        # åç½®å¤„ç†: åˆå¹¶è¢«åˆ†ç¦»çš„æ ‡ç‚¹ç¬¦å·
        result = self._merge_split_punctuation(result)
        debug_print("split_into_clausesæœ€ç»ˆè¾“å‡º", result)
        
        return result

    def _ends_with_sentence_terminator(self, text: str) -> bool:
        """
        æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä»¥å¥æœ«åˆ†éš”ç¬¦ç»“å°¾(å»é™¤ç©ºæ ¼å)
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            True if æ–‡æœ¬ä»¥å¥æœ«åˆ†éš”ç¬¦ç»“å°¾
        """
        if not text:
            return False
            
        # å»é™¤å°¾éƒ¨ç©ºæ ¼åæ£€æŸ¥æœ€åä¸€ä¸ªå­—ç¬¦
        trimmed = text.rstrip()
        if not trimmed:
            return False
            
        return trimmed[-1] in SENTENCE_TERMINATORS

    def _merge_split_punctuation(self, clauses: List[str]) -> List[str]:
        """
        åç½®å¤„ç†: åˆå¹¶è¢«åˆ†ç¦»çš„åˆ†éš”ç¬¦å’Œæˆå¯¹ç¬¦å·
        ä¸¤ç§æƒ…å†µ:
        1. åˆ†éš”ç¬¦(éå†’å·) + æˆå¯¹ç¬¦å·ç»“æŸéƒ¨åˆ†
        2. æˆå¯¹ç¬¦å·ç»“æŸéƒ¨åˆ† + åˆ†éš”ç¬¦(éå†’å·)
        
        Args:
            clauses: åŸå§‹å­å¥åˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„å­å¥åˆ—è¡¨
        """
        if not clauses:
            return clauses
            
        merged = []
        
        for clause in clauses:
            if merged and self._should_merge_with_previous(merged[-1], clause):
                # åˆå¹¶ç¬¦å·
                clause = clause.lstrip()
                merged[-1] += clause[0]
                if len(clause) > 1:
                    merged.append(clause[1:].lstrip())
            else:
                merged.append(clause)
        
        return merged
    
    def _should_merge_with_previous(self, prev_clause: str, current_clause: str) -> bool:
        """
        æ£€æŸ¥å½“å‰å­å¥æ˜¯å¦åº”è¯¥ä¸å‰ä¸€ä¸ªå­å¥åˆå¹¶
        ä¸¤ç§æƒ…å†µ:
        1. åˆ†éš”ç¬¦(éå†’å·) + æˆå¯¹ç¬¦å·ç»“æŸéƒ¨åˆ†
        2. æˆå¯¹ç¬¦å·ç»“æŸéƒ¨åˆ† + åˆ†éš”ç¬¦(éå†’å·)
        
        Args:
            prev_clause: å‰ä¸€ä¸ªå­å¥
            current_clause: å½“å‰å­å¥
            
        Returns:
            True if åº”è¯¥åˆå¹¶
        """
        if not prev_clause or not current_clause:
            return False
            
        # æ£€æŸ¥å‰ä¸€å¥çš„ç»“å°¾å­—ç¬¦
        prev_trimmed = prev_clause.rstrip()
        if not prev_trimmed:
            return False
        prev_end_char = prev_trimmed[-1]
        
        # æ£€æŸ¥å½“å‰å¥çš„å¼€å¤´å­—ç¬¦
        current_trimmed = current_clause.lstrip()
        if not current_trimmed:
            return False
        current_start_char = current_trimmed[0]
        
        # æƒ…å†µ1: åˆ†éš”ç¬¦(éå†’å·) + æˆå¯¹ç¬¦å·ç»“æŸéƒ¨åˆ†
        case1 = (prev_end_char in PREV_MERGEABLE_SEPARATORS and 
                current_start_char in NEXT_SYMBOL_ENDINGS)
        
        # æƒ…å†µ2: æˆå¯¹ç¬¦å·ç»“æŸéƒ¨åˆ† + åˆ†éš”ç¬¦(éå†’å·)
        case2 = (prev_end_char in PREV_SYMBOL_ENDINGS and 
                current_start_char in NEXT_MERGEABLE_SEPARATORS)
        
        return case1 or case2

    def _is_quoted_or_parenthesized(self, text: str) -> bool:
        """
        æ£€æŸ¥æ–‡æœ¬æ˜¯å¦è¢«æ‹¬å·æˆ–å¼•å·åŒ…å›´
        """
        if len(text) < 2:
            return False
        
        # æ£€æŸ¥æ˜¯å¦è¢«æ‹¬å·åŒ…å›´
        for open_sym, close_sym in PAIR_SYMBOLS_PARENS:
            if text.startswith(open_sym) and text.endswith(close_sym):
                return True
        
        # æ£€æŸ¥æ˜¯å¦è¢«å¼•å·åŒ…å›´
        for open_sym, close_sym in PAIR_SYMBOLS_QUOTES:
            if text.startswith(open_sym) and text.endswith(close_sym):
                return True
        
        return False

    def _extract_inner_content_and_wrapper(self, text: str) -> tuple[str, tuple[str, str]]:
        """
        ä»è¢«åŒ…å›´çš„æ–‡æœ¬ä¸­æå–å†…éƒ¨å†…å®¹å’ŒåŒ…å›´ç¬¦å·
        
        Returns:
            (å†…éƒ¨å†…å®¹, (å¼€å§‹ç¬¦å·, ç»“æŸç¬¦å·))
        """
        if len(text) < 2:
            return text, ("", "")
        
        # æ£€æŸ¥æ‹¬å·
        for open_sym, close_sym in PAIR_SYMBOLS_PARENS:
            if text.startswith(open_sym) and text.endswith(close_sym):
                inner = text[len(open_sym):-len(close_sym)]
                return inner, (open_sym, close_sym)
        
        # æ£€æŸ¥å¼•å·
        for open_sym, close_sym in PAIR_SYMBOLS_QUOTES:
            if text.startswith(open_sym) and text.endswith(close_sym):
                inner = text[len(open_sym):-len(close_sym)]
                return inner, (open_sym, close_sym)
        
        return text, ("", "")


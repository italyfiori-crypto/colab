#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å•è¯æå–æ¨¡å—
ä»ç« èŠ‚æ–‡æœ¬ä¸­æå–å•è¯ï¼Œè¿›è¡Œè¯å½¢è¿˜åŸï¼Œè¿‡æ»¤å¤„ç†
"""

import os
import re
import json
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple
from dataclasses import dataclass
from collections import defaultdict
from ._vocabulary_enricher import load_master_vocabulary

# éœ€è¦å®‰è£…: pip install spacy
# ä¸‹è½½æ¨¡å‹: python -m spacy download en_core_web_sm
try:
    import spacy
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {e}")
    print("è¯·å®‰è£…: pip install spacy")
    print("å¹¶ä¸‹è½½æ¨¡å‹: python -m spacy download en_core_web_sm")
    raise


@dataclass
class WordExtractionConfig:
    """å•è¯æå–é…ç½®"""
    
    # è¿‡æ»¤é…ç½®
    min_word_length: int = 2
    max_word_length: int = 20
    filter_numbers: bool = True
    filter_proper_nouns: bool = True
    filter_stop_words: bool = True
    filter_names: bool = True
    
    # è¾“å‡ºé…ç½®
    vocabulary_subdir: str = "vocabulary"
    chapters_subdir: str = "chapters"
    
    # SpaCyæ¨¡å‹
    spacy_model: str = "en_core_web_sm"


class WordExtractor:
    """å•è¯æå–å™¨ - ä»æ–‡æœ¬ä¸­æå–å’Œæ ‡å‡†åŒ–å•è¯"""
    
    def __init__(self, config: WordExtractionConfig):
        """
        åˆå§‹åŒ–å•è¯æå–å™¨
        
        Args:
            config: æå–é…ç½®
        """
        self.config = config
        
        # åŠ è½½SpaCyæ¨¡å‹
        try:
            self.nlp = spacy.load(self.config.spacy_model)
            print(f"âœ… SpaCyæ¨¡å‹åŠ è½½æˆåŠŸ: {self.config.spacy_model}")
        except OSError:
            raise RuntimeError(f"SpaCyæ¨¡å‹åŠ è½½å¤±è´¥: {self.config.spacy_model}ï¼Œè¯·è¿è¡Œ: python -m spacy download {self.config.spacy_model}")
        
        # è·å–spaCyåœç”¨è¯
        self.stop_words = self.nlp.Defaults.stop_words
        
        print(f"ğŸ“ å•è¯æå–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    
    def extract_subchapter_words(self, sub_chapter_files: List[str], output_dir: str, master_vocab_path: str) -> Tuple[List[str], List[str]]:
        """
        ä»å¥å­æ–‡ä»¶ä¸­æå–å­ç« èŠ‚è¯æ±‡ï¼ˆä»¥å­ç« èŠ‚ä¸ºå•ä½ï¼‰
        
        Args:
            sub_chapter_files: å¥å­æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            master_vocab_path: æ€»è¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„
            
        Returns:
            (å¤„ç†çš„å­ç« èŠ‚è¯æ±‡æ–‡ä»¶åˆ—è¡¨, æ‰€æœ‰æ–°è¯åˆ—è¡¨)
        """
        # åŠ è½½å·²æœ‰çš„æ€»è¯æ±‡è¡¨
        existing_vocab = load_master_vocabulary(master_vocab_path)
        print(f"ğŸ“ åŠ è½½ç°æœ‰è¯æ±‡è¡¨: {len(existing_vocab)} ä¸ªå•è¯")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        vocab_dir = os.path.join(output_dir, self.config.vocabulary_subdir)
        os.makedirs(vocab_dir, exist_ok=True)
        
        subchapter_vocab_files = []
        all_new_words = set()
        
        # æŒ‰å­ç« èŠ‚å¤„ç†å¥å­æ–‡ä»¶ï¼ˆæ¯ä¸ªå¥å­æ–‡ä»¶å¯¹åº”ä¸€ä¸ªå­ç« èŠ‚ï¼‰
        for sub_chapter_file in sorted(sub_chapter_files):
            # è·å–å­ç« èŠ‚åç§°
            filename = os.path.basename(sub_chapter_file)
            subchapter_name = os.path.splitext(filename)[0]
            
            print(f"ğŸ“ å¤„ç†å­ç« èŠ‚: {subchapter_name}")
            
            # æå–å­ç« èŠ‚æ‰€æœ‰å•è¯
            all_words, filtered_words = self._extract_words_from_files([sub_chapter_file])
            
            # æ”¶é›†æ‰€æœ‰æå–çš„å•è¯ï¼ˆä¸åŒºåˆ†æ–°æ—§ï¼‰
            all_new_words.update(all_words)
            
            # ç¬¬ä¸€é˜¶æ®µï¼šä¿å­˜åŸå§‹å•è¯åˆ—è¡¨ï¼ˆä¸æ ¼å¼åŒ–ï¼‰ï¼Œä¿æŒåŸæ–‡é¡ºåº
            unique_words = self._preserve_order_dedup(all_words)  # ä¿åºå»é‡
            
            # ä¿å­˜å­ç« èŠ‚è¯æ±‡æ–‡ä»¶ï¼ˆç¬¬ä¸€é˜¶æ®µï¼šåªå«å•è¯åˆ—è¡¨ï¼‰
            subchapter_vocab_data = {
                "word_list": unique_words,  # ç¬¬ä¸€é˜¶æ®µï¼šçº¯å•è¯åˆ—è¡¨ï¼Œä¿æŒåŸæ–‡é¡ºåº
                "filtered_words": sorted(list(set(filtered_words)))
            }
            
            subchapter_vocab_file = os.path.join(vocab_dir, f"{subchapter_name}.json")
            self._save_json(subchapter_vocab_data, subchapter_vocab_file)
            subchapter_vocab_files.append(subchapter_vocab_file)
            
            print(f"  ğŸ“„ å·²ä¿å­˜å­ç« èŠ‚è¯æ±‡: {subchapter_name}")
            print(f"  ğŸ“ˆ è¯æ±‡ç»Ÿè®¡: æ€»è®¡{len(set(all_words))}ä¸ª")
        
        print(f"\nğŸ“ å­ç« èŠ‚è¯æ±‡æå–å®Œæˆï¼Œå…±æå– {len(all_new_words)} ä¸ªå•è¯")
        return subchapter_vocab_files, list(all_new_words)
    
    def _extract_words_from_files(self, sub_chapter_files: List[str]) -> Tuple[List[str], List[str]]:
        """
        ä»æ–‡ä»¶åˆ—è¡¨ä¸­æå–å•è¯
        
        Returns:
            (æœ‰æ•ˆå•è¯åˆ—è¡¨, è¢«è¿‡æ»¤å•è¯åˆ—è¡¨)
        """
        all_text = ""
        
        # è¯»å–æ‰€æœ‰æ–‡ä»¶å†…å®¹
        for sub_chapter_file in sub_chapter_files:
            try:
                with open(sub_chapter_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # è·³è¿‡ç¬¬ä¸€è¡Œæ ‡é¢˜
                    lines = content.strip().split('\n')
                    if len(lines) > 1:
                        all_text += " " + " ".join(lines[1:])
            except Exception as e:
                print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥: {sub_chapter_file}, {e}")
                continue
        
        return self._extract_words_from_text(all_text)
    
    def _preserve_order_dedup(self, items: List[str]) -> List[str]:
        """
        ä¿åºå»é‡å‡½æ•° - ä¿æŒå…ƒç´ é¦–æ¬¡å‡ºç°çš„é¡ºåº
        
        Args:
            items: åŸå§‹åˆ—è¡¨
            
        Returns:
            å»é‡åä¿æŒåŸå§‹é¡ºåºçš„åˆ—è¡¨
        """
        seen = set()
        result = []
        for item in items:
            if item not in seen:
                seen.add(item)
                result.append(item)
        return result
    
    def _extract_words_from_text(self, text: str) -> Tuple[List[str], List[str]]:
        """
        ä»æ–‡æœ¬ä¸­æå–å•è¯å¹¶è¿›è¡Œè¯å½¢è¿˜åŸå’Œè¿‡æ»¤
        
        Returns:
            (æœ‰æ•ˆå•è¯åˆ—è¡¨, è¢«è¿‡æ»¤å•è¯åˆ—è¡¨)
        """
        if not text.strip():
            return [], []
        
        # ä½¿ç”¨SpaCyå¤„ç†æ–‡æœ¬
        doc = self.nlp(text)
        
        valid_words = []
        filtered_words = []
        
        for token in doc:
            original = token.text.lower().strip()
            
            # åŸºç¡€è¿‡æ»¤
            if not original or not original.isalpha():
                filtered_words.append(original)
                continue
            
            # é•¿åº¦è¿‡æ»¤
            if len(original) < self.config.min_word_length or len(original) > self.config.max_word_length:
                filtered_words.append(original)
                continue
            
            valid_words.append(original)
        
        return valid_words, filtered_words
    
    def _format_words_with_info(self, words: List[str], vocab_dict: Dict[str, Dict]) -> List[str]:
        """
        å°†å•è¯åˆ—è¡¨æ ¼å¼åŒ–ä¸ºåŒ…å«è¯æ±‡ä¿¡æ¯çš„é€—å·åˆ†éš”æ ¼å¼
        
        Args:
            words: å•è¯åˆ—è¡¨
            vocab_dict: æ€»è¯æ±‡è¡¨å­—å…¸
            
        Returns:
            æ ¼å¼åŒ–åçš„å•è¯ä¿¡æ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º: word,tags,frq,collins,oxford
        """
        formatted_words = []
        unique_words = self._preserve_order_dedup(words)  # ä¿åºå»é‡
        
        for word in unique_words:
            word_info = vocab_dict.get(word, {})
            
            # æå–è¯æ±‡ä¿¡æ¯ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
            tags = ""
            if 'tags' in word_info:
                if isinstance(word_info['tags'], list):
                    tags = " ".join(word_info['tags'])
                elif isinstance(word_info['tags'], str):
                    tags = word_info['tags']
            
            frq = word_info.get('frq', 0)
            collins = word_info.get('collins', 0)
            oxford = word_info.get('oxford', 0)
            
            # æ ¼å¼åŒ–ä¸ºé€—å·åˆ†éš”å­—ç¬¦ä¸²: word,tags,frq,collins,oxford
            formatted_word = f"{word},{tags},{frq},{collins},{oxford}"
            formatted_words.append(formatted_word)
        
        # ä¿æŒåŸæ–‡ä¸­å‡ºç°çš„é¡ºåºï¼Œä¸è¿›è¡Œæ’åº
        return formatted_words
    
    def update_vocabulary_info(self, output_dir: str, master_vocab_path: str) -> bool:
        """
        ç¬¬äºŒé˜¶æ®µï¼šæ›´æ–°ç« èŠ‚è¯æ±‡æ–‡ä»¶ï¼Œå°†å•è¯åˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”ä¿¡æ¯æ ¼å¼
        
        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼ˆåŒ…å«vocabularyå­ç›®å½•ï¼‰
            master_vocab_path: æ€»è¯æ±‡è¡¨æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        try:
            # åŠ è½½æ€»è¯æ±‡è¡¨
            existing_vocab = load_master_vocabulary(master_vocab_path)
            if not existing_vocab:
                print("âš ï¸ æ€»è¯æ±‡è¡¨ä¸ºç©ºï¼Œæ— æ³•æ›´æ–°è¯æ±‡ä¿¡æ¯")
                return False
            
            # æŸ¥æ‰¾æ‰€æœ‰ç« èŠ‚è¯æ±‡æ–‡ä»¶
            vocab_dir = os.path.join(output_dir, self.config.vocabulary_subdir)
            if not os.path.exists(vocab_dir):
                print("âš ï¸ ç« èŠ‚è¯æ±‡ç›®å½•ä¸å­˜åœ¨")
                return False
            
            vocab_files = []
            for file in os.listdir(vocab_dir):
                if file.endswith('.json'):
                    vocab_files.append(os.path.join(vocab_dir, file))
            
            if not vocab_files:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç« èŠ‚è¯æ±‡æ–‡ä»¶")
                return False
            
            updated_count = 0
            
            print(f"ğŸ“ å¼€å§‹æ›´æ–° {len(vocab_files)} ä¸ªç« èŠ‚è¯æ±‡æ–‡ä»¶...")
            
            # å¤„ç†æ¯ä¸ªç« èŠ‚è¯æ±‡æ–‡ä»¶
            for vocab_file in vocab_files:
                if self._update_single_vocab_file(vocab_file, existing_vocab):
                    updated_count += 1
                    filename = os.path.basename(vocab_file)
                    print(f"  âœ… å·²æ›´æ–°: {filename}")
            
            print(f"\nğŸ“ è¯æ±‡ä¿¡æ¯æ›´æ–°å®Œæˆ: æˆåŠŸæ›´æ–° {updated_count}/{len(vocab_files)} ä¸ªæ–‡ä»¶")
            return updated_count > 0
            
        except Exception as e:
            print(f"âŒ æ›´æ–°è¯æ±‡ä¿¡æ¯å¤±è´¥: {e}")
            return False
    
    def _update_single_vocab_file(self, vocab_file: str, vocab_dict: Dict[str, Dict]) -> bool:
        """
        æ›´æ–°å•ä¸ªç« èŠ‚è¯æ±‡æ–‡ä»¶
        
        Args:
            vocab_file: ç« èŠ‚è¯æ±‡æ–‡ä»¶è·¯å¾„
            vocab_dict: æ€»è¯æ±‡è¡¨å­—å…¸
            
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        try:
            # è¯»å–ç°æœ‰æ–‡ä»¶
            with open(vocab_file, 'r', encoding='utf-8') as f:
                vocab_data = json.load(f)
            
            words = vocab_data.get('word_list', [])
            if not words:
                return False
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯æ ¼å¼åŒ–æ ¼å¼
            if isinstance(words[0], str) and ',' in words[0] and len(words[0].split(',')) == 5:
                print(f"  â­• å·²æ˜¯æ ¼å¼åŒ–æ ¼å¼ï¼Œè·³è¿‡: {os.path.basename(vocab_file)}")
                return False
            
            # æ ¼å¼åŒ–å•è¯ä¿¡æ¯
            formatted_words = self._format_words_with_info(words, vocab_dict)
            
            # æ›´æ–°æ•°æ®
            vocab_data['word_info_list'] = formatted_words
            
            # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
            self._save_json(vocab_data, vocab_file)
            return True
            
        except Exception as e:
            print(f"âŒ æ›´æ–°æ–‡ä»¶å¤±è´¥ {vocab_file}: {e}")
            return False
    
    def _save_json(self, data: dict, file_path: str):
        """ä¿å­˜JSONæ•°æ®åˆ°æ–‡ä»¶"""
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
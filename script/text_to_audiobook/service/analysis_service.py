#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†ææœåŠ¡ - ä¸“é—¨å¤„ç†å­—å¹•è¯­è¨€å­¦åˆ†æ
ç»Ÿè®¡åŠŸèƒ½å·²ç‹¬ç«‹åˆ°StatisticsService
"""

import os
import json
import re
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from infra import AIClient, FileManager
from infra.config_loader import AppConfig
from util import OUTPUT_DIRECTORIES, parse_jsonl_subtitle_file, write_jsonl_subtitle_file


class AnalysisService:
    """å­—å¹•è¯­è¨€å­¦åˆ†ææœåŠ¡"""
    
    def __init__(self, config: AppConfig):
        """
        åˆå§‹åŒ–åˆ†ææœåŠ¡
        
        Args:
            config: åº”ç”¨é…ç½®
        """
        self.config = config
        self.ai_client = AIClient(config.api)
        self.file_manager = FileManager()
    
    def analyze_subtitle_files(self, subtitle_files: List[str], output_dir: str) -> List[str]:
        """
        æ‰¹é‡åˆ†æå­—å¹•æ–‡ä»¶ï¼Œé€æ–‡ä»¶å¢é‡å¤„ç†
        
        Args:
            subtitle_files: å­—å¹•æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºæ ¹ç›®å½•
            
        Returns:
            æˆåŠŸåˆ†æçš„æ–‡ä»¶åˆ—è¡¨
        """
        if not subtitle_files:
            print("âš ï¸ æœªæ‰¾åˆ°å­—å¹•æ–‡ä»¶ï¼Œè·³è¿‡åˆ†æ")
            return []
        
        # åˆ›å»ºåˆ†æç»“æœç›®å½•
        analysis_dir = os.path.join(output_dir, OUTPUT_DIRECTORIES['analysis'])
        self.file_manager.create_directory(analysis_dir)
        
        print(f"ğŸ” å¼€å§‹é€æ–‡ä»¶åˆ†æ {len(subtitle_files)} ä¸ªå­—å¹•æ–‡ä»¶...")
        
        # æ€»ä½“ç»Ÿè®¡
        analyzed_files = []
        total_stats = {
            'entries_processed': 0,
            'entries_failed': 0,
            'files_completed': 0,
            'files_failed': 0,
            'files_skipped': 0
        }
        
        # é€ä¸ªæ–‡ä»¶å¤„ç†
        for file_idx, subtitle_file in enumerate(subtitle_files, 1):
            print(f"\nğŸ“„ [{file_idx}/{len(subtitle_files)}] å¤„ç†æ–‡ä»¶: {os.path.basename(subtitle_file)}")
            
            try:
                file_stats = self._process_single_file(subtitle_file, analysis_dir)
                
                if file_stats['success']:
                    analyzed_files.append(subtitle_file)
                    total_stats['files_completed'] += 1
                else:
                    total_stats['files_failed'] += 1
                    
                # ç´¯è®¡æ¡ç›®ç»Ÿè®¡
                total_stats['entries_processed'] += file_stats['entries_processed']
                total_stats['entries_failed'] += file_stats['entries_failed']
                total_stats['files_skipped'] += file_stats.get('skipped', 0)
                
            except Exception as e:
                print(f"    âŒ å¤„ç†æ–‡ä»¶å¼‚å¸¸: {e}")
                total_stats['files_failed'] += 1
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        print(f"\nğŸ“Š åˆ†æå®Œæˆç»Ÿè®¡:")
        print(f"   ğŸ“ æˆåŠŸæ–‡ä»¶: {total_stats['files_completed']}/{len(subtitle_files)}")
        print(f"   ğŸ“„ å¤„ç†å­—å¹•æ¡ç›®: {total_stats['entries_processed']}")
        print(f"   âŒ å¤±è´¥å­—å¹•æ¡ç›®: {total_stats['entries_failed']}")
        if total_stats['files_skipped'] > 0:
            print(f"   â­ï¸  è·³è¿‡å·²å¤„ç†: {total_stats['files_skipped']} ä¸ªæ–‡ä»¶")
        
        return analyzed_files
    
    def _process_single_file(self, subtitle_file: str, analysis_dir: str) -> Dict:
        """
        å¤„ç†å•ä¸ªå­—å¹•æ–‡ä»¶
        
        Args:
            subtitle_file: å­—å¹•æ–‡ä»¶è·¯å¾„
            analysis_dir: åˆ†æç»“æœç›®å½•
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡å­—å…¸
        """
        file_stats = {
            'success': False,
            'entries_processed': 0,
            'entries_failed': 0,
            'skipped': 0
        }
        
        try:
            # 1. è§£æå­—å¹•æ–‡ä»¶
            subtitle_entries = self._parse_jsonl_subtitle_file(subtitle_file)
            if not subtitle_entries:
                print(f"    âš ï¸ æ–‡ä»¶æ— æœ‰æ•ˆå­—å¹•ï¼Œè·³è¿‡")
                return file_stats
            
            # 2. åŠ è½½å·²æœ‰åˆ†æç»“æœ
            subtitle_name = self.file_manager.get_basename_without_extension(subtitle_file)
            json_file = os.path.join(analysis_dir, f"{subtitle_name}.jsonl")
            existing_results = self._load_existing_results(json_file)
            
            # 3. è¯†åˆ«éœ€è¦å¤„ç†çš„å­—å¹•æ¡ç›®
            missing_subtitles = self._get_missing_subtitles(subtitle_entries, existing_results)
            
            if not missing_subtitles:
                print(f"    âœ… æ‰€æœ‰å­—å¹•å·²å¤„ç†å®Œæ¯•ï¼Œè·³è¿‡ ({len(subtitle_entries)} æ¡)")
                file_stats['skipped'] = 1
                file_stats['success'] = True
                return file_stats
            
            print(f"    ğŸ” éœ€è¦å¤„ç† {len(missing_subtitles)}/{len(subtitle_entries)} æ¡å­—å¹•")
            
            # 4. å¹¶å‘åˆ†æç¼ºå¤±çš„å­—å¹•æ¡ç›®
            new_results = []
            max_workers = min(len(missing_subtitles), self.config.api.max_concurrent_workers)
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤åˆ†æä»»åŠ¡
                future_to_entry = {
                    executor.submit(self._analyze_subtitle_entry, entry): entry
                    for entry in missing_subtitles
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_entry):
                    entry = future_to_entry[future]
                    try:
                        result = future.result()
                        if result:
                            new_results.append(result)
                            file_stats['entries_processed'] += 1
                        else:
                            file_stats['entries_failed'] += 1
                            
                        # æ˜¾ç¤ºè¿›åº¦
                        completed = file_stats['entries_processed'] + file_stats['entries_failed']
                        if completed % 5 == 0 or completed == len(missing_subtitles):
                            print(f"      ğŸ“Š è¿›åº¦: {completed}/{len(missing_subtitles)} æ¡")
                            
                    except Exception as e:
                        print(f"      âŒ åˆ†æå­—å¹• {entry.get('index', '?')} å‡ºé”™: {e}")
                        file_stats['entries_failed'] += 1
            
            # 5. ä¿å­˜åˆå¹¶ç»“æœ
            if new_results or existing_results:
                # ä¿å­˜åˆ†æç»“æœ
                analysis_success = self._save_analysis_results(new_results, existing_results, subtitle_file, analysis_dir)
                
                # æ›´æ–°å­—å¹•æ–‡ä»¶ï¼Œæ·»åŠ has_analysiså­—æ®µ
                subtitle_success = self._update_subtitle_with_analysis_flag(subtitle_file, new_results, existing_results)
                
                if analysis_success and subtitle_success:
                    file_stats['success'] = True
                    print(f"    âœ… æ–‡ä»¶å¤„ç†å®Œæˆ")
                else:
                    print(f"    âŒ ä¿å­˜å¤±è´¥")
            else:
                print(f"    âš ï¸ æ— æœ‰æ•ˆåˆ†æç»“æœ")
                
        except Exception as e:
            print(f"    âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
            
        return file_stats
    
    
    def _analyze_subtitle_entry(self, entry: Dict) -> Optional[Dict]:
        """
        åˆ†æå•ä¸ªå­—å¹•æ¡ç›®çš„è¯­è¨€å­¦ç‰¹å¾
        
        Args:
            entry: å­—å¹•æ¡ç›®
            
        Returns:
            åˆ†æç»“æœæˆ–None
        """
        try:
            english_text = entry['english_text']
            chinese_text = entry.get('chinese_text', '')
            
            system_prompt = """ä½œä¸ºè‹±è¯­è¯­è¨€å­¦å®¶ï¼Œè¯·åˆ†æç”¨æˆ·è¾“å…¥çš„è‹±è¯­å¥å­ï¼Œä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ‡è®°æ ¼å¼è¾“å‡ºè¯­è¨€å­¦åˆ†æã€‚

ä½¿ç”¨ä»¥ä¸‹æ ‡è®°åˆ†éš”å„ä¸ªæ®µè½ï¼Œæ¯ä¸ªæ ‡è®°ç‹¬å ä¸€è¡Œï¼Œå†…å®¹ç´§éšå…¶åï¼š

[SENTENCE_STRUCTURE]
ä¸¥æ ¼æŒ‰å¥å­ä¸­è¯è¯­å‡ºç°çš„é¡ºåºåˆ†æå¥æ³•æˆåˆ†ï¼Œä¸è¦é—æ¼ä»»ä½•å†…å®¹ï¼Œä¹Ÿä¸è¦å¢åŠ åŸå¥ä¸­æ²¡æœ‰çš„å†…å®¹

[STRUCTURE_EXPLANATION]  
è§£é‡Šå¥å­ä¸­çš„é‡è¦è¯­æ³•ç°è±¡ï¼ˆæ—¶æ€ã€è¯­æ€ã€ç‰¹æ®Šå¥å¼ã€ä»å¥ç­‰ï¼‰

[KEY_WORDS]
æå–æœ€æœ‰æ„ä¹‰çš„æ ¸å¿ƒè¯æ±‡ï¼Œå¿½ç•¥theã€aã€anã€isã€wasã€Mrs.ã€Mr.ç­‰å¸¸è§è¯ï¼Œæœ€å¤šä¸è¶…è¿‡5ä¸ª
æ ¼å¼ï¼šå•è¯|è¯æ€§|å«ä¹‰|éŸ³æ ‡

[FIXED_PHRASES]
æå–çœŸæ­£ç°å®çš„å›ºå®šæ­é…ï¼Œä¸æ˜¯ä»»æ„çŸ­è¯­éƒ½ä½œä¸ºå›ºå®šæ­é…ï¼Œæœ€å¤šä¸è¶…è¿‡3ä¸ª
æ ¼å¼ï¼šçŸ­è¯­|å«ä¹‰

[COLLOQUIAL_EXPRESSION]
å°†ä¹¦é¢æˆ–ä¸å¸¸ç”¨è¡¨è¾¾ç”¨å£è¯­è¡¨è¾¾æ›¿ä»£ï¼Œå¦‚æœæœ¬èº«å¾ˆç®€å•æˆ–å·²ç»æ˜¯å£è¯­åŒ–è¡¨è¾¾åˆ™ç•™ç©º
æ ¼å¼ï¼šæ­£å¼è¡¨è¾¾|å£è¯­è¡¨è¾¾|ç”¨æ³•è¯´æ˜

ç¤ºä¾‹1ï¼ˆå¤æ‚å¥å­ï¼‰:
è¾“å…¥: "The project that we've been working on for months will be completed soon."
è¾“å‡º:
[SENTENCE_STRUCTURE]
ä¸»è¯­(The project) + å®šè¯­ä»å¥(that we've been working on for months) + è°“è¯­(will be completed) + æ—¶é—´çŠ¶è¯­(soon)

[STRUCTURE_EXPLANATION]
å®šè¯­ä»å¥ä¿®é¥°ä¸»è¯­projectï¼Œthatå¼•å¯¼é™åˆ¶æ€§å®šè¯­ä»å¥ï¼›ä¸»å¥ä½¿ç”¨ä¸€èˆ¬å°†æ¥æ—¶çš„è¢«åŠ¨è¯­æ€

[KEY_WORDS]
project|n.|é¡¹ç›®|/ËˆprÉ‘ËdÊ’ekt/
working|v.|å·¥ä½œï¼Œä»äº‹|/ËˆwÉœËrkÉªÅ‹/
completed|v.|å®Œæˆ|/kÉ™mËˆpliËtÉªd/

[FIXED_PHRASES]
work on|ä»äº‹ï¼Œè‡´åŠ›äº

[COLLOQUIAL_EXPRESSION]
will be completed soon|will be done soon|å£è¯­ä¸­ç”¨doneæ›¿ä»£completedæ›´ç®€æ´

ç¤ºä¾‹2ï¼ˆç®€å•å¥å­ï¼‰:
è¾“å…¥: "I see a red bird."
è¾“å‡º:
[SENTENCE_STRUCTURE]
ä¸»è¯­(I) + è°“è¯­(see) + å®¾è¯­(a red bird)

[STRUCTURE_EXPLANATION]
ç®€å•çš„ä¸»è°“å®¾ç»“æ„ï¼Œä¸€èˆ¬ç°åœ¨æ—¶

[KEY_WORDS]
bird|n.|é¸Ÿ|/bÉœËrd/

[FIXED_PHRASES]

[COLLOQUIAL_EXPRESSION]

æ³¨æ„äº‹é¡¹:
1. ä¸¥æ ¼æŒ‰æ ‡è®°æ ¼å¼è¾“å‡ºï¼Œæ¯ä¸ªæ ‡è®°å¿…é¡»ç‹¬å ä¸€è¡Œ
2. å¥å­ç»“æ„åˆ†æå¿…é¡»æŒ‰è¯è¯­åœ¨å¥å­ä¸­çš„å®é™…é¡ºåº
3. å…³é”®è¯æœ€å¤š5ä¸ªï¼Œå›ºå®šæ­é…æœ€å¤š3ä¸ª
4. æ— ç›¸å…³å†…å®¹æ—¶è¯¥æ®µè½ç•™ç©ºï¼Œä½†æ ‡è®°å¿…é¡»ä¿ç•™
5. ç®€å•æˆ–å·²ç»å£è¯­åŒ–çš„è¡¨è¾¾ä¸éœ€è¦å£è¯­è½¬æ¢"""
            
            user_prompt = f"è¯·åˆ†æä»¥ä¸‹è‹±è¯­å¥å­: \"{english_text}\""""
            
            # ä½¿ç”¨AIClientçš„chat_completionæ–¹æ³•
            response = self.ai_client.chat_completion(user_prompt, system_prompt, temperature=0.2, max_tokens=5000)
            if not response:
                return None
            
            # è§£æç»“æ„åŒ–å“åº”
            try:
                # ä½¿ç”¨æ–°çš„ç»“æ„åŒ–è§£ææ–¹æ³•
                analysis_data = self._parse_structured_response(response)
                if not analysis_data:
                    print(f"âš ï¸ æ— æ³•è§£æå“åº”å†…å®¹: {response[:100]}...")
                    return None
                
                result = {
                    "subtitle_index": entry['index'],
                    "timestamp": entry['timestamp'],
                    "english_text": english_text,
                    "chinese_text": chinese_text,
                    "sentence_structure": analysis_data.get("sentence_structure", ""),
                    "key_words": analysis_data.get("key_words", []),
                    "fixed_phrases": analysis_data.get("fixed_phrases", []),
                    "structure_explanation": analysis_data.get("structure_explanation", ""),
                    "colloquial_expression": analysis_data.get("colloquial_expression", []),
                    "cultural_context": analysis_data.get("cultural_context", [])
                }
                
                return result
                
            except Exception as e:
                print(f"âš ï¸ å“åº”è§£æå¤±è´¥: {e}")
                print(f"âš ï¸ åŸå§‹å“åº”: {response[:200]}...")
                return None
                
        except Exception as e:
            print(f"âŒ åˆ†æå­—å¹•æ¡ç›®å¤±è´¥: {e}")
            return None
    
    def _parse_structured_response(self, response: str) -> Dict:
        """
        ä»AIå“åº”ä¸­è§£æç»“æ„åŒ–æ ‡è®°å†…å®¹
        
        Args:
            response: AIè¿”å›çš„åŸå§‹å“åº”
            
        Returns:
            è§£æçš„ç»“æ„åŒ–æ•°æ®å­—å…¸
        """
        if not response:
            return {}
        
        # åˆå§‹åŒ–ç»“æœ
        result = {
            "sentence_structure": "",
            "structure_explanation": "",
            "key_words": [],
            "fixed_phrases": [],
            "colloquial_expression": [],
            "cultural_context": []
        }
        
        try:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å„ä¸ªæ®µè½
            
            # æå–å¥å­ç»“æ„
            structure_match = re.search(r'\[SENTENCE_STRUCTURE\]\s*(.*?)(?=\[|$)', response, re.DOTALL)
            if structure_match:
                result["sentence_structure"] = structure_match.group(1).strip()
            
            # æå–ç»“æ„è§£é‡Š
            explanation_match = re.search(r'\[STRUCTURE_EXPLANATION\]\s*(.*?)(?=\[|$)', response, re.DOTALL)
            if explanation_match:
                result["structure_explanation"] = explanation_match.group(1).strip()
            
            # æå–å…³é”®è¯
            keywords_match = re.search(r'\[KEY_WORDS\]\s*(.*?)(?=\[|$)', response, re.DOTALL)
            if keywords_match:
                keywords_text = keywords_match.group(1).strip()
                for line in keywords_text.split('\n'):
                    line = line.strip()
                    if line and '|' in line:
                        parts = line.split('|')
                        if len(parts) >= 3:
                            word_dict = {
                                "word": parts[0].strip(),
                                "pos": parts[1].strip(),
                                "meaning": parts[2].strip()
                            }
                            # å¯é€‰çš„éŸ³æ ‡
                            if len(parts) >= 4:
                                word_dict["pronunciation"] = parts[3].strip()
                            result["key_words"].append(word_dict)
            
            # æå–å›ºå®šçŸ­è¯­
            phrases_match = re.search(r'\[FIXED_PHRASES\]\s*(.*?)(?=\[|$)', response, re.DOTALL)
            if phrases_match:
                phrases_text = phrases_match.group(1).strip()
                for line in phrases_text.split('\n'):
                    line = line.strip()
                    if line and '|' in line:
                        parts = line.split('|')
                        if len(parts) >= 2:
                            phrase_dict = {
                                "phrase": parts[0].strip(),
                                "meaning": parts[1].strip()
                            }
                            result["fixed_phrases"].append(phrase_dict)
            
            # æå–å£è¯­è¡¨è¾¾
            colloquial_match = re.search(r'\[COLLOQUIAL_EXPRESSION\]\s*(.*?)(?=\[|$)', response, re.DOTALL)
            if colloquial_match:
                colloquial_text = colloquial_match.group(1).strip()
                for line in colloquial_text.split('\n'):
                    line = line.strip()
                    if line and '|' in line:
                        parts = line.split('|')
                        if len(parts) >= 3:
                            expression_dict = {
                                "formal": parts[0].strip(),
                                "informal": parts[1].strip(),
                                "explanation": parts[2].strip()
                            }
                            result["colloquial_expression"].append(expression_dict)
            
            return result
            
        except Exception as e:
            print(f"âš ï¸ è§£æç»“æ„åŒ–å“åº”å¤±è´¥: {e}")
            return result
    
    def _save_analysis_results(self, new_results: List[Dict], existing_results: Dict[str, Dict], 
                               subtitle_file: str, analysis_dir: str) -> bool:
        """
        åˆå¹¶æ–°æ—§åˆ†æç»“æœå¹¶ä¿å­˜åˆ°JSONLæ–‡ä»¶
        
        Args:
            new_results: æ–°åˆ†æçš„ç»“æœåˆ—è¡¨
            existing_results: å·²å­˜åœ¨çš„ç»“æœå­—å…¸
            subtitle_file: å­—å¹•æ–‡ä»¶è·¯å¾„
            analysis_dir: åˆ†æç»“æœç›®å½•
            
        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            subtitle_name = self.file_manager.get_basename_without_extension(subtitle_file)
            json_file = os.path.join(analysis_dir, f"{subtitle_name}.jsonl")
            
            # åˆå¹¶æ–°æ—§ç»“æœ
            all_results = dict(existing_results)  # å¤åˆ¶å·²æœ‰ç»“æœ
            
            # æ·»åŠ æ–°ç»“æœï¼Œè¦†ç›–å·²æœ‰çš„ç›¸åŒåºå·
            for result in new_results:
                subtitle_index = str(result.get('subtitle_index', ''))
                if subtitle_index:
                    # ä¸ºæ¯ä¸ªå­—å¹•æ¡ç›®æ·»åŠ æºæ–‡ä»¶ä¿¡æ¯
                    result_with_source = {
                        "source_file": os.path.basename(subtitle_file),
                        **result
                    }
                    all_results[subtitle_index] = result_with_source
            
            # æŒ‰å­—å¹•åºå·æ’åºï¼ˆæ•°å€¼æ’åºï¼‰
            sorted_results = sorted(all_results.values(), 
                                  key=lambda x: int(x.get('subtitle_index', '0')) if str(x.get('subtitle_index', '0')).isdigit() else 0)
            
            # ä¿å­˜ä¸ºJSONLæ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªå­—å¹•æ¡ç›®çš„è§£æJSON
            with open(json_file, 'w', encoding='utf-8') as f:
                for result in sorted_results:
                    f.write(json.dumps(result, ensure_ascii=False, separators=(',', ':')) + '\n')
            
            total_count = len(sorted_results)
            new_count = len(new_results)
            print(f"    âœ… åˆ†æç»“æœå·²ä¿å­˜: {json_file} (æ–°å¢{new_count}æ¡ï¼Œæ€»è®¡{total_count}æ¡)")
            return True
            
        except Exception as e:
            print(f"    âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")
            return False
    
    
    def _load_existing_results(self, json_file: str) -> Dict[str, Dict]:
        """
        åŠ è½½å·²å­˜åœ¨çš„åˆ†æç»“æœ
        
        Args:
            json_file: JSONæ–‡ä»¶è·¯å¾„
            
        Returns:
            å·²å­˜åœ¨çš„åˆ†æç»“æœå­—å…¸ï¼Œé”®ä¸ºsubtitle_indexï¼Œå€¼ä¸ºåˆ†æç»“æœ
        """
        existing_results = {}
        
        if not os.path.exists(json_file):
            return existing_results
            
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                        
                    try:
                        result = json.loads(line)
                        subtitle_index = str(result.get('subtitle_index', ''))
                        if subtitle_index:
                            existing_results[subtitle_index] = result
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ è§£æJSONè¡Œ {line_num} å¤±è´¥: {e}")
                        continue
                        
        except Exception as e:
            print(f"âš ï¸ è¯»å–å·²æœ‰ç»“æœæ–‡ä»¶å¤±è´¥ {json_file}: {e}")
            
        return existing_results
    
    def _get_missing_subtitles(self, all_subtitles: List[Dict], existing_results: Dict[str, Dict]) -> List[Dict]:
        """
        è¯†åˆ«éœ€è¦å¤„ç†çš„å­—å¹•æ¡ç›®
        
        Args:
            all_subtitles: æ‰€æœ‰å­—å¹•æ¡ç›®åˆ—è¡¨
            existing_results: å·²å­˜åœ¨çš„åˆ†æç»“æœå­—å…¸
            
        Returns:
            éœ€è¦å¤„ç†çš„å­—å¹•æ¡ç›®åˆ—è¡¨
        """
        missing_subtitles = []
        
        for subtitle in all_subtitles:
            subtitle_index = str(subtitle.get('index', ''))
            if subtitle_index not in existing_results:
                missing_subtitles.append(subtitle)
                
        return missing_subtitles
    
    def _parse_jsonl_subtitle_file(self, subtitle_file: str) -> List[Dict]:
        """è§£æJSONLå­—å¹•æ–‡ä»¶ - ä½¿ç”¨ç»Ÿä¸€çš„è§£æå·¥å…·"""
        return parse_jsonl_subtitle_file(subtitle_file)
    
    def _update_subtitle_with_analysis_flag(self, subtitle_file: str, new_results: List[Dict], existing_results: Dict[str, Dict]) -> bool:
        """
        æ›´æ–°å­—å¹•æ–‡ä»¶ï¼Œä¸ºå·²åˆ†æçš„æ¡ç›®æ·»åŠ has_analysiså­—æ®µ
        
        Args:
            subtitle_file: å­—å¹•æ–‡ä»¶è·¯å¾„
            new_results: æ–°åˆ†æçš„ç»“æœåˆ—è¡¨
            existing_results: å·²å­˜åœ¨çš„åˆ†æç»“æœå­—å…¸
            
        Returns:
            æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        try:
            # è¯»å–åŸå­—å¹•æ–‡ä»¶
            subtitle_entries = self._parse_jsonl_subtitle_file(subtitle_file)
            if not subtitle_entries:
                return True  # å¦‚æœæ²¡æœ‰å­—å¹•æ¡ç›®ï¼Œè§†ä¸ºæˆåŠŸ
            
            # æ”¶é›†æ‰€æœ‰æœ‰åˆ†æç»“æœçš„åºå·
            analyzed_indices = set()
            
            # ä»æ–°ç»“æœä¸­æ”¶é›†
            for result in new_results:
                subtitle_index = str(result.get('subtitle_index', ''))
                if subtitle_index:
                    analyzed_indices.add(subtitle_index)
            
            # ä»å·²æœ‰ç»“æœä¸­æ”¶é›†
            analyzed_indices.update(existing_results.keys())
            
            # æ›´æ–°å­—å¹•æ¡ç›®ï¼Œæ·»åŠ has_analysiså­—æ®µ
            updated_entries = []
            for entry in subtitle_entries:
                entry_index = str(entry.get('index', ''))
                entry_copy = dict(entry)  # åˆ›å»ºå‰¯æœ¬
                
                if entry_index in analyzed_indices:
                    entry_copy['has_analysis'] = True
                
                updated_entries.append(entry_copy)
            
            # å†™å›å­—å¹•æ–‡ä»¶
            write_jsonl_subtitle_file(updated_entries, subtitle_file)
            return True
            
        except Exception as e:
            print(f"    âŒ æ›´æ–°å­—å¹•æ–‡ä»¶has_analysiså­—æ®µå¤±è´¥: {e}")
            return False
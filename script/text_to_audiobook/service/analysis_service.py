#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†ææœåŠ¡ - ä¸“é—¨å¤„ç†å­—å¹•è¯­è¨€å­¦åˆ†æ
ç»Ÿè®¡åŠŸèƒ½å·²ç‹¬ç«‹åˆ°StatisticsService
"""

import os
import json
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from infra import AIClient, FileManager
from infra.config_loader import AppConfig
from util import OUTPUT_DIRECTORIES, parse_srt_file


class AnalysisService:
    """å­—å¹•è¯­è¨€å­¦åˆ†ææœåŠ¡"""
    
    def __init__(self, config: AppConfig):
        """
        åˆå§‹åŒ–åˆ†ææœåŠ¡
        
        Args:
            config: åº”ç”¨é…ç½®
        """
        self.config = config
        self.ai_client = AIClient(config.api)
        self.file_manager = FileManager()
    
    def analyze_subtitle_files(self, subtitle_files: List[str], output_dir: str) -> List[str]:
        """
        æ‰¹é‡åˆ†æå­—å¹•æ–‡ä»¶ï¼Œé€æ–‡ä»¶å¢é‡å¤„ç†
        
        Args:
            subtitle_files: å­—å¹•æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºæ ¹ç›®å½•
            
        Returns:
            æˆåŠŸåˆ†æçš„æ–‡ä»¶åˆ—è¡¨
        """
        if not subtitle_files:
            print("âš ï¸ æœªæ‰¾åˆ°å­—å¹•æ–‡ä»¶ï¼Œè·³è¿‡åˆ†æ")
            return []
        
        # åˆ›å»ºåˆ†æç»“æœç›®å½•
        analysis_dir = os.path.join(output_dir, OUTPUT_DIRECTORIES['parsed_analysis'])
        self.file_manager.create_directory(analysis_dir)
        
        print(f"ğŸ” å¼€å§‹é€æ–‡ä»¶åˆ†æ {len(subtitle_files)} ä¸ªå­—å¹•æ–‡ä»¶...")
        
        # æ€»ä½“ç»Ÿè®¡
        analyzed_files = []
        total_stats = {
            'entries_processed': 0,
            'entries_failed': 0,
            'files_completed': 0,
            'files_failed': 0,
            'files_skipped': 0
        }
        
        # é€ä¸ªæ–‡ä»¶å¤„ç†
        for file_idx, subtitle_file in enumerate(subtitle_files, 1):
            print(f"\nğŸ“„ [{file_idx}/{len(subtitle_files)}] å¤„ç†æ–‡ä»¶: {os.path.basename(subtitle_file)}")
            
            try:
                file_stats = self._process_single_file(subtitle_file, analysis_dir)
                
                if file_stats['success']:
                    analyzed_files.append(subtitle_file)
                    total_stats['files_completed'] += 1
                else:
                    total_stats['files_failed'] += 1
                    
                # ç´¯è®¡æ¡ç›®ç»Ÿè®¡
                total_stats['entries_processed'] += file_stats['entries_processed']
                total_stats['entries_failed'] += file_stats['entries_failed']
                total_stats['files_skipped'] += file_stats.get('skipped', 0)
                
            except Exception as e:
                print(f"    âŒ å¤„ç†æ–‡ä»¶å¼‚å¸¸: {e}")
                total_stats['files_failed'] += 1
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        print(f"\nğŸ“Š åˆ†æå®Œæˆç»Ÿè®¡:")
        print(f"   ğŸ“ æˆåŠŸæ–‡ä»¶: {total_stats['files_completed']}/{len(subtitle_files)}")
        print(f"   ğŸ“„ å¤„ç†å­—å¹•æ¡ç›®: {total_stats['entries_processed']}")
        print(f"   âŒ å¤±è´¥å­—å¹•æ¡ç›®: {total_stats['entries_failed']}")
        if total_stats['files_skipped'] > 0:
            print(f"   â­ï¸  è·³è¿‡å·²å¤„ç†: {total_stats['files_skipped']} ä¸ªæ–‡ä»¶")
        
        return analyzed_files
    
    def _process_single_file(self, subtitle_file: str, analysis_dir: str) -> Dict:
        """
        å¤„ç†å•ä¸ªå­—å¹•æ–‡ä»¶
        
        Args:
            subtitle_file: å­—å¹•æ–‡ä»¶è·¯å¾„
            analysis_dir: åˆ†æç»“æœç›®å½•
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡å­—å…¸
        """
        file_stats = {
            'success': False,
            'entries_processed': 0,
            'entries_failed': 0,
            'skipped': 0
        }
        
        try:
            # 1. è§£æå­—å¹•æ–‡ä»¶
            subtitle_entries = self._parse_srt_file(subtitle_file)
            if not subtitle_entries:
                print(f"    âš ï¸ æ–‡ä»¶æ— æœ‰æ•ˆå­—å¹•ï¼Œè·³è¿‡")
                return file_stats
            
            # 2. åŠ è½½å·²æœ‰åˆ†æç»“æœ
            subtitle_name = self.file_manager.get_basename_without_extension(subtitle_file)
            json_file = os.path.join(analysis_dir, f"{subtitle_name}.json")
            existing_results = self._load_existing_results(json_file)
            
            # 3. è¯†åˆ«éœ€è¦å¤„ç†çš„å­—å¹•æ¡ç›®
            missing_subtitles = self._get_missing_subtitles(subtitle_entries, existing_results)
            
            if not missing_subtitles:
                print(f"    âœ… æ‰€æœ‰å­—å¹•å·²å¤„ç†å®Œæ¯•ï¼Œè·³è¿‡ ({len(subtitle_entries)} æ¡)")
                file_stats['skipped'] = 1
                file_stats['success'] = True
                return file_stats
            
            print(f"    ğŸ” éœ€è¦å¤„ç† {len(missing_subtitles)}/{len(subtitle_entries)} æ¡å­—å¹•")
            
            # 4. å¹¶å‘åˆ†æç¼ºå¤±çš„å­—å¹•æ¡ç›®
            new_results = []
            max_workers = min(len(missing_subtitles), self.config.api.max_concurrent_workers)
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤åˆ†æä»»åŠ¡
                future_to_entry = {
                    executor.submit(self._analyze_subtitle_entry, entry): entry
                    for entry in missing_subtitles
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_entry):
                    entry = future_to_entry[future]
                    try:
                        result = future.result()
                        if result:
                            new_results.append(result)
                            file_stats['entries_processed'] += 1
                        else:
                            file_stats['entries_failed'] += 1
                            
                        # æ˜¾ç¤ºè¿›åº¦
                        completed = file_stats['entries_processed'] + file_stats['entries_failed']
                        if completed % 5 == 0 or completed == len(missing_subtitles):
                            print(f"      ğŸ“Š è¿›åº¦: {completed}/{len(missing_subtitles)} æ¡")
                            
                    except Exception as e:
                        print(f"      âŒ åˆ†æå­—å¹• {entry.get('index', '?')} å‡ºé”™: {e}")
                        file_stats['entries_failed'] += 1
            
            # 5. ä¿å­˜åˆå¹¶ç»“æœ
            if new_results or existing_results:
                success = self._save_analysis_results(new_results, existing_results, subtitle_file, analysis_dir)
                if success:
                    file_stats['success'] = True
                    print(f"    âœ… æ–‡ä»¶å¤„ç†å®Œæˆ")
                else:
                    print(f"    âŒ ä¿å­˜å¤±è´¥")
            else:
                print(f"    âš ï¸ æ— æœ‰æ•ˆåˆ†æç»“æœ")
                
        except Exception as e:
            print(f"    âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
            
        return file_stats
    
    
    def _analyze_subtitle_entry(self, entry: Dict) -> Optional[Dict]:
        """
        åˆ†æå•ä¸ªå­—å¹•æ¡ç›®çš„è¯­è¨€å­¦ç‰¹å¾
        
        Args:
            entry: å­—å¹•æ¡ç›®
            
        Returns:
            åˆ†æç»“æœæˆ–None
        """
        try:
            english_text = entry['english_text']
            chinese_text = entry.get('chinese_text', '')
            
            system_prompt = """IMPORTANT: åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦ä»»ä½•é¢å¤–æ–‡å­—æˆ–è§£é‡Šã€‚

ä½œä¸ºè‹±è¯­è¯­è¨€å­¦å®¶ï¼Œè¯·åˆ†æç”¨æˆ·è¾“å…¥çš„è‹±è¯­å¥å­ï¼Œå¹¶ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºè¯­è¨€å­¦åˆ†æã€‚

å­—æ®µè¦æ±‚:
- sentence_structure: å¥æ³•æˆåˆ†åˆ†æï¼ˆä¸»è¯­+è°“è¯­+å®¾è¯­+çŠ¶è¯­ç­‰ï¼‰
- key_words: å¥å­ä¸­æœ‰æ„ä¹‰çš„æ ¸å¿ƒè¯æ±‡ï¼Œæ’é™¤theã€aã€isã€Mrs.ç­‰å¸¸è§è¯
- fixed_phrases: æœ‰å›ºå®šå«ä¹‰çš„çŸ­è¯­æ­é…ï¼Œæ’é™¤è¿‡äºç®€å•çš„ç»„åˆ
- core_grammar: é‡è¦è¯­æ³•ç°è±¡ï¼ˆæ—¶æ€ã€è¯­æ€ã€å¥å¼ç­‰ï¼‰
- colloquial_expression: æ­£å¼ä¸å£è¯­è¡¨è¾¾å¯¹æ¯”

è¾“å‡ºæ ¼å¼:
{
  "sentence_structure": "å¥å­ç»“æ„åˆ†æ",
  "key_words": [{"word": "å•è¯", "pos": "è¯æ€§", "meaning": "å«ä¹‰", "pronunciation": "éŸ³æ ‡"}],
  "fixed_phrases": [{"phrase": "çŸ­è¯­", "meaning": "å«ä¹‰"}],
  "core_grammar": [{"point": "è¯­æ³•ç‚¹", "explanation": "è§£é‡Š"}],
  "colloquial_expression": [{"formal": "æ­£å¼è¡¨è¾¾", "informal": "å£è¯­è¡¨è¾¾", "explanation": "ç”¨æ³•è¯´æ˜"}]
}

ç¤ºä¾‹1:
è¾“å…¥: "The project that we've been working on for months, which involves multiple stakeholders, will be completed once we receive the final approval."
è¾“å‡º:
{
  "sentence_structure": "ä¸»è¯­(The project) + å®šè¯­ä»å¥1(that we've been working on for months) + å®šè¯­ä»å¥2(which involves multiple stakeholders) + è°“è¯­(will be completed) + æ—¶é—´çŠ¶è¯­ä»å¥(once we receive the final approval)",
  "key_words": [{"word": "stakeholders", "pos": "n.", "meaning": "åˆ©ç›Šç›¸å…³è€…", "pronunciation": "/ËˆsteÉªkhoÊŠldÉ™rz/"}, {"word": "approval", "pos": "n.", "meaning": "æ‰¹å‡†ï¼ŒåŒæ„", "pronunciation": "/É™ËˆpruËvÉ™l/"}],
  "fixed_phrases": [{"phrase": "work on", "meaning": "ä»äº‹ï¼Œè‡´åŠ›äº"}],
  "core_grammar": [{"point": "å®šè¯­ä»å¥åµŒå¥—", "explanation": "ä¸¤ä¸ªå®šè¯­ä»å¥ä¿®é¥°åŒä¸€ä¸»è¯­ï¼Œ'that'å¼•å¯¼é™åˆ¶æ€§å®šè¯­ä»å¥ï¼Œ'which'å¼•å¯¼éé™åˆ¶æ€§å®šè¯­ä»å¥"}],
  "colloquial_expression": [{"formal": "receive the final approval", "informal": "get the green light", "explanation": "'get the green light'è¡¨ç¤ºè·å¾—è®¸å¯ï¼Œæ¯”'receive approval'æ›´ç”ŸåŠ¨"}]
}

ç¤ºä¾‹2:
è¾“å…¥: "I visited the museum yesterday."
è¾“å‡º:
{
  "sentence_structure": "ä¸»è¯­(I) + è°“è¯­(visited) + å®¾è¯­(the museum) + æ—¶é—´çŠ¶è¯­(yesterday)",
  "key_words": [{"word": "museum", "pos": "n.", "meaning": "åšç‰©é¦†", "pronunciation": "/mjuËˆziËÉ™m/"}, {"word": "visited", "pos": "v.", "meaning": "å‚è§‚ï¼Œæ‹œè®¿", "pronunciation": "/ËˆvÉªzÉªtÉªd/"}],
  "fixed_phrases": [],
  "core_grammar": [{"point": "ä¸€èˆ¬è¿‡å»æ—¶", "explanation": "åŠ¨è¯visitçš„è¿‡å»å¼visitedï¼Œè¡¨ç¤ºè¿‡å»å‘ç”Ÿçš„åŠ¨ä½œ"}],
  "colloquial_expression": [{"formal": "I went to see the museum yesterday.", "informal": "I checked out the museum yesterday.", "explanation": "å£è¯­ä¸­å¸¸ç”¨check outä»£æ›¿visitï¼Œè¯­æ°”æ›´éšæ„"}]
}

æ³¨æ„: æ— ç›¸å…³å†…å®¹æ—¶å­—æ®µç•™ç©º(ç©ºæ•°ç»„[]æˆ–ç©ºå­—ç¬¦ä¸²"")ï¼Œä½†ä¸å¯çœç•¥å­—æ®µã€‚è®°ä½ï¼šä»…è¾“å‡ºJSONæ ¼å¼ã€‚"""
            
            user_prompt = f"è¯·åˆ†æä»¥ä¸‹è‹±è¯­å¥å­: \"{english_text}\""""
            
            # ä½¿ç”¨AIClientçš„chat_completionæ–¹æ³•
            response = self.ai_client.chat_completion(user_prompt, system_prompt, temperature=0.2, max_tokens=1500)
            if not response:
                return None
            
            # è§£æJSONå“åº”
            try:
                # æ™ºèƒ½æå–JSONå†…å®¹
                json_str = self._extract_json_from_response(response)
                if not json_str:
                    print(f"âš ï¸ æ— æ³•ä»å“åº”ä¸­æå–JSON: {response[:100]}...")
                    return None
                
                analysis_data = json.loads(json_str)
                
                result = {
                    "subtitle_index": entry['index'],
                    "timestamp": entry['timestamp'],
                    "english_text": english_text,
                    "chinese_text": chinese_text,
                    "sentence_structure": analysis_data.get("sentence_structure", ""),
                    "key_words": analysis_data.get("key_words", []),
                    "fixed_phrases": analysis_data.get("fixed_phrases", []),
                    "core_grammar": analysis_data.get("core_grammar", []),
                    "colloquial_expression": analysis_data.get("colloquial_expression", [])
                }
                
                return result
                
            except json.JSONDecodeError as e:
                print(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
                print(f"âš ï¸ åŸå§‹å“åº”: {response[:200]}...")
                return None
                
        except Exception as e:
            print(f"âŒ åˆ†æå­—å¹•æ¡ç›®å¤±è´¥: {e}")
            return None
    
    def _extract_json_from_response(self, response: str) -> str:
        """
        ä»AIå“åº”ä¸­æ™ºèƒ½æå–JSONå†…å®¹
        
        Args:
            response: AIè¿”å›çš„åŸå§‹å“åº”
            
        Returns:
            æå–çš„JSONå­—ç¬¦ä¸²ï¼Œå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        if not response:
            return ""
        
        # æ¸…ç†å“åº”å†…å®¹
        response = response.strip()
        
        # 1. å°è¯•ç§»é™¤ä»£ç å—æ ‡è®°
        if response.startswith('```json'):
            response = response[7:]  # ç§»é™¤ ```json
        if response.startswith('```'):
            response = response[3:]  # ç§»é™¤ ```
        if response.endswith('```'):
            response = response[:-3]  # ç§»é™¤ç»“å°¾çš„ ```
        
        response = response.strip()
        
        # 2. å¯»æ‰¾ç¬¬ä¸€ä¸ª{å’Œæœ€åä¸€ä¸ª}
        first_brace = response.find('{')
        last_brace = response.rfind('}')
        
        if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
            json_str = response[first_brace:last_brace + 1]
            return json_str
        
        # 3. å¦‚æœå·²ç»æ˜¯å®Œæ•´JSONæ ¼å¼ï¼Œç›´æ¥è¿”å›
        if response.startswith('{') and response.endswith('}'):
            return response
        
        return ""
    
    def _save_analysis_results(self, new_results: List[Dict], existing_results: Dict[str, Dict], 
                               subtitle_file: str, analysis_dir: str) -> bool:
        """
        åˆå¹¶æ–°æ—§åˆ†æç»“æœå¹¶ä¿å­˜åˆ°JSONLæ–‡ä»¶
        
        Args:
            new_results: æ–°åˆ†æçš„ç»“æœåˆ—è¡¨
            existing_results: å·²å­˜åœ¨çš„ç»“æœå­—å…¸
            subtitle_file: å­—å¹•æ–‡ä»¶è·¯å¾„
            analysis_dir: åˆ†æç»“æœç›®å½•
            
        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            subtitle_name = self.file_manager.get_basename_without_extension(subtitle_file)
            json_file = os.path.join(analysis_dir, f"{subtitle_name}.json")
            
            # åˆå¹¶æ–°æ—§ç»“æœ
            all_results = dict(existing_results)  # å¤åˆ¶å·²æœ‰ç»“æœ
            
            # æ·»åŠ æ–°ç»“æœï¼Œè¦†ç›–å·²æœ‰çš„ç›¸åŒåºå·
            for result in new_results:
                subtitle_index = str(result.get('subtitle_index', ''))
                if subtitle_index:
                    # ä¸ºæ¯ä¸ªå­—å¹•æ¡ç›®æ·»åŠ æºæ–‡ä»¶ä¿¡æ¯
                    result_with_source = {
                        "source_file": os.path.basename(subtitle_file),
                        **result
                    }
                    all_results[subtitle_index] = result_with_source
            
            # æŒ‰å­—å¹•åºå·æ’åºï¼ˆæ•°å€¼æ’åºï¼‰
            sorted_results = sorted(all_results.values(), 
                                  key=lambda x: int(x.get('subtitle_index', '0')) if str(x.get('subtitle_index', '0')).isdigit() else 0)
            
            # ä¿å­˜ä¸ºJSONLæ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªå­—å¹•æ¡ç›®çš„è§£æJSON
            with open(json_file, 'w', encoding='utf-8') as f:
                for result in sorted_results:
                    f.write(json.dumps(result, ensure_ascii=False, separators=(',', ':')) + '\n')
            
            total_count = len(sorted_results)
            new_count = len(new_results)
            print(f"    âœ… åˆ†æç»“æœå·²ä¿å­˜: {json_file} (æ–°å¢{new_count}æ¡ï¼Œæ€»è®¡{total_count}æ¡)")
            return True
            
        except Exception as e:
            print(f"    âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")
            return False
    
    
    def _load_existing_results(self, json_file: str) -> Dict[str, Dict]:
        """
        åŠ è½½å·²å­˜åœ¨çš„åˆ†æç»“æœ
        
        Args:
            json_file: JSONæ–‡ä»¶è·¯å¾„
            
        Returns:
            å·²å­˜åœ¨çš„åˆ†æç»“æœå­—å…¸ï¼Œé”®ä¸ºsubtitle_indexï¼Œå€¼ä¸ºåˆ†æç»“æœ
        """
        existing_results = {}
        
        if not os.path.exists(json_file):
            return existing_results
            
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                        
                    try:
                        result = json.loads(line)
                        subtitle_index = str(result.get('subtitle_index', ''))
                        if subtitle_index:
                            existing_results[subtitle_index] = result
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ è§£æJSONè¡Œ {line_num} å¤±è´¥: {e}")
                        continue
                        
        except Exception as e:
            print(f"âš ï¸ è¯»å–å·²æœ‰ç»“æœæ–‡ä»¶å¤±è´¥ {json_file}: {e}")
            
        return existing_results
    
    def _get_missing_subtitles(self, all_subtitles: List[Dict], existing_results: Dict[str, Dict]) -> List[Dict]:
        """
        è¯†åˆ«éœ€è¦å¤„ç†çš„å­—å¹•æ¡ç›®
        
        Args:
            all_subtitles: æ‰€æœ‰å­—å¹•æ¡ç›®åˆ—è¡¨
            existing_results: å·²å­˜åœ¨çš„åˆ†æç»“æœå­—å…¸
            
        Returns:
            éœ€è¦å¤„ç†çš„å­—å¹•æ¡ç›®åˆ—è¡¨
        """
        missing_subtitles = []
        
        for subtitle in all_subtitles:
            subtitle_index = str(subtitle.get('index', ''))
            if subtitle_index not in existing_results:
                missing_subtitles.append(subtitle)
                
        return missing_subtitles
    
    def _parse_srt_file(self, subtitle_file: str) -> List[Dict]:
        """è§£æSRTå­—å¹•æ–‡ä»¶ - ä½¿ç”¨ç»Ÿä¸€çš„è§£æå·¥å…·"""
        return parse_srt_file(subtitle_file)
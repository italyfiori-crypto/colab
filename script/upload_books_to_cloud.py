#!/usr/bin/env python3
"""
å¾®ä¿¡äº‘æœåŠ¡ä¹¦ç±æ•°æ®ä¸Šä¼ è„šæœ¬
ç”¨äºå°†outputç›®å½•ä¸‹çš„æœ‰å£°ä¹¦æ•°æ®ä¸Šä¼ åˆ°å¾®ä¿¡äº‘æ•°æ®åº“å’Œå­˜å‚¨
"""

import os
import json
import time
import requests
import logging
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from pathlib import Path


class WeChatCloudUploader:
    """å¾®ä¿¡äº‘æœåŠ¡ä¸Šä¼ å™¨"""
    
    def __init__(self, app_id: str, app_secret: str, env_id: str):
        """
        åˆå§‹åŒ–ä¸Šä¼ å™¨
        
        Args:
            app_id: å¾®ä¿¡å°ç¨‹åºAppID
            app_secret: å¾®ä¿¡å°ç¨‹åºAppSecret
            env_id: å¾®ä¿¡äº‘ç¯å¢ƒID
        """
        self.app_id = app_id
        self.app_secret = app_secret
        self.env_id = env_id
        
        # Access Tokenç›¸å…³
        self.access_token = None
        self.token_expires_at = None
        
        # APIç«¯ç‚¹
        self.token_url = "https://api.weixin.qq.com/cgi-bin/token"
        self.upload_file_url = "https://api.weixin.qq.com/tcb/uploadfile"
        self.database_add_url = "https://api.weixin.qq.com/tcb/databaseadd"
        self.database_query_url = "https://api.weixin.qq.com/tcb/databasequery"
        self.database_update_url = "https://api.weixin.qq.com/tcb/databaseupdate"
        
        # é…ç½®æ—¥å¿—
        self._setup_logging()
        
        # æ•°æ®ç›®å½•
        self.output_dir = Path("output")
        
    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        # æ–‡ä»¶å¤„ç†å™¨ - è®°å½•æ‰€æœ‰æ—¥å¿—
        file_handler = logging.FileHandler('upload_books.log', encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
        
        # æ§åˆ¶å°å¤„ç†å™¨ - åªè®°å½•é‡è¦ä¿¡æ¯
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(logging.Formatter('%(message)s'))
        
        # é…ç½®æ ¹æ—¥å¿—å™¨
        logging.basicConfig(
            level=logging.INFO,
            handlers=[file_handler, console_handler]
        )
        self.logger = logging.getLogger(__name__)
        
    def calculate_md5(self, file_path: str) -> str:
        """
        è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        if not os.path.exists(file_path):
            return ""
            
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            self.logger.error(f"âŒ è®¡ç®—MD5å¤±è´¥ {file_path}: {e}")
            return ""
        
    def get_access_token(self) -> str:
        """
        è·å–Access Token
        
        Returns:
            access_tokenå­—ç¬¦ä¸²
        """
        # æ£€æŸ¥tokenæ˜¯å¦è¿‡æœŸ
        if (self.access_token and self.token_expires_at and 
            datetime.now() < self.token_expires_at):
            return self.access_token
            
        self.logger.info("æ­£åœ¨è·å–æ–°çš„Access Token...")
        
        params = {
            'grant_type': 'client_credential',
            'appid': self.app_id,
            'secret': self.app_secret
        }
        
        try:
            response = requests.get(self.token_url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            if 'access_token' in data:
                self.access_token = data['access_token']
                # æå‰5åˆ†é’Ÿè¿‡æœŸï¼Œé¿å…è¾¹ç•Œæƒ…å†µ
                expires_in = data.get('expires_in', 7200) - 300
                self.token_expires_at = datetime.now() + timedelta(seconds=expires_in)
                
                self.logger.info(f"Access Tokenè·å–æˆåŠŸï¼Œæœ‰æ•ˆæœŸè‡³: {self.token_expires_at}")
                return self.access_token
            else:
                raise Exception(f"è·å–Access Tokenå¤±è´¥: {data}")
                
        except Exception as e:
            self.logger.error(f"è·å–Access Tokenå‡ºé”™: {e}")
            raise
            
    def upload_file(self, local_path: str, cloud_path: str) -> Optional[str]:
        """
        ä¸Šä¼ æ–‡ä»¶åˆ°äº‘å­˜å‚¨
        
        Args:
            local_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
            cloud_path: äº‘å­˜å‚¨è·¯å¾„
            
        Returns:
            ä¸Šä¼ æˆåŠŸè¿”å›file_idï¼Œå¤±è´¥è¿”å›None
        """
        filename = os.path.basename(local_path)
        
        if not os.path.exists(local_path):
            self.logger.error(f"âŒ æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {local_path}")
            return None
            
        try:
            # 1. è·å–ä¸Šä¼ é“¾æ¥
            access_token = self.get_access_token()
            
            upload_data = {
                "env": self.env_id,
                "path": cloud_path
            }
            
            response = requests.post(
                f"{self.upload_file_url}?access_token={access_token}",
                json=upload_data,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            
            if result.get('errcode') != 0:
                self.logger.error(f"âŒ è·å–ä¸Šä¼ é“¾æ¥å¤±è´¥: {result}")
                return None
            
            # æ£€æŸ¥å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨
            required_fields = ['url', 'file_id']
            for field in required_fields:
                if field not in result:
                    self.logger.error(f"âŒ APIå“åº”ç¼ºå°‘å¿…è¦å­—æ®µ '{field}': {result}")
                    return None
                    
            upload_url = result['url']
            file_id = result['file_id']
            
            # 2. ä¸Šä¼ æ–‡ä»¶
            # æŒ‰ç…§å®˜æ–¹æ–‡æ¡£æ ¼å¼ï¼šPOST multipart/form-data
            with open(local_path, 'rb') as f:
                # æ„å»ºPOSTè¡¨å•ï¼Œä¸¥æ ¼æŒ‰ç…§å®˜æ–¹æ–‡æ¡£æ ¼å¼
                files = {
                    'key': (None, cloud_path),                              # è¯·æ±‚åŒ…ä¸­çš„ path å­—æ®µ  
                    'Signature': (None, result['authorization']),           # è¿”å›æ•°æ®çš„ authorization å­—æ®µ
                    'x-cos-security-token': (None, result['token']),        # è¿”å›æ•°æ®çš„ token å­—æ®µ
                    'x-cos-meta-fileid': (None, result['cos_file_id']),     # è¿”å›æ•°æ®çš„ cos_file_id å­—æ®µ
                    'file': (os.path.basename(local_path), f)               # æ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹
                }
                
                upload_response = requests.post(upload_url, files=files, timeout=60)
                upload_response.raise_for_status()
            
            self.logger.info(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {filename}")
            return file_id
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥ {os.path.basename(local_path)}: {e}")
            return None
        except KeyError as e:
            self.logger.error(f"âŒ APIå“åº”å­—æ®µç¼ºå¤± {os.path.basename(local_path)}: ç¼ºå°‘å­—æ®µ {e}")
            self.logger.error(f"å®Œæ•´APIå“åº”: {locals().get('result', 'APIå“åº”æœªè·å–')}")
            return None
        except Exception as e:
            self.logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥ {os.path.basename(local_path)}: {type(e).__name__}: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return None
            
    def add_database_records(self, collection: str, records: List[Dict]) -> bool:
        """
        æ‰¹é‡æ·»åŠ æ•°æ®åº“è®°å½•
        
        Args:
            collection: é›†åˆåç§°
            records: è®°å½•åˆ—è¡¨
            
        Returns:
            æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
        """
        if not records:
            return True
            
        try:
            access_token = self.get_access_token()
            
            # æ„é€ æ•°æ®åº“æŸ¥è¯¢è¯­å¥
            records_str = json.dumps(records, ensure_ascii=False)
            query = f"db.collection('{collection}').add({{data: {records_str}}})"
            
            data = {
                "env": self.env_id,
                "query": query
            }
            
            response = requests.post(
                f"{self.database_add_url}?access_token={access_token}",
                json=data,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            if result.get('errcode') != 0:
                self.logger.error(f"âŒ æ•°æ®åº“æ’å…¥å¤±è´¥: {result}")
                return False
                
            self.logger.info(f"âœ… æˆåŠŸæ’å…¥{len(records)}æ¡è®°å½•åˆ°{collection}é›†åˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
            return False
            
    def update_database_records(self, collection: str, record_id: str, update_data: Dict) -> bool:
        """
        æ›´æ–°æ•°æ®åº“è®°å½•
        
        Args:
            collection: é›†åˆåç§°
            record_id: è®°å½•ID
            update_data: æ›´æ–°æ•°æ®
            
        Returns:
            æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
        """
        if not update_data:
            return True
            
        try:
            access_token = self.get_access_token()
            
            # æ„é€ æ•°æ®åº“æ›´æ–°è¯­å¥
            # è¿‡æ»¤æ‰_idå­—æ®µï¼Œå› ä¸ºæ›´æ–°æ—¶ä¸èƒ½åŒ…å«_id
            filtered_data = {k: v for k, v in update_data.items() if k != '_id'}
            update_str = json.dumps(filtered_data, ensure_ascii=False)
            query = f"db.collection('{collection}').doc('{record_id}').update({{data: {update_str}}})"
            
            data = {
                "env": self.env_id,
                "query": query
            }
            
            response = requests.post(
                f"{self.database_update_url}?access_token={access_token}",
                json=data,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            if result.get('errcode') != 0:
                self.logger.error(f"âŒ æ•°æ®åº“æ›´æ–°å¤±è´¥: {result}")
                return False
                
            modified_count = result.get('modified', 0)
            if modified_count > 0:
                self.logger.info(f"âœ… æˆåŠŸæ›´æ–°è®°å½•: {record_id}")
            else:
                self.logger.warning(f"âš ï¸ è®°å½•æœªå‘ç”Ÿå˜åŒ–: {record_id}")
                
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®åº“æ›´æ–°å¤±è´¥: {e}")
            return False
            
    def query_database(self, collection: str, query_filter: Dict = None, skip: int = 0, limit: int = 20) -> List[Dict]:
        """
        æŸ¥è¯¢æ•°æ®åº“è®°å½•
        
        Args:
            collection: é›†åˆåç§°
            query_filter: æŸ¥è¯¢æ¡ä»¶ï¼Œå¦‚æœä¸ºNoneåˆ™æŸ¥è¯¢æ‰€æœ‰è®°å½•
            skip: è·³è¿‡çš„è®°å½•æ•°
            limit: é™åˆ¶è¿”å›çš„è®°å½•æ•°ï¼ˆæœ€å¤§20ï¼‰
            
        Returns:
            æŸ¥è¯¢ç»“æœåˆ—è¡¨
        """
        try:
            access_token = self.get_access_token()
            
            # æ„é€ æŸ¥è¯¢è¯­å¥
            if query_filter:
                filter_str = json.dumps(query_filter, ensure_ascii=False)
                query = f"db.collection('{collection}').where({filter_str}).skip({skip}).limit({limit}).get()"
            else:
                query = f"db.collection('{collection}').skip({skip}).limit({limit}).get()"
            
            data = {
                "env": self.env_id,
                "query": query
            }
            
            response = requests.post(
                f"{self.database_query_url}?access_token={access_token}",
                json=data,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            if result.get('errcode') != 0:
                self.logger.error(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {result}")
                return []
                
            # è§£ææŸ¥è¯¢ç»“æœ
            data_list = result.get('data', [])
            if isinstance(data_list, str):
                # å¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æJSON
                data_list = json.loads(data_list)
            
            # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®ï¼Œå¤„ç†åµŒå¥—JSONå­—ç¬¦ä¸²
            if isinstance(data_list, list):
                parsed_list = []
                for item in data_list:
                    if isinstance(item, str):
                        # å¦‚æœåˆ—è¡¨ä¸­çš„é¡¹æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºJSON
                        try:
                            parsed_item = json.loads(item)
                            parsed_list.append(parsed_item)
                        except json.JSONDecodeError:
                            # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œä¿æŒåŸæ ·
                            parsed_list.append(item)
                    else:
                        # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥æ·»åŠ 
                        parsed_list.append(item)
                return parsed_list
            else:
                return []
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
            return []
            
    def query_all_records(self, collection: str, query_filter: Dict = None) -> List[Dict]:
        """
        åˆ†æ‰¹æŸ¥è¯¢æ‰€æœ‰è®°å½•ï¼Œè‡ªåŠ¨å¤„ç†åˆ†é¡µ
        
        Args:
            collection: é›†åˆåç§°
            query_filter: æŸ¥è¯¢æ¡ä»¶ï¼Œå¦‚æœä¸ºNoneåˆ™æŸ¥è¯¢æ‰€æœ‰è®°å½•
            
        Returns:
            æ‰€æœ‰æŸ¥è¯¢ç»“æœçš„åˆ—è¡¨
        """
        all_records = []
        batch_size = 20  # æ¯æ‰¹20æ¡è®°å½•
        skip = 0
        
        self.logger.info(f"ğŸ” å¼€å§‹åˆ†æ‰¹æŸ¥è¯¢{collection}é›†åˆ...")
        
        while True:
            # æŸ¥è¯¢å½“å‰æ‰¹æ¬¡
            batch_records = self.query_database(collection, query_filter, skip, batch_size)
            
            if not batch_records:
                # æ²¡æœ‰æ›´å¤šè®°å½•ï¼Œé€€å‡ºå¾ªç¯
                break
                
            all_records.extend(batch_records)
            
            # å¦‚æœè¿”å›çš„è®°å½•æ•°å°‘äºæ‰¹æ¬¡å¤§å°ï¼Œè¯´æ˜å·²ç»æ˜¯æœ€åä¸€æ‰¹
            if len(batch_records) < batch_size:
                break
                
            skip += batch_size
            self.logger.info(f"ğŸ“„ å·²æŸ¥è¯¢{len(all_records)}æ¡è®°å½•...")
        
        self.logger.info(f"âœ… å®ŒæˆæŸ¥è¯¢{collection}é›†åˆï¼Œå…±{len(all_records)}æ¡è®°å½•")
        return all_records
            
    def compare_book_data(self, new_data: Dict, existing_data: Dict) -> Tuple[bool, List[str]]:
        """
        æ¯”è¾ƒä¹¦ç±æ•°æ®æ˜¯å¦éœ€è¦æ›´æ–°
        
        Args:
            new_data: æ–°çš„ä¹¦ç±æ•°æ®
            existing_data: æ•°æ®åº“ä¸­çš„ç°æœ‰æ•°æ®
            
        Returns:
            (éœ€è¦æ›´æ–°, å˜åŒ–å­—æ®µåˆ—è¡¨) å…ƒç»„
        """
        if not existing_data:
            return True, ["new_book"]
            
        # ç±»å‹æ£€æŸ¥å’Œè½¬æ¢
        if isinstance(existing_data, str):
            try:
                existing_data = json.loads(existing_data)
            except json.JSONDecodeError:
                self.logger.error(f"âŒ æ— æ³•è§£æç°æœ‰æ•°æ®: {existing_data}")
                return True, ["data_parse_error"]
        
        if not isinstance(existing_data, dict):
            self.logger.error(f"âŒ ç°æœ‰æ•°æ®æ ¼å¼é”™è¯¯: {type(existing_data)}")
            return True, ["data_format_error"]
        
        # éœ€è¦æ¯”è¾ƒçš„å­—æ®µåˆ—è¡¨
        compare_fields = [
            'title', 'author', 'description', 'category', 
            'total_chapters', 'total_duration', 'is_active', 
            'cover_md5', 'tags'
        ]
        
        changed_fields = []
        
        for field in compare_fields:
            new_value = new_data.get(field, '')
            existing_value = existing_data.get(field, '')
            
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒï¼Œé¿å…ç±»å‹é—®é¢˜
            new_str = str(new_value).strip()
            existing_str = str(existing_value).strip()
            if new_str != existing_str:
                changed_fields.append(field)
                # self.logger.info(f"ğŸ”„ ä¹¦ç±å­—æ®µå˜åŒ– [{field}]: {existing_str} â†’ {new_str}")
        
        needs_update = len(changed_fields) > 0
        return needs_update, changed_fields
        
    def compare_chapter_data(self, new_data: Dict, existing_data: Dict) -> Tuple[bool, List[str]]:
        """
        æ¯”è¾ƒç« èŠ‚æ•°æ®æ˜¯å¦éœ€è¦æ›´æ–°
        
        Args:
            new_data: æ–°çš„ç« èŠ‚æ•°æ®
            existing_data: æ•°æ®åº“ä¸­çš„ç°æœ‰æ•°æ®
            
        Returns:
            (éœ€è¦æ›´æ–°, å˜åŒ–å­—æ®µåˆ—è¡¨) å…ƒç»„
        """
        if not existing_data:
            return True, ["new_chapter"]
        
        # éœ€è¦æ¯”è¾ƒçš„å­—æ®µåˆ—è¡¨
        compare_fields = [
            'title', 'duration', 'is_active', 
            'audio_md5', 'subtitle_md5', 'chapter_number'
        ]
        
        changed_fields = []
        
        for field in compare_fields:
            new_value = new_data.get(field, '')
            existing_value = existing_data.get(field, '')
            
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒï¼Œé¿å…ç±»å‹é—®é¢˜
            new_str = str(new_value).strip()
            existing_str = str(existing_value).strip()
            if new_str != existing_str:
                changed_fields.append(field)
                # self.logger.info(f"ğŸ”„ ç« èŠ‚å­—æ®µå˜åŒ– [{field}]: {existing_str} â†’ {new_str}")
        
        needs_update = len(changed_fields) > 0
        return needs_update, changed_fields
        
    def parse_book_data(self, book_dir: Path) -> Tuple[Dict, List[Dict]]:
        """
        è§£æå•æœ¬ä¹¦çš„æ•°æ®
        
        Args:
            book_dir: ä¹¦ç±ç›®å½•è·¯å¾„
            
        Returns:
            (book_data, chapters_data) å…ƒç»„
        """
        book_id = book_dir.name
        meta_file = book_dir / "meta.json"
        
        if not meta_file.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°meta.jsonæ–‡ä»¶: {meta_file}")
            
        with open(meta_file, 'r', encoding='utf-8') as f:
            meta_data = json.load(f)
            
        book_info = meta_data['book']
        chapters_info = meta_data['chapters']
        
        # è®¡ç®—å°é¢æ–‡ä»¶MD5
        cover_file_path = os.path.join(book_dir, book_info.get('local_cover_file', ''))
        cover_md5 = self.calculate_md5(cover_file_path)
        
        # æ„é€ booksè¡¨æ•°æ®
        book_data = {
            '_id': book_id,
            'title': book_info['title'],
            'author': book_info.get('author', ''),
            'cover_url': '',  # ç¨åä¸Šä¼ å°é¢åå¡«å……
            'cover_md5': cover_md5,  # æ–°å¢å°é¢MD5å­—æ®µ
            'category': book_info.get('category', ''),  # é»˜è®¤æ–‡å­¦ç±»
            'description': book_info.get('description', ''),
            'total_chapters': book_info.get('total_chapters', 0),
            'total_duration': int(book_info.get('total_duration', 0)), 
            'is_active': True,
            'tags': book_info.get('tags', []),
            'local_cover_file': cover_file_path,
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat(),
            'done': book_info.get('done', False)
        }
        
        # æ„é€ chaptersè¡¨æ•°æ®
        chapters_data = []
        
        for i, chapter_info in enumerate(chapters_info):
            # è®¡ç®—éŸ³é¢‘æ–‡ä»¶MD5
            audio_file_path = os.path.join(book_dir, chapter_info.get('local_audio_file', ''))
            audio_md5 = self.calculate_md5(audio_file_path)
            
            # è®¡ç®—å­—å¹•æ–‡ä»¶MD5
            subtitle_file_path = os.path.join(book_dir, chapter_info.get('local_subtitle_file', ''))
            subtitle_md5 = self.calculate_md5(subtitle_file_path)

            chapter_data = {
                '_id': f"{book_id}_{chapter_info['chapter_number']}_{i}",
                'book_id': book_id,
                'chapter_number': chapter_info['chapter_number'],
                'title': chapter_info['title_cn'] or chapter_info['title'],
                'duration': int(chapter_info['duration']) if chapter_info['duration'] else 0,
                'is_active': True,
                'audio_url': '',  # ç¨åä¸Šä¼ éŸ³é¢‘åå¡«å……
                'audio_md5': audio_md5,  # æ–°å¢éŸ³é¢‘MD5å­—æ®µ
                'subtitle_url': '',  # ç¨åä¸Šä¼ å­—å¹•åå¡«å……
                'subtitle_md5': subtitle_md5,  # æ–°å¢å­—å¹•MD5å­—æ®µ
                'local_audio_file': audio_file_path,
                'local_subtitle_file': subtitle_file_path,
                'created_at': datetime.now().isoformat(),
                'updated_at': datetime.now().isoformat()
            }
            
            chapters_data.append(chapter_data)
            
        return book_data, chapters_data
        
    def upload_book_cover(self, book_dir: Path, book_id: str, book_data: Dict) -> str:
        """
        ä¸Šä¼ ä¹¦ç±å°é¢æ–‡ä»¶
        
        Args:
            book_dir: ä¹¦ç±ç›®å½•
            book_id: ä¹¦ç±ID
            
        Returns:
            cover_file_id
        """
        # ä¸Šä¼ å°é¢
        cover_file = book_data.get('local_cover_file', '')
        if os.path.exists(cover_file):
            cloud_path = f"books/{book_id}/cover.jpg"
            cover_file_id = self.upload_file(str(cover_file), cloud_path)
        else:
            self.logger.warning(f"âš ï¸  å°é¢æ–‡ä»¶ä¸å­˜åœ¨: {cover_file}")
            cover_file_id = ""

        return cover_file_id
        
    def upload_book_if_needed(self, book_dir: Path, book_data: Dict, existing_book: Dict, changed_fields: List[str]) -> bool:
        """
        æ ¹æ®å˜åŒ–å­—æ®µå†³å®šæ˜¯å¦ä¸Šä¼ ä¹¦ç±ç›¸å…³æ–‡ä»¶
        
        Args:
            book_dir: ä¹¦ç±ç›®å½•
            book_data: ä¹¦ç±æ•°æ®
            existing_book: æ•°æ®åº“ä¸­çš„ç°æœ‰ä¹¦ç±æ•°æ®
            changed_fields: å‘ç”Ÿå˜åŒ–çš„å­—æ®µåˆ—è¡¨
            
        Returns:
            ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        book_id = book_data['_id']
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸Šä¼ å°é¢
        if 'cover_md5' in changed_fields or not existing_book:
            cover_file_id = self.upload_book_cover(book_dir, book_id, book_data)
            if cover_file_id:
                book_data['cover_url'] = cover_file_id
                return True
            elif not existing_book:
                # æ–°ä¹¦ä½†å°é¢ä¸Šä¼ å¤±è´¥
                return False
            else:
                # æ›´æ–°ä¹¦ç±ä½†å°é¢ä¸Šä¼ å¤±è´¥ï¼Œä¿ç•™åŸURL
                book_data['cover_url'] = existing_book.get('cover_url', '')
                return True
        else:
            # å°é¢æ²¡æœ‰å˜åŒ–ï¼Œä¿ç•™åŸURL
            book_data['cover_url'] = existing_book.get('cover_url', '')
            return True
            
    def upload_chapter_if_needed(self, book_dir: Path, book_id: str, chapter_data: Dict, existing_chapter: Dict, changed_fields: List[str]) -> bool:
        """
        æ ¹æ®å˜åŒ–å­—æ®µå†³å®šæ˜¯å¦ä¸Šä¼ ç« èŠ‚ç›¸å…³æ–‡ä»¶
        
        Args:
            book_dir: ä¹¦ç±ç›®å½•
            book_id: ä¹¦ç±ID
            chapter_data: ç« èŠ‚æ•°æ®
            existing_chapter: æ•°æ®åº“ä¸­çš„ç°æœ‰ç« èŠ‚æ•°æ®
            changed_fields: å‘ç”Ÿå˜åŒ–çš„å­—æ®µåˆ—è¡¨
            
        Returns:
            ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        audio_file_id = ""
        subtitle_file_id = ""
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
        if 'audio_md5' in changed_fields or not existing_chapter:
            audio_file = chapter_data.get('local_audio_file', '')
            if audio_file and os.path.exists(audio_file):
                audio_filename = os.path.basename(audio_file)
                audio_cloud_path = f"books/{book_id}/audio/{audio_filename}"
                audio_file_id = self.upload_file(audio_file, audio_cloud_path)
                if not audio_file_id and not existing_chapter:
                    return False  # æ–°ç« èŠ‚éŸ³é¢‘ä¸Šä¼ å¤±è´¥
        else:
            # éŸ³é¢‘æ²¡æœ‰å˜åŒ–ï¼Œä¿ç•™åŸURL
            audio_file_id = existing_chapter.get('audio_url', '')
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸Šä¼ å­—å¹•æ–‡ä»¶
        if 'subtitle_md5' in changed_fields or not existing_chapter:
            subtitle_file = chapter_data.get('local_subtitle_file', '')
            if subtitle_file and os.path.exists(subtitle_file):
                subtitle_filename = os.path.basename(subtitle_file)
                subtitle_cloud_path = f"books/{book_id}/subtitles/{subtitle_filename}"
                subtitle_file_id = self.upload_file(subtitle_file, subtitle_cloud_path)
        else:
            # å­—å¹•æ²¡æœ‰å˜åŒ–ï¼Œä¿ç•™åŸURL
            subtitle_file_id = existing_chapter.get('subtitle_url', '')
        
        # æ›´æ–°ç« èŠ‚æ–‡ä»¶URL
        if audio_file_id:
            chapter_data['audio_url'] = audio_file_id
        if subtitle_file_id:
            chapter_data['subtitle_url'] = subtitle_file_id
            
        return True
        
    def process_single_chapter(self, book_dir: Path, book_id: str, chapter_data: Dict, existing_chapters_dict: Dict, stats: Dict) -> bool:
        """
        å¤„ç†å•ä¸ªç« èŠ‚çš„ä¸Šä¼ å’Œæ•°æ®åº“æ›´æ–°
        
        Args:
            book_dir: ä¹¦ç±ç›®å½•
            book_id: ä¹¦ç±ID
            chapter_data: ç« èŠ‚æ•°æ®
            existing_chapters_dict: ç°æœ‰ç« èŠ‚æ•°æ®å­—å…¸
            stats: ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        chapter_id = chapter_data['_id']
        existing_chapter = existing_chapters_dict.get(chapter_id)
        
        # æ¯”è¾ƒç« èŠ‚æ•°æ®æ˜¯å¦éœ€è¦æ›´æ–°
        needs_update, changed_fields = self.compare_chapter_data(chapter_data, existing_chapter)
        
        if not needs_update:
            stats['skipped_chapters'] += 1
            self.logger.info(f"â­ï¸ ç« èŠ‚æ— å˜åŒ–ï¼Œè·³è¿‡: {chapter_data['title']}")
            return True
        
        # è®°å½•å˜åŒ–ä¿¡æ¯
        if existing_chapter:
            stats['updated_chapters'] += 1
            self.logger.info(f"ğŸ“ æ£€æµ‹åˆ°ç« èŠ‚å˜åŒ– {chapter_data['title']}: {', '.join(changed_fields)}")
        else:
            stats['new_chapters'] += 1
            self.logger.info(f"ğŸ†• æ–°ç« èŠ‚: {chapter_data['title']}")
        
        # ä¸Šä¼ ç« èŠ‚æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not self.upload_chapter_if_needed(book_dir, book_id, chapter_data, existing_chapter, changed_fields):
            self.logger.error(f"âŒ ç« èŠ‚æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {chapter_data['title']}")
            return False
        
        # æ›´æ–°æ—¶é—´æˆ³
        chapter_data['updated_at'] = datetime.now().isoformat()
        if existing_chapter:
            chapter_data['created_at'] = existing_chapter.get('created_at', chapter_data['created_at'])
        
        # æ¸…ç†æœ¬åœ°æ–‡ä»¶è·¯å¾„
        chapter_clean = chapter_data.copy()
        del chapter_clean['local_audio_file']
        del chapter_clean['local_subtitle_file']
        
        # æ’å…¥æˆ–æ›´æ–°ç« èŠ‚æ•°æ®
        if existing_chapter:
            # æ›´æ–°ç°æœ‰è®°å½•
            self.logger.info(f"ğŸ’¾ æ›´æ–°ç« èŠ‚æ•°æ®: {chapter_data['title']}")
            if not self.update_database_records('chapters', chapter_id, chapter_clean):
                self.logger.error(f"âŒ ç« èŠ‚æ•°æ®æ›´æ–°å¤±è´¥: {chapter_data['title']}")
                return False
        else:
            # æ’å…¥æ–°è®°å½•
            self.logger.info(f"ğŸ’¾ æ’å…¥ç« èŠ‚æ•°æ®: {chapter_data['title']}")
            if not self.add_database_records('chapters', [chapter_clean]):
                self.logger.error(f"âŒ ç« èŠ‚æ•°æ®æ’å…¥å¤±è´¥: {chapter_data['title']}")
                return False
        
        return True
        
    def process_single_book(self, book_dir: Path, stats: Dict) -> bool:
        """
        å¤„ç†å•æœ¬ä¹¦çš„ä¸Šä¼ å’Œæ•°æ®åº“æ›´æ–°
        
        Args:
            book_dir: ä¹¦ç±ç›®å½•
            stats: ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        book_id = book_dir.name
        self.logger.info(f"\nğŸ“– å¤„ç†ä¹¦ç±: {book_id}")
        
        try:
            # è§£æä¹¦ç±æ•°æ®
            book_data, chapters_data = self.parse_book_data(book_dir)

            # å¦‚æœä¹¦ç±å·²å¤„ç†å®Œæˆï¼Œåˆ™è·³è¿‡
            if book_data and book_data['done']:
                stats['skipped_books'] += 1
                stats['skipped_chapters'] += len(chapters_data)
                self.logger.info(f"â­ï¸ ä¹¦ç±å·²å¤„ç†å®Œæˆï¼Œè·³è¿‡: {book_id}")
                return True
            
            # æŸ¥è¯¢æ•°æ®åº“ä¸­çš„ä¹¦ç±è®°å½•
            existing_books = self.query_database('books', {'_id': book_id})
            existing_book = existing_books[0] if existing_books else None
            
            # æ¯”è¾ƒä¹¦ç±æ•°æ®æ˜¯å¦éœ€è¦æ›´æ–°
            book_needs_update, changed_fields = self.compare_book_data(book_data, existing_book)
            
            # å¤„ç†ä¹¦ç±æ•°æ®
            if book_needs_update:
                # è®°å½•å˜åŒ–ä¿¡æ¯
                if existing_book:
                    stats['updated_books'] += 1
                    self.logger.info(f"ğŸ“š æ£€æµ‹åˆ°ä¹¦ç±å˜åŒ– {book_data['title']}: {', '.join(changed_fields)}")
                else:
                    stats['new_books'] += 1
                    self.logger.info(f"ğŸ†• æ–°ä¹¦ç±: {book_data['title']}")
                
                # ä¸Šä¼ ä¹¦ç±æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if not self.upload_book_if_needed(book_dir, book_data, existing_book, changed_fields):
                    self.logger.error(f"âŒ ä¹¦ç±æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {book_id}")
                    return False
                
                # æ›´æ–°æ—¶é—´æˆ³
                book_data['updated_at'] = datetime.now().isoformat()
                if existing_book:
                    book_data['created_at'] = existing_book.get('created_at', book_data['created_at'])
                
                # æ¸…ç†æœ¬åœ°æ–‡ä»¶è·¯å¾„
                book_data_clean = book_data.copy()
                del book_data_clean['local_cover_file']
                
                # æ’å…¥æˆ–æ›´æ–°ä¹¦ç±æ•°æ®
                if existing_book:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    self.logger.info(f"ğŸ’¾ æ›´æ–°ä¹¦ç±æ•°æ®: {book_data['title']}")
                    if not self.update_database_records('books', book_id, book_data_clean):
                        self.logger.error(f"âŒ ä¹¦ç±æ•°æ®æ›´æ–°å¤±è´¥: {book_id}")
                        return False
                else:
                    # æ’å…¥æ–°è®°å½•
                    self.logger.info(f"ğŸ’¾ æ’å…¥ä¹¦ç±æ•°æ®: {book_data['title']}")
                    if not self.add_database_records('books', [book_data_clean]):
                        self.logger.error(f"âŒ ä¹¦ç±æ•°æ®æ’å…¥å¤±è´¥: {book_id}")
                        return False
            else:
                stats['skipped_books'] += 1
                self.logger.info(f"â­ï¸ ä¹¦ç±æ— å˜åŒ–ï¼Œè·³è¿‡: {book_data['title']}")
            
            # æŸ¥è¯¢æ•°æ®åº“ä¸­çš„ç« èŠ‚è®°å½•ï¼ˆåˆ†æ‰¹æŸ¥è¯¢ï¼‰
            existing_chapters = self.query_all_records('chapters', {'book_id': book_id})
            existing_chapters_dict = {ch['_id']: ch for ch in existing_chapters}
            
            # å¤„ç†ç« èŠ‚æ•°æ®
            for i, chapter in enumerate(chapters_data):
                self.logger.info(f"ğŸ“ å¤„ç†ç« èŠ‚ {i+1}/{len(chapters_data)}: {chapter['title']}")
                if not self.process_single_chapter(book_dir, book_id, chapter, existing_chapters_dict, stats):
                    return False
            
            self.logger.info(f"âœ… ä¹¦ç±{book_id}å¤„ç†å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ å¤„ç†ä¹¦ç±{book_dir.name}å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False
            
    def print_upload_statistics(self, stats: Dict) -> None:
        """
        æ‰“å°ä¸Šä¼ ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        self.logger.info(f"\nğŸ“Š ä¸Šä¼ ç»Ÿè®¡:")
        self.logger.info(f"   æ€»ä¹¦ç±æ•°: {stats['total_books']}")
        self.logger.info(f"   æ–°å¢ä¹¦ç±: {stats['new_books']}")
        self.logger.info(f"   æ›´æ–°ä¹¦ç±: {stats['updated_books']}")
        self.logger.info(f"   è·³è¿‡ä¹¦ç±: {stats['skipped_books']}")
        self.logger.info(f"   æ–°å¢ç« èŠ‚: {stats['new_chapters']}")
        self.logger.info(f"   æ›´æ–°ç« èŠ‚: {stats['updated_chapters']}")
        self.logger.info(f"   è·³è¿‡ç« èŠ‚: {stats['skipped_chapters']}")
        
    def upload_all_books(self) -> bool:
        """
        æ™ºèƒ½ä¸Šä¼ æ‰€æœ‰ä¹¦ç±æ•°æ®ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰
        
        Returns:
            æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
        """
        # éªŒè¯è¾“å‡ºç›®å½•
        if not self.output_dir.exists():
            self.logger.error(f"âŒ è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {self.output_dir}")
            return False
            
        # è·å–ä¹¦ç±ç›®å½•åˆ—è¡¨
        book_dirs = [d for d in self.output_dir.iterdir() 
                    if d.is_dir() and (d / "meta.json").exists()]
                    
        if not book_dirs:
            self.logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•åŒ…å«meta.jsonçš„ä¹¦ç±ç›®å½•")
            return False
            
        self.logger.info(f"ğŸ“š å‘ç°{len(book_dirs)}æœ¬ä¹¦ç±: {[d.name for d in book_dirs]}")
        
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_books': len(book_dirs),
            'new_books': 0,
            'updated_books': 0,
            'skipped_books': 0,
            'new_chapters': 0,
            'updated_chapters': 0,
            'skipped_chapters': 0
        }
        
        # å¤„ç†æ‰€æœ‰ä¹¦ç±
        success = True
        for book_dir in book_dirs:
            if not self.process_single_book(book_dir, stats):
                success = False
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self.print_upload_statistics(stats)
                    
        return success


def main():
    """ä¸»å‡½æ•°"""
    print("å¾®ä¿¡äº‘æœåŠ¡ä¹¦ç±æ•°æ®ä¸Šä¼ è„šæœ¬")
    print("=" * 50)
    print("æ­¤è„šæœ¬å°†æŠŠoutputç›®å½•ä¸‹çš„æœ‰å£°ä¹¦æ•°æ®ä¸Šä¼ åˆ°å¾®ä¿¡äº‘æœåŠ¡")
    print("è¯·ç¡®ä¿å·²ç»åˆ›å»ºäº†bookså’Œchaptersæ•°æ®åº“é›†åˆ")
    print("=" * 50)
    
    # è·å–ç”¨æˆ·è¾“å…¥
    app_id = input("è¯·è¾“å…¥AppID (é»˜è®¤: wx7040936883aa6dad): ").strip() or "wx7040936883aa6dad"
    app_secret = input("è¯·è¾“å…¥AppSecret: ").strip() or "94051e8239a7a5181f32695c3c4895d9"
    env_id = input("è¯·è¾“å…¥äº‘ç¯å¢ƒID: ").strip() or "cloud1-1gpp78j208f0f610"
    
    if not app_secret or not env_id:
        print("é”™è¯¯: AppSecretå’Œäº‘ç¯å¢ƒIDä¸èƒ½ä¸ºç©º")
        return
        
    # ç¡®è®¤æ“ä½œ
    print(f"\né…ç½®ä¿¡æ¯:")
    print(f"AppID: {app_id}")
    print(f"AppSecret: {'*' * len(app_secret)}")
    print(f"äº‘ç¯å¢ƒID: {env_id}")
    
    confirm = input("\nç¡®è®¤å¼€å§‹ä¸Šä¼ ï¼Ÿ(y/N): ").strip().lower()
    if confirm != 'y':
        print("æ“ä½œå·²å–æ¶ˆ")
        return
        
    # åˆ›å»ºä¸Šä¼ å™¨
    uploader = WeChatCloudUploader(app_id, app_secret, env_id)
    
    # æµ‹è¯•è¿æ¥
    try:
        token = uploader.get_access_token()
        print(f"âœ… è¿æ¥æˆåŠŸï¼ŒAccess Token: {token[:20]}...")
    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥: {e}")
        return
        
    # å¼€å§‹ä¸Šä¼ 
    print("\nğŸš€ å¼€å§‹ä¸Šä¼ ä¹¦ç±æ•°æ®...")
    start_time = time.time()
    
    try:
        success = uploader.upload_all_books()
        
        elapsed_time = time.time() - start_time
        
        if success:
            print(f"\nğŸ‰ ä¸Šä¼ å®Œæˆï¼è€—æ—¶: {elapsed_time:.2f}ç§’")
            print("ğŸ“‹ è¯·æ£€æŸ¥å¾®ä¿¡äº‘æ§åˆ¶å°ç¡®è®¤æ•°æ®æ˜¯å¦æ­£ç¡®ä¸Šä¼ ")
        else:
            print(f"\nâŒ ä¸Šä¼ è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶: upload_books.log")
            
    except Exception as e:
        print(f"\nâŒ ä¸Šä¼ å¤±è´¥: {e}")
        
    print("\nğŸ“ è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°: upload_books.log")


if __name__ == "__main__":
    main()
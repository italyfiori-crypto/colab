#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¥å­æ‹†åˆ†æ¨¡å—
å°†å­ç« èŠ‚çš„æ®µè½æ‹†åˆ†ä¸ºå¥å­ï¼Œæ¯ä¸ªå¥å­å ä¸€è¡Œï¼Œä¿ç•™æ®µè½é—´éš”
"""

import os
import re
import nltk
import pysbd
from typing import List
from dataclasses import dataclass


@dataclass
class SentenceSplitterConfig:
    """å¥å­æ‹†åˆ†é…ç½®ç±»"""
    
    # è¾“å‡ºå­ç›®å½•å
    output_subdir: str = "sentences"
    
    # åˆ†å‰²å™¨ç±»å‹ï¼š'nltk' æˆ– 'pysbd'
    segmenter: str = "pysbd"
    
    # è¯­è¨€è®¾ç½®
    language: str = "en"
    
    # æ˜¯å¦æ¸…ç†æ–‡æœ¬
    clean: bool = False


class SentenceSplitter:
    """å¥å­æ‹†åˆ†å™¨"""
    
    def __init__(self, config: SentenceSplitterConfig):
        """
        åˆå§‹åŒ–å¥å­æ‹†åˆ†å™¨
        
        Args:
            config: å¥å­æ‹†åˆ†é…ç½®
        """
        nltk.download('punkt_tab')
        self.config = config
        self._ensure_nltk_data()
    
    def _ensure_nltk_data(self):
        """
        ç¡®ä¿NLTKæ•°æ®åŒ…å¯ç”¨
        """
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            print("ä¸‹è½½NLTK punktæ•°æ®åŒ…...")
            nltk.download('punkt')
    
    def split_files(self, input_files: List[str], output_dir: str) -> List[str]:
        """
        æ‹†åˆ†æ–‡ä»¶åˆ—è¡¨ä¸ºå¥å­çº§æ–‡ä»¶
        
        Args:
            input_files: è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            ç”Ÿæˆçš„å¥å­æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        # åˆ›å»ºè¾“å‡ºç›®å½•
        sentences_dir = os.path.join(output_dir, self.config.output_subdir)
        os.makedirs(sentences_dir, exist_ok=True)
        
        output_files = []
        
        for input_file in input_files:
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
            filename = os.path.basename(input_file)
            output_file = os.path.join(sentences_dir, filename)
            
            # å¤„ç†å•ä¸ªæ–‡ä»¶
            self._process_file(input_file, output_file)
            output_files.append(output_file)
            
            print(f"ğŸ“ å·²å¤„ç†å¥å­æ‹†åˆ†: {filename}")
        
        print(f"\nğŸ“ å¥å­æ‹†åˆ†å®Œæˆï¼Œè¾“å‡ºåˆ°: {sentences_dir}")
        return output_files
    
    def _process_file(self, input_file: str, output_file: str):
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶çš„å¥å­æ‹†åˆ†
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # è¯»å–è¾“å…¥æ–‡ä»¶
        with open(input_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–æ ‡é¢˜å’Œæ­£æ–‡
        title, body = self._extract_title_and_body(content)
        
        # å¤„ç†æ®µè½å¥å­æ‹†åˆ†
        processed_content = self._split_paragraphs_to_sentences(body)
        
        # æ„å»ºæœ€ç»ˆå†…å®¹
        final_content = f"{title}\n\n{processed_content}"
        
        # å†™å…¥è¾“å‡ºæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(final_content)
    
    def _extract_title_and_body(self, content: str) -> tuple[str, str]:
        """
        æå–æ ‡é¢˜å’Œæ­£æ–‡å†…å®¹
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            
        Returns:
            (æ ‡é¢˜, æ­£æ–‡å†…å®¹)
        """
        lines = content.split('\n')
        
        # ç¬¬ä¸€è¡Œæ˜¯æ ‡é¢˜
        title = lines[0].strip() if lines else "Unknown Title"
        
        # å…¶ä½™æ˜¯æ­£æ–‡ï¼ˆå»é™¤å¼€å¤´çš„ç©ºè¡Œï¼‰
        body_lines = lines[1:]
        while body_lines and not body_lines[0].strip():
            body_lines.pop(0)
        
        body = '\n'.join(body_lines)
        return title, body
    
    def _split_paragraphs_to_sentences(self, content: str) -> str:
        """
        å°†å†…å®¹æŒ‰æ®µè½æ‹†åˆ†ï¼Œå†å°†æ¯ä¸ªæ®µè½æ‹†åˆ†ä¸ºå¥å­
        
        Args:
            content: æ­£æ–‡å†…å®¹
            
        Returns:
            å¤„ç†åçš„å†…å®¹
        """
        # æŒ‰æ®µè½åˆ†å‰²ï¼ˆåŒæ¢è¡Œåˆ†å‰²ï¼‰
        paragraphs = re.split(r'\n\s*\n', content)
        
        # è¿‡æ»¤ç©ºæ®µè½
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        
        processed_paragraphs = []
        
        for paragraph in paragraphs:
            # å¯¹æ®µè½è¿›è¡Œå¥å­æ‹†åˆ†
            sentences = self._split_sentences(paragraph)
            
            # å°†å¥å­åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆæ¯å¥ä¸€è¡Œï¼‰
            if sentences:
                paragraph_content = '\n'.join(sentences)
                processed_paragraphs.append(paragraph_content)
        
        # æ®µè½é—´ç”¨ç©ºè¡Œåˆ†éš”
        return '\n\n'.join(processed_paragraphs)
    
    def _split_sentences(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå¥å­ï¼Œæ”¯æŒå¤šç§åˆ†å‰²å™¨
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å¥å­åˆ—è¡¨
        """
        # æ¸…ç†æ–‡æœ¬ï¼ˆç§»é™¤å¤šä½™ç©ºç™½ï¼‰
        text = re.sub(r'\s+', ' ', text.strip())
        
        if not text:
            return []
        
        # æ ¹æ®é…ç½®é€‰æ‹©åˆ†å‰²å™¨
        if self.config.segmenter == "pysbd":
            sentences = self._split_with_pysbd(text)
        else:
            sentences = self._split_with_nltk(text)
        
        # åå¤„ç†ï¼šåˆå¹¶å¼•å·å†…çš„å¥å­
        sentences = self._merge_quoted_sentences(sentences)
        
        # æ¸…ç†å¥å­ï¼ˆå»é™¤é¦–å°¾ç©ºç™½ï¼‰
        sentences = [s.strip() for s in sentences if s.strip()]
        
        return sentences
    
    def _split_with_pysbd(self, text: str) -> List[str]:
        """
        ä½¿ç”¨pySBDè¿›è¡Œå¥å­åˆ†å‰²
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å¥å­åˆ—è¡¨
        """
        seg = pysbd.Segmenter(language=self.config.language, clean=self.config.clean)
        return seg.segment(text)
    
    def _split_with_nltk(self, text: str) -> List[str]:
        """
        ä½¿ç”¨NLTKè¿›è¡Œå¥å­åˆ†å‰²
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å¥å­åˆ—è¡¨
        """
        return nltk.sent_tokenize(text)
    
    def _merge_quoted_sentences(self, sentences: List[str]) -> List[str]:
        """
        åˆå¹¶å¼•å·å†…è¢«é”™è¯¯æ‹†åˆ†çš„å¥å­
        
        Args:
            sentences: åŸå§‹å¥å­åˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„å¥å­åˆ—è¡¨
        """
        if not sentences:
            return sentences
        
        merged = []
        current_sentence = ""
        in_quote = False
        
        for sentence in sentences:
            # æ¸…ç†å¥å­ä¸­çš„è½¬ä¹‰å­—ç¬¦
            clean_sentence = sentence.replace('\\!', '!').replace('\\"', '"')
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¼•å·
            quote_count = clean_sentence.count('"')
            
            if not in_quote:
                # ä¸åœ¨å¼•å·å†…
                if quote_count % 2 == 1:
                    # å¼€å§‹å¼•å·
                    in_quote = True
                    current_sentence = clean_sentence
                else:
                    # å®Œæ•´å¥å­
                    merged.append(clean_sentence)
            else:
                # åœ¨å¼•å·å†…
                current_sentence += " " + clean_sentence
                if quote_count % 2 == 1:
                    # ç»“æŸå¼•å·
                    in_quote = False
                    merged.append(current_sentence)
                    current_sentence = ""
        
        # å¤„ç†æœªé—­åˆçš„å¼•å·
        if current_sentence:
            merged.append(current_sentence)
        
        return merged
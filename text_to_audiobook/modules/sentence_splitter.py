#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¥å­æ‹†åˆ†æ¨¡å—
å°†å­ç« èŠ‚çš„æ®µè½æ‹†åˆ†ä¸ºå¥å­ï¼Œæ¯ä¸ªå¥å­å ä¸€è¡Œï¼Œä¿ç•™æ®µè½é—´éš”
ä½¿ç”¨å¼•å·ä¼˜å…ˆçš„è¯­ä¹‰æ„ŸçŸ¥åˆ†å‰²æ–¹æ³•
"""

import os
import re
import nltk
import pysbd
from typing import List, Tuple
from dataclasses import dataclass

# é…ç½®ï¼šæ‹¬å·ç±»ç¬¦å·ï¼ˆæ•´ä½“ä¿ç•™ï¼‰
PAIR_SYMBOLS_PARENS = [
    ("(", ")"),
    ("[", "]"),
    ("{", "}"),
    ("ï¼ˆ", "ï¼‰"),
    ("ã€", "ã€‘"),
    ("ã€Š", "ã€‹"),
]

# é…ç½®ï¼šå¼•å·ç±»ç¬¦å·ï¼ˆå…è®¸å†…éƒ¨ç»§ç»­æ‹†åˆ†ï¼‰
PAIR_SYMBOLS_QUOTES = [
    ("â€œ", "â€"),
    ('"', '"'),
]

# é…ç½®ï¼šå¥ä¸­åˆ†éš”ç¬¦ï¼ˆå¯ä»¥å†æ‰©å±•ï¼‰
SPLIT_PUNCT = [",", "ï¼Œ", ":", "ï¼š", ";", "ï¼›", "!", "?"]

# åˆ†éš”ç¬¦ä¼˜å…ˆçº§åˆ—è¡¨ï¼ˆæŒ‰è¯­ä¹‰å¼ºåº¦æ’åºï¼‰
SEPARATORS = [
    ". ",        # å¥å·+ç©ºæ ¼ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
    "! ",        # æ„Ÿå¹å·+ç©ºæ ¼
    "? ",        # é—®å·+ç©ºæ ¼
    "; ",        # åˆ†å·+ç©ºæ ¼
    ": ",        # å†’å·+ç©ºæ ¼
    ", and ",    # é€—å·+andè¿è¯
    ", but ",    # é€—å·+butè¿è¯
    ", or ",     # é€—å·+orè¿è¯
    ", when ",   # é€—å·+whenè¿è¯
    ", that ",   # é€—å·+thatè¿è¯
    ", ",        # é€—å·+ç©ºæ ¼ï¼ˆæœ€ä½ä¼˜å…ˆçº§ï¼‰
]

# ä¿æŠ¤æ¨¡å¼ï¼ˆå¼•å·ã€æ‹¬å·ç­‰ï¼Œç»å¯¹ä¸æ‹†åˆ†ï¼‰
PROTECTED_PATTERNS = [
    r'"[^"]*"',      # åŒå¼•å·å†…å®¹
    r"'[^']*'",      # å•å¼•å·å†…å®¹
    r'\([^)]*\)',    # åœ†æ‹¬å·å†…å®¹
    r'\[[^\]]*\]',   # æ–¹æ‹¬å·å†…å®¹
]

# é•¿åº¦æ§åˆ¶å¸¸é‡ - é’ˆå¯¹è¯­éŸ³åˆæˆä¼˜åŒ–
MAX_SENTENCE_LENGTH = 80      # ç›®æ ‡æœ€å¤§é•¿åº¦ï¼ˆé€‚åˆè¯­éŸ³åˆæˆï¼‰
MIN_MERGE_LENGTH = 50      # æœ€å¤§åˆå¹¶é•¿åº¦
MAX_MERGE_LENGTH = 100      # æœ€å¤§åˆå¹¶é•¿åº¦

# æˆå¯¹ç¬¦å·å®šä¹‰ï¼ˆæ”¯æŒæ‰€æœ‰ç±»å‹å¼•å·å’Œæ‹¬å·ï¼‰
QUOTE_PAIRS = [
    ('"', '"'),     # æ ‡å‡†åŒå¼•å·
    ('"', '"'),     # å¼¯æ›²åŒå¼•å·
    ('â€', '"'),     # å¾·å¼åŒå¼•å·
    ("'", "'"),     # æ ‡å‡†å•å¼•å·
    ("'", "'"),     # å¼¯æ›²å•å¼•å·
    ("â€š", "'"),     # å¾·å¼å•å¼•å·
    ('(', ')'),     # åœ†æ‹¬å·
    ('[', ']'),     # æ–¹æ‹¬å·
    ('{', '}'),     # èŠ±æ‹¬å·
]


@dataclass
class SentenceSplitterConfig:
    """å¥å­æ‹†åˆ†é…ç½®ç±»"""
    
    # è¾“å‡ºå­ç›®å½•å
    output_subdir: str = "sentences"
    
    # åˆ†å‰²å™¨ç±»å‹ï¼š'nltk' æˆ– 'pysbd'
    segmenter: str = "nltk"
    
    # è¯­è¨€è®¾ç½®
    language: str = "en"
    
    # æ˜¯å¦æ¸…ç†æ–‡æœ¬
    clean: bool = False
    
    # æ˜¯å¦å¯ç”¨çŸ­å¥æ‹†åˆ†
    enable_clause_splitting: bool = True
    
    # è§¦å‘æ‹†åˆ†çš„æœ€å¤§å¥å­é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
    max_sentence_length: int = 100


class SentenceSplitter:
    """å¥å­æ‹†åˆ†å™¨"""
    
    def __init__(self, config: SentenceSplitterConfig):
        """
        åˆå§‹åŒ–å¥å­æ‹†åˆ†å™¨
        
        Args:
            config: å¥å­æ‹†åˆ†é…ç½®
        """
        self.config = config
        self._ensure_nltk_data()
    
    def _ensure_nltk_data(self):
        """
        ç¡®ä¿NLTKæ•°æ®åŒ…å¯ç”¨
        """
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            print("ä¸‹è½½NLTK punktæ•°æ®åŒ…...")
            nltk.download('punkt')
    
    def split_files(self, input_files: List[str], output_dir: str) -> List[str]:
        """
        æ‹†åˆ†æ–‡ä»¶åˆ—è¡¨ä¸ºå¥å­çº§æ–‡ä»¶
        
        Args:
            input_files: è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            ç”Ÿæˆçš„å¥å­æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        # åˆ›å»ºè¾“å‡ºç›®å½•
        sentences_dir = os.path.join(output_dir, self.config.output_subdir)
        os.makedirs(sentences_dir, exist_ok=True)
        
        output_files = []
        
        for input_file in input_files:
            try:
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
                filename = os.path.basename(input_file)
                output_file = os.path.join(sentences_dir, filename)
                
                # å¤„ç†å•ä¸ªæ–‡ä»¶
                self._process_file(input_file, output_file)
                output_files.append(output_file)
                
                print(f"ğŸ“ å·²å¤„ç†å¥å­æ‹†åˆ†: {filename}")
            except Exception as e:
                print(f"âŒ æ‹†åˆ†å¤±è´¥: {e}")
                continue
        
        print(f"\nğŸ“ å¥å­æ‹†åˆ†å®Œæˆï¼Œè¾“å‡ºåˆ°: {sentences_dir}")
        return output_files
    
    def _process_file(self, input_file: str, output_file: str):
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶çš„å¥å­æ‹†åˆ†
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # è¯»å–è¾“å…¥æ–‡ä»¶
        with open(input_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–æ ‡é¢˜å’Œæ­£æ–‡
        title, body = self._extract_title_and_body(content)
        
        # å¤„ç†æ®µè½å¥å­æ‹†åˆ†
        processed_content = self._split_paragraphs_to_sentences(body)
        
        # æ„å»ºæœ€ç»ˆå†…å®¹
        final_content = f"{title}\n\n{processed_content}"
        
        # å†™å…¥è¾“å‡ºæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(final_content)
    
    def _extract_title_and_body(self, content: str) -> tuple[str, str]:
        """
        æå–æ ‡é¢˜å’Œæ­£æ–‡å†…å®¹
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            
        Returns:
            (æ ‡é¢˜, æ­£æ–‡å†…å®¹)
        """
        lines = content.split('\n')
        
        # ç¬¬ä¸€è¡Œæ˜¯æ ‡é¢˜
        title = lines[0].strip() if lines else "Unknown Title"
        
        # å…¶ä½™æ˜¯æ­£æ–‡ï¼ˆå»é™¤å¼€å¤´çš„ç©ºè¡Œï¼‰
        body_lines = lines[1:]
        while body_lines and not body_lines[0].strip():
            body_lines.pop(0)
        
        body = '\n'.join(body_lines)
        return title, body
    
    def _split_paragraphs_to_sentences(self, content: str) -> str:
        """
        å°†å†…å®¹æŒ‰æ®µè½æ‹†åˆ†ï¼Œå†å°†æ¯ä¸ªæ®µè½æ‹†åˆ†ä¸ºå¥å­
        
        Args:
            content: æ­£æ–‡å†…å®¹
            
        Returns:
            å¤„ç†åçš„å†…å®¹
        """
        # æŒ‰æ®µè½åˆ†å‰²ï¼ˆåŒæ¢è¡Œåˆ†å‰²ï¼‰
        paragraphs = re.split(r'\n\n', content)
        
        # è¿‡æ»¤ç©ºæ®µè½
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        
        processed_paragraphs = []
        
        for paragraph in paragraphs:
            # å¯¹æ®µè½è¿›è¡Œå¥å­æ‹†åˆ†
            sentences = self._split_sentences(paragraph)
            
            # å°†å¥å­åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆæ¯å¥ä¸€è¡Œï¼‰
            if sentences:
                paragraph_content = '\n'.join(sentences)
                processed_paragraphs.append(paragraph_content)
        
        # æ®µè½é—´ç”¨ç©ºè¡Œåˆ†éš”
        return '\n\n'.join(processed_paragraphs)
    
    def _split_sentences(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå¥å­ï¼Œä½¿ç”¨å¼•å·ä¼˜å…ˆçš„è¿­ä»£åˆ†å‰²æ–¹æ³•
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å¥å­åˆ—è¡¨
        """
        # æ¸…ç†æ–‡æœ¬ï¼ˆç§»é™¤å¤šä½™ç©ºç™½ï¼‰
        text = re.sub(r'\s+', ' ', text.strip())
        
        if not text:
            return []
        
        # ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨ä¸“ä¸šå·¥å…·è¿›è¡ŒåŸºç¡€å¥å­åˆ†å‰²
        if self.config.segmenter == "pysbd":
            sentences = self._split_with_pysbd(text)
        else:
            sentences = self._split_with_nltk(text)
        
        # ç¬¬äºŒé˜¶æ®µï¼šæ–°çš„é•¿å¥æ‹†åˆ†é€»è¾‘
        if self.config.enable_clause_splitting:
            sentences = self._split_long_sentences_new(sentences)
        
        # æ¸…ç†å¥å­ï¼ˆå»é™¤é¦–å°¾ç©ºç™½ï¼‰
        sentences = [s.strip() for s in sentences if s.strip()]
        
        return sentences
    
    def _split_with_pysbd(self, text: str) -> List[str]:
        """
        ä½¿ç”¨pySBDè¿›è¡Œå¥å­åˆ†å‰²
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å¥å­åˆ—è¡¨
        """
        seg = pysbd.Segmenter(language=self.config.language, clean=self.config.clean)
        return seg.segment(text)
    
    def _split_with_nltk(self, text: str) -> List[str]:
        """
        ä½¿ç”¨NLTKè¿›è¡Œå¥å­åˆ†å‰²
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å¥å­åˆ—è¡¨
        """
        return nltk.sent_tokenize(text)
    
    
    def _split_long_sentences_new(self, sentences: List[str]) -> List[str]:
        """
        æ–°çš„é•¿å¥æ‹†åˆ†ç­–ç•¥ï¼šæˆå¯¹ç¬¦å·ä¿æŠ¤ + åˆ†éš”ç¬¦æ‹†åˆ† + æ™ºèƒ½åˆå¹¶
        
        Args:
            sentences: åŸå§‹å¥å­åˆ—è¡¨
            
        Returns:
            å¤„ç†åçš„å¥å­åˆ—è¡¨
        """
        result = []
        
        for sentence in sentences:
            if len(sentence) <= MAX_SENTENCE_LENGTH:
                result.append(sentence)
                continue
            
            # å¯¹é•¿å¥è¿›è¡Œæ‹†åˆ†-åˆå¹¶å¤„ç†
            split_result = self.split_into_clauses(sentence)
            result.extend(split_result)
        
        return result
    
    def _parse_text_into_clauses(self, text: str,
                                paren_symbols=PAIR_SYMBOLS_PARENS,
                                quote_symbols=PAIR_SYMBOLS_QUOTES,
                                split_punct=SPLIT_PUNCT):
        """
        å°†æ–‡æœ¬æ‹†åˆ†æˆå­å¥çš„æ ¸å¿ƒé€»è¾‘ï¼š
        1. ä½¿ç”¨ç»Ÿä¸€çš„é…å¯¹ç¬¦å·å¤„ç†é€»è¾‘
        2. åˆ†éš”ç¬¦åŒ…å«åœ¨å­å¥æœ«å°¾ï¼Œè§¦å‘æ‹†åˆ†
        3. ç¡®ä¿è¯­ä¹‰è¾¹ç•Œçš„æ­£ç¡®æ€§
        """
        # ç»Ÿä¸€æ‰€æœ‰é…å¯¹ç¬¦å·
        all_pairs = []
        # æ·»åŠ æ‹¬å·ç±»ç¬¦å·
        for open_sym, close_sym in paren_symbols:
            all_pairs.append((open_sym, close_sym, "paren"))
        # æ·»åŠ å¼•å·ç±»ç¬¦å·
        for open_sym, close_sym in quote_symbols:
            all_pairs.append((open_sym, close_sym, "quote"))
        
        # åˆ›å»ºç¬¦å·æ˜ å°„
        open_to_close = {}
        close_to_open = {}
        symbol_types = {}
        
        for open_sym, close_sym, symbol_type in all_pairs:
            open_to_close[open_sym] = close_sym
            close_to_open[close_sym] = open_sym
            symbol_types[open_sym] = symbol_type
            symbol_types[close_sym] = symbol_type
        
        clauses = []
        buf = []
        stack = []  # è·Ÿè¸ªé…å¯¹ç¬¦å·çŠ¶æ€ [(symbol_type, open_symbol, content)]
        
        for ch in text:
            # æ£€æŸ¥æ˜¯å¦æ˜¯é…å¯¹ç¬¦å·
            if ch in open_to_close:
                # å¯èƒ½æ˜¯å¼€å§‹ç¬¦å·
                expected_close = open_to_close[ch]
                symbol_type = symbol_types[ch]
                
                # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯å¼€å§‹ç¬¦å·ï¼ˆå¯¹äºç›¸åŒå¼€å§‹/ç»“æŸç¬¦å·å¦‚å¼•å·ï¼‰
                is_opening = True
                if ch == expected_close:  # ç›¸åŒç¬¦å·ï¼Œéœ€è¦é€šè¿‡æ ˆçŠ¶æ€åˆ¤æ–­
                    # æŸ¥æ‰¾æ ˆä¸­æ˜¯å¦æœ‰ç›¸åŒç¬¦å·ç±»å‹çš„æœªé…å¯¹ç¬¦å·
                    for stack_item in stack:
                        if stack_item[0] == symbol_type and stack_item[1] == ch:
                            is_opening = False
                            break
                
                if is_opening:
                    # å¼€å§‹ç¬¦å·ï¼šä¿å­˜å½“å‰ç¼“å†²åŒºï¼Œå¼€å§‹æ”¶é›†é…å¯¹å†…å®¹
                    if buf:
                        clause = ''.join(buf).rstrip()  # å»é™¤å°¾éƒ¨ç©ºæ ¼
                        if clause:
                            clauses.append(clause)
                        buf = []
                    
                    stack.append((symbol_type, ch, [ch]))
                else:
                    # ç»“æŸç¬¦å·ï¼šå®Œæˆé…å¯¹å†…å®¹æ”¶é›†
                    if stack:
                        for i in reversed(range(len(stack))):
                            if stack[i][0] == symbol_type and stack[i][1] == ch:
                                # æ‰¾åˆ°åŒ¹é…çš„å¼€å§‹ç¬¦å·
                                symbol_type, open_sym, content = stack.pop(i)
                                content.append(ch)
                                clause = ''.join(content).strip()
                                if clause:
                                    clauses.append(clause)
                                break
                continue
            
            elif ch in close_to_open:
                # æ˜ç¡®çš„ç»“æŸç¬¦å·ï¼ˆå¼€å§‹å’Œç»“æŸä¸åŒçš„æƒ…å†µï¼‰
                open_sym = close_to_open[ch]
                symbol_type = symbol_types[ch]
                
                # æŸ¥æ‰¾æ ˆä¸­åŒ¹é…çš„å¼€å§‹ç¬¦å·
                if stack:
                    for i in reversed(range(len(stack))):
                        if stack[i][0] == symbol_type and stack[i][1] == open_sym:
                            # æ‰¾åˆ°åŒ¹é…çš„å¼€å§‹ç¬¦å·
                            symbol_type, open_sym, content = stack.pop(i)
                            content.append(ch)
                            clause = ''.join(content).strip()
                            if clause:
                                clauses.append(clause)
                            break
                continue
            
            # å¦‚æœåœ¨é…å¯¹ç¬¦å·å†…ï¼Œæ·»åŠ åˆ°ç›¸åº”çš„å†…å®¹ä¸­
            if stack:
                stack[-1][2].append(ch)
                continue
            
            # æ­£å¸¸å­—ç¬¦å¤„ç†
            buf.append(ch)
            
            # åˆ†éš”ç¬¦å¤„ç†ï¼šåŒ…å«åœ¨å½“å‰å­å¥ä¸­ï¼Œç„¶åè§¦å‘æ‹†åˆ†
            if ch in split_punct:
                clause = ''.join(buf).strip()
                if clause and len(clause) > 1:  # é¿å…å•ä¸ªåˆ†éš”ç¬¦æˆä¸ºç‹¬ç«‹å­å¥
                    clauses.append(clause)
                    buf = []
                continue
        
        # æ”¶å°¾å¤„ç†
        if buf:
            clause = ''.join(buf).strip()
            if clause:
                clauses.append(clause)
        
        # æ¸…ç†å¹¶è¿‡æ»¤ç©ºå­å¥
        clauses = [clause.strip() for clause in clauses if clause.strip()]
        
        return clauses

    def split_into_clauses(self, text: str,
                        paren_symbols=PAIR_SYMBOLS_PARENS,
                        quote_symbols=PAIR_SYMBOLS_QUOTES,
                        split_punct=SPLIT_PUNCT,
                        min_len: int = 15):
        """
        å°†æ–‡æœ¬æ‹†åˆ†æˆå­å¥ï¼š
        1. æ‹¬å·ç±»ç¬¦å·å†…çš„æ–‡æœ¬ä½œä¸ºç‹¬ç«‹å­å¥ã€‚
        2. å¼•å·ç±»ç¬¦å·å†…çš„æ–‡æœ¬ä½œä¸ºç‹¬ç«‹å­å¥ã€‚
        3. åˆ†éš”ç¬¦è§¦å‘æ‹†åˆ†ï¼Œåˆ†éš”ç¬¦ä¿ç•™åœ¨å­å¥æœ«å°¾ã€‚
        4. çŸ­å­å¥è‡ªåŠ¨ä¸å‰ä¸€ä¸ªå­å¥åˆå¹¶ã€‚
        5. å¯¹é•¿åº¦è¶…è¿‡é˜ˆå€¼çš„æ‹¬å·æˆ–å¼•å·å­å¥å†æ¬¡æ‹†åˆ†ã€‚
        """
        # ç¬¬ä¸€æ¬¡è°ƒç”¨å†…éƒ¨é€»è¾‘è¿›è¡ŒåŸºç¡€æ‹†åˆ†
        clauses = self._parse_text_into_clauses(text, paren_symbols, quote_symbols, split_punct)

        # ç¬¬äºŒæ¬¡è°ƒç”¨å†…éƒ¨é€»è¾‘ï¼Œå¯¹é•¿åº¦è¶…è¿‡é˜ˆå€¼çš„æ‹¬å·æˆ–å¼•å·åŒ…å›´çš„å­å¥è¿›è¡Œå†æ‹†åˆ†
        final_result = []
        for clause in clauses:
            if len(clause) > MAX_SENTENCE_LENGTH and self._is_quoted_or_parenthesized(clause):
                # å»æ‰å¤–å±‚æ‹¬å·æˆ–å¼•å·ï¼Œæ‹†åˆ†å†…éƒ¨å†…å®¹ï¼Œç„¶åé‡æ–°åŒ…å›´
                inner_content, wrapper = self._extract_inner_content_and_wrapper(clause)
                if inner_content:
                    inner_clauses = self._parse_text_into_clauses(inner_content, paren_symbols, quote_symbols, split_punct)
                    # é‡æ–°æ·»åŠ åŒ…å›´ç¬¦å·
                    for i, inner_clause in enumerate(inner_clauses):
                        if i == 0 and i == len(inner_clauses) - 1:
                            # åªæœ‰ä¸€ä¸ªå­å¥ï¼Œå®Œæ•´åŒ…å›´
                            final_result.append(f"{wrapper[0]}{inner_clause}{wrapper[1]}")
                        elif i == 0:
                            # ç¬¬ä¸€ä¸ªå­å¥ï¼ŒåªåŠ å¼€å§‹ç¬¦å·
                            final_result.append(f"{wrapper[0]}{inner_clause}")
                        elif i == len(inner_clauses) - 1:
                            # æœ€åä¸€ä¸ªå­å¥ï¼ŒåªåŠ ç»“æŸç¬¦å·
                            final_result.append(f"{inner_clause}{wrapper[1]}")
                        else:
                            # ä¸­é—´å­å¥ï¼Œä¸åŠ ç¬¦å·
                            final_result.append(inner_clause)
                else:
                    final_result.append(clause)
            else:
                final_result.append(clause)

        # åˆå¹¶è¿‡çŸ­çš„å­å¥
        merged = []
        for c in final_result:
            if merged and len(merged[-1]) < MIN_MERGE_LENGTH and len(merged[-1]) + len(c) < MAX_MERGE_LENGTH:
                merged[-1] += " " + c
            else:
                merged.append(c)

        return merged

    def _is_quoted_or_parenthesized(self, text: str) -> bool:
        """
        æ£€æŸ¥æ–‡æœ¬æ˜¯å¦è¢«æ‹¬å·æˆ–å¼•å·åŒ…å›´
        """
        if len(text) < 2:
            return False
        
        # æ£€æŸ¥æ˜¯å¦è¢«æ‹¬å·åŒ…å›´
        for open_sym, close_sym in PAIR_SYMBOLS_PARENS:
            if text.startswith(open_sym) and text.endswith(close_sym):
                return True
        
        # æ£€æŸ¥æ˜¯å¦è¢«å¼•å·åŒ…å›´
        for open_sym, close_sym in PAIR_SYMBOLS_QUOTES:
            if text.startswith(open_sym) and text.endswith(close_sym):
                return True
        
        return False

    def _extract_inner_content_and_wrapper(self, text: str) -> tuple[str, tuple[str, str]]:
        """
        ä»è¢«åŒ…å›´çš„æ–‡æœ¬ä¸­æå–å†…éƒ¨å†…å®¹å’ŒåŒ…å›´ç¬¦å·
        
        Returns:
            (å†…éƒ¨å†…å®¹, (å¼€å§‹ç¬¦å·, ç»“æŸç¬¦å·))
        """
        if len(text) < 2:
            return text, ("", "")
        
        # æ£€æŸ¥æ‹¬å·
        for open_sym, close_sym in PAIR_SYMBOLS_PARENS:
            if text.startswith(open_sym) and text.endswith(close_sym):
                inner = text[len(open_sym):-len(close_sym)]
                return inner, (open_sym, close_sym)
        
        # æ£€æŸ¥å¼•å·
        for open_sym, close_sym in PAIR_SYMBOLS_QUOTES:
            if text.startswith(open_sym) and text.endswith(close_sym):
                inner = text[len(open_sym):-len(close_sym)]
                return inner, (open_sym, close_sym)
        
        return text, ("", "")

